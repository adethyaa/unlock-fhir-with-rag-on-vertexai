{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3674aa63-bc87-45d7-9ddf-11a89e010cb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a772cf-6f39-46c1-a76b-bd304c160869",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Unlock FHIR with RAG on Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fc0b21-0a5a-4c32-b372-9624d84a13f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run the Notebook\n",
    "\n",
    "**_NOTE_**: This notebook has been tested in the following environment:\n",
    "\n",
    "* Python version = 3.10.13\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/adethyaa/unlock-fhir-with-rag-on-vertexai/blob/main/Unlock%20FHIR%20with%20RAG%20on%20Vertex%20AI.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/adethyaa/unlock-fhir-with-rag-on-vertexai/blob/main/Unlock%20FHIR%20with%20RAG%20on%20Vertex%20AI.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/adethyaa/unlock-fhir-with-rag-on-vertexai/blob/main/Unlock%20FHIR%20with%20RAG%20on%20Vertex%20AI.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298b88f6-9934-4b82-a34f-71eaaa774205",
   "metadata": {},
   "source": [
    "| | |\n",
    "|-|-|\n",
    "|Author(s) | [Vikrama Adethyaa](https://github.com/adethyaa) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8e0f8b-8c58-42c0-a427-c0fef6de822b",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Overview\n",
    "\n",
    "This notebook demonstrates building a natural language interface to complex [FHIR](https://fhir.org/about.html) datasets using [Google Cloud’s Vertex AI](https://cloud.google.com/vertex-ai?hl=en).  Leveraging Retrieval Augmented Generation (RAG), Enterprise Knowledge Graphs, and Vector Search, this solution empowers healthcare professionals to query FHIR data with natural language. This demo is inspired by [Sam Schifman's work](https://medium.com/@samschifman/rag-on-fhir-with-knowledge-graphs-04d8e13ee96e)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8993564-2aba-4121-bfaf-26707c4567f2",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c0101e-93aa-45d4-a0f9-472a2d198ff1",
   "metadata": {},
   "source": [
    "### 2.1. Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "3. [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
    "\n",
    "4. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242e0dab-155e-4d9d-9895-c0588da7784d",
   "metadata": {},
   "source": [
    "### 2.2. Install Packages and Dependencies\n",
    "Please install the following packages to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39d7707-82f7-421f-908f-1d398c1e9537",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install Vertex AI SDK\n",
    "! pip install --user -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e97732f-ad64-40ff-bea3-f0feb709fd3b",
   "metadata": {},
   "source": [
    "### 2.3 Download Matching Engine Helper Scripts\n",
    "\n",
    "- ***Matching Engine*** is now called ***[Vertex AI Vector Search](https://cloud.google.com/vertex-ai/docs/vector-search/overview)***\n",
    "- The cell below downloads helper functions necessary for the Vertex AI Matching Engine. These functions improve notebook readability. You can find the ***[source code on Github](https://github.com/GoogleCloudPlatform/generative-ai/tree/main/language/use-cases/document-qa/utils).***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005b5a21-7464-4ef7-adc3-782285d2d309",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "util_folder = \"utils\"\n",
    "\n",
    "if not os.path.exists(util_folder):\n",
    "    os.makedirs(util_folder)\n",
    "\n",
    "url_prefix = \"https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/language/use-cases/document-qa/utils\"\n",
    "files = [\"__init__.py\", \"matching_engine.py\", \"matching_engine_utils.py\"]\n",
    "\n",
    "for fname in files:\n",
    "    urllib.request.urlretrieve(f\"{url_prefix}/{fname}\", filename=f\"{util_folder}/{fname}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ea4c31-de61-423d-9e19-09d28c15737f",
   "metadata": {
    "tags": []
   },
   "source": [
    "***Restart Kernel***\n",
    "\n",
    "Run the following cell to restart the kernel or use the button to restart the kernel.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ Kindly allow the kernel to finish restarting before continuing. ⚠️</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913677bf-4b62-4b66-aec2-ba9e346fa547",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs so that your environment can access the new packages\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc33f0ce-daa7-4704-8648-202c0bd6e0ea",
   "metadata": {},
   "source": [
    "### 2.4. Authenticating your notebook environment\n",
    "\n",
    "- If you are using **Colab** to run this notebook, run the cell below and continue.\n",
    "- If you are using **Vertex AI Workbench**, check out the setup instructions [here](https://github.com/GoogleCloudPlatform/generative-ai/tree/main/setup-env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fc7c013-4bd0-4c24-a28d-599f5143eca4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79683a19-3adf-46ab-8e71-334683196fbb",
   "metadata": {},
   "source": [
    "**Vertex AI Workbench**\n",
    "- Open a Terminal in the Jupyter notebook\n",
    "- Execute the below command and follow the instructions\n",
    "\n",
    "```bash\n",
    "gcloud auth login\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f061980-d6fa-4b08-8aa2-65c000674e62",
   "metadata": {},
   "source": [
    "### 2.5. Define Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "916368fc-fcaf-49ec-aa35-c3491cfa426c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Limit FHIR files loaded for demo purposes\n",
    "# Since Data Ingestion into Neo4J and Vector Search Index takes time, We set the parameter below to control the number of files ingested.\n",
    "DEMO_FILES_INGEST_LIMIT = 20 # @param {type:\"integer\"}\n",
    "\n",
    "# GCP Parameters\n",
    "PROJECT_ID = \"propane-crawler-363311\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"  # @param {type: \"string\"}\n",
    "\n",
    "# Neo4J Connection Parameters\n",
    "NEO4J_URL=\"bolt://localhost:7687\" # @param {type:\"string\"}\n",
    "NEO4J_USER=\"neo4j\" # @param {type:\"string\"}\n",
    "NEO4J_PASSWORD=\"password\" # @param {type:\"string\"}\n",
    "\n",
    "# Dimension Vertex PaLM Text Embedding\n",
    "ME_DIMENSIONS = 768 # @param {type:\"integer\"} \n",
    "ME_DISTANCE_MEASURE_TYPE = \"DOT_PRODUCT_DISTANCE\" # @param {type:\"string\"} \n",
    "\n",
    "# Update to bigger SHARDS for larger data volumes & performance\n",
    "# Doc - https://cloud.google.com/vertex-ai/docs/vector-search/create-manage-index\n",
    "ME_SHARD_SIZE = \"SHARD_SIZE_SMALL\" # @param [\"SHARD_SIZE_SMALL\", \"SHARD_SIZE_MEDIUM\", \"SHARD_SIZE_LARGE\"] \n",
    "\n",
    "# Vertex AI Vector Search (MatchingEngine) Endpoint Parameters\n",
    "# Doc - https://cloud.google.com/vertex-ai/docs/vector-search/create-manage-index\n",
    "\n",
    "# The machine types that you can use to deploy your index\n",
    "ME_ENDPOINT_MACHINE_TYPE = \"e2-standard-2\" # @param [\"n1-standard-16\", \"n1-standard-32\", \"e2-standard-2\", \"e2-standard-16\", \"e2-highmem-16\", \"n2d-standard-32\"] \n",
    "\n",
    "ME_ENDPOINT_MIN_REPLICA_COUNT = 2 # @param {type:\"integer\"} \n",
    "ME_ENDPOINT_MAX_REPLICA_COUNT = 10 # @param {type:\"integer\"} \n",
    "\n",
    "# Vertex AI Vector Search (MatchingEngine) Index Parameters\n",
    "ME_INDEX_NAME = 'fhir_me_index'  # @param {type: \"string\"}\n",
    "ME_EMBEDDING_GCS_DIR = f'{PROJECT_ID}-me-bucket' # @param {type:\"string\"} \n",
    "ME_DESCRIPTION = \"Index for FHIR Resources\" # @param {type:\"string\"} \n",
    "\n",
    "ME_ENHANCED_CONTEXT_INDEX_NAME = f'{ME_INDEX_NAME}_enhanced' # @param {type:\"string\"} \n",
    "ME_ENHANCED_EMBEDDING_GCS_DIR = f'{ME_EMBEDDING_GCS_DIR}_enhanced' # @param {type:\"string\"} \n",
    "ME_ENHANCED_DESCRIPTION = f'Enhanced Context {ME_DESCRIPTION}' # @param {type:\"string\"} \n",
    "\n",
    "\n",
    "\n",
    "# Set the LLM to use\n",
    "VERTEX_AI_MODEL_NAME = 'gemini-1.0-pro-001'\n",
    "TEXT_EMBEDDING_MODEL_NAME = \"textembedding-gecko@003\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f253c06-aa6c-40f0-9eec-d17f8db3366c",
   "metadata": {},
   "source": [
    "### 2.6. Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f54b383-6811-4494-9e6f-cc57e4b9b879",
   "metadata": {},
   "source": [
    "**Colab only:** Run the below cell to initialize the Vertex AI SDK. For Vertex AI Workbench, you don't need to run this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da298a4d-bdcd-494f-9dab-de0fd9fe7341",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import vertexai\n",
    "vertexai.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1669cb-591e-4cb6-9eb9-70307c428c9a",
   "metadata": {},
   "source": [
    "<br>*Import Python Libraries*\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ Restart python kernel if issues while importing langchain  ⚠️</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3982e5e8-9a8c-4203-9110-76b239afa014",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain version: 0.1.16\n"
     ]
    }
   ],
   "source": [
    "# Utils\n",
    "import os\n",
    "import glob\n",
    "from pprint import pprint\n",
    "import json\n",
    "import csv\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import uuid\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "\n",
    "# Google Vertex AI\n",
    "# from google.cloud import aiplatform\n",
    "# print(f\"Vertex AI SDK version: {aiplatform.__version__}\")\n",
    "\n",
    "# Neo4j Helper scripts\n",
    "from utils.NEO4J_Graph import Graph\n",
    "from utils.FHIR_to_string import FHIR_to_string\n",
    "from utils.FHIR_to_graph import resource_to_node, resource_to_edges, flat_fhir_to_json_str\n",
    "\n",
    "# Langchain\n",
    "import langchain\n",
    "print(f\"LangChain version: {langchain.__version__}\")\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "\n",
    "from langchain_google_vertexai import VertexAI\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_google_vertexai import VectorSearchVectorStore\n",
    "from google.cloud.aiplatform.matching_engine.matching_engine_index_endpoint import (\n",
    "    Namespace,\n",
    "    NumericNamespace,\n",
    ")\n",
    "\n",
    "\n",
    "# Import libraries\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "\n",
    "\n",
    "# Import custom Matching Engine packages\n",
    "from utils.matching_engine import MatchingEngine\n",
    "from utils.matching_engine_utils import MatchingEngineUtils\n",
    "\n",
    "# Import Custom LangChain Retriever\n",
    "from utils.FHIRResourcesRetriever import FHIRResourcesRetriever\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f3ac83-23fe-4d26-abf5-46b0dce647ad",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Setup RAG Prerequisites\n",
    "\n",
    "In this section we will create:\n",
    "- Neo4J - local deployment for Enterprise Knowledge Graph\n",
    "- VertexAI VectorSearch - Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af17c1c2-25ee-4614-8446-3ab330280399",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.1. Deploy Neo4J Docker Container\n",
    "\n",
    "We'll use **Docker** to deploy a local Neo4j instance for demo purposes.  For production environments, we highly recommend Neo4j on Google Cloud Marketplace. [Try Neo4j on Google Cloud Marketplace](https://console.cloud.google.com/marketplace/product/endpoints/prod.n4gcp.neo4j.io?pli=1&mpp=4bfb2414ab973c741b6f067bf06d5575&mpid=%24device%3A18e0c346ea25f9-098a968de268b5-1d525637-384000-18e0c346ea25fa).\n",
    "\n",
    "The command below launches a local Neo4j database Docker container named ***testneo4j***. Let's break down what it does:\n",
    "\n",
    "**Port Mapping:**\n",
    "- 7474: Access the Neo4j web interface through your browser (usually http://localhost:7474).\n",
    "- 7687: Enables communication with Neo4j using the Bolt protocol (essential for working with the database).\n",
    "\n",
    "**Data Volumes:** Folders on your machine ($HOME/neo4j/*) store your database, logs, imports, etc., so your data is safe even if the container stops.\n",
    "\n",
    "**Secure Credentials:** Variables `{NEO4J_USER}` and `{NEO4J_PASSWORD}` set your Neo4j username and password for secure access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09d7f89c-a285-43c7-9228-6078390f2a19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: NEO4J_USER=neo4j\n",
      "env: NEO4J_PASSWORD=password\n"
     ]
    }
   ],
   "source": [
    "%env NEO4J_USER={NEO4J_USER}\n",
    "%env NEO4J_PASSWORD={NEO4J_PASSWORD}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74753b6e-1af0-43e7-bc5e-3ed8a32cef19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docker: Error response from daemon: Conflict. The container name \"/testneo4j\" is already in use by container \"c11076036c54a06a362119887aa38eebcfe7f287f7f436f8b5b241f0d98e0012\". You have to remove (or rename) that container to be able to reuse that name.\n",
      "See 'docker run --help'.\n"
     ]
    }
   ],
   "source": [
    "! docker run --name testneo4j -p7474:7474 -p7687:7687 -d \\\n",
    "    -v $HOME/neo4j/data:/data \\\n",
    "    -v $HOME/neo4j/logs:/logs \\\n",
    "    -v $HOME/neo4j/import:/var/lib/neo4j/import \\\n",
    "    -v $HOME/neo4j/plugins:/plugins \\\n",
    "    --env NEO4J_AUTH=$NEO4J_USER/$NEO4J_PASSWORD \\\n",
    "    --env='NEO4JLABS_PLUGINS=[\"apoc\"]' \\\n",
    "    neo4j:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52722622-ef65-458b-aae9-cc98121af1f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE          COMMAND                  CREATED        STATUS        PORTS                                                                                            NAMES\n",
      "c11076036c54   neo4j:latest   \"tini -g -- /startup…\"   43 hours ago   Up 10 hours   0.0.0.0:7474->7474/tcp, :::7474->7474/tcp, 7473/tcp, 0.0.0.0:7687->7687/tcp, :::7687->7687/tcp   testneo4j\n"
     ]
    }
   ],
   "source": [
    "# Check if Docker Container is running\n",
    "! docker ps -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c01e6b9c-6d70-49b6-ae3c-754f89476301",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testneo4j\n"
     ]
    }
   ],
   "source": [
    "# Start the Container if it is not running\n",
    "! docker start testneo4j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbba084-d2d8-466d-93b8-b813d26b4eae",
   "metadata": {},
   "source": [
    "#### 3.1.1. Connect to Neo4J Database\n",
    "\n",
    "This code block creates a Graph object instance, establishing a connection to the Neo4j database.\n",
    "See **utils/NEO4J_Graph.py** for more information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d31a64f-3ef0-41fe-9c1b-e34d09abd2e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "graph = Graph(NEO4J_URL, NEO4J_USER, NEO4J_PASSWORD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae410c5-c84d-4fdd-b8ab-ea905fc1fa1a",
   "metadata": {},
   "source": [
    "#### 3.1.2. Neo4j Database Helper Cells\n",
    "\n",
    "The following three cells contain database management functions. For a new or blank database, you can skip them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f1dc653-1a32-4fbc-9af2-c1b9d27cadaa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['AllergyIntolerance', 10],\n",
      " ['CarePlan', 60],\n",
      " ['CareTeam', 60],\n",
      " ['Claim', 3486],\n",
      " ['Condition', 656],\n",
      " ['Device', 39],\n",
      " ['DiagnosticReport', 3100],\n",
      " ['DocumentReference', 1859],\n",
      " ['Encounter', 1859],\n",
      " ['ExplanationOfBenefit', 3486],\n",
      " ['ImagingStudy', 8],\n",
      " ['Immunization', 259],\n",
      " ['Medication', 957],\n",
      " ['MedicationAdministration', 957],\n",
      " ['MedicationRequest', 1627],\n",
      " ['Observation', 13501],\n",
      " ['Patient', 20],\n",
      " ['Procedure', 2966],\n",
      " ['SupplyDelivery', 239]]\n"
     ]
    }
   ],
   "source": [
    "# Get type and number of each FHIR resource in the database\n",
    "resource_metrics = graph.resource_metrics()\n",
    "resource_metrics.sort()\n",
    "pprint(resource_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04cf6497-5998-46bf-aa31-565dab333130",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database Metrics:\n",
      "    - Node Count = 37884\n",
      "    - Relationship Count = 190926\n"
     ]
    }
   ],
   "source": [
    "# metrics for counting nodes and relationships\n",
    "node_count, relationship_count = graph.database_metrics()\n",
    "print('Database Metrics:')\n",
    "print(f'    - Node Count = {node_count}')\n",
    "print(f'    - Relationship Count = {relationship_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c61ea78-fd92-470e-a404-f9e7f47a7ce2",
   "metadata": {},
   "source": [
    "### 3.2. Create VertexAI VectorSearch Index\n",
    "\n",
    "This involves the following steps:\n",
    "- Create a GCS Bucket for storing text.\n",
    "- Instantiate embedding model - used for creating vectors of Text.\n",
    "- Create a VectorSearch Streaming Index\n",
    "- Create VectorSearch IndexEndpoint & deploy Index\n",
    "- Create LangChain VectorStore on VertexAI VectorSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbf6d60-9eaa-4f7b-a862-fb19de7ed192",
   "metadata": {},
   "source": [
    "#### 3.2.1. Create GCS Bucket \n",
    "\n",
    "The Google Cloud Storage Bucket will be used by Vector Store Index.\n",
    "[Create and manage your index](https://cloud.google.com/vertex-ai/docs/vector-search/create-manage-index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cd712a6-8a88-47b3-8805-65cb0d81112b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ gsutil mb -p propane-crawler-363311 -l us-central1 gs://propane-crawler-363311-me-bucket\n",
      "Creating gs://propane-crawler-363311-me-bucket/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'propane-crawler-363311-me-bucket' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
     ]
    }
   ],
   "source": [
    "! set -x && gsutil mb -p $PROJECT_ID -l $REGION gs://$ME_EMBEDDING_GCS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56e6e20-2b13-46f4-aac8-a6bace021db5",
   "metadata": {},
   "source": [
    "#### 3.2.2. Instantiate embedding model\n",
    "\n",
    "Load the pre-trained models textembedding-gecko@003 (embedding generation)\n",
    "Learn more about Google's Foundation Models and their capabilities in this [documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#foundation_model_apis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b138962e-6942-47f7-986b-d4a8502361a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VertexAIEmbeddings(client=<vertexai.language_models.TextEmbeddingModel object at 0x7f77bbc18a90>, project='propane-crawler-363311', location='us-central1', request_parallelism=5, max_retries=6, stop=None, model_name='textembedding-gecko@003', client_preview=None, temperature=None, max_output_tokens=None, top_p=None, top_k=None, credentials=None, n=1, streaming=False, safety_settings=None, api_transport=None, api_endpoint=None, instance={'max_batch_size': 250, 'batch_size': 250, 'min_batch_size': 5, 'min_good_batch_size': 5, 'lock': <unlocked _thread.lock object at 0x7f77c36a9d40>, 'batch_size_validated': False, 'task_executor': <concurrent.futures.thread.ThreadPoolExecutor object at 0x7f77bbc18970>, 'embeddings_task_type_supported': True, 'get_embeddings_with_retry': <function TextEmbeddingModel.get_embeddings at 0x7f77b94be9e0>})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Text Embedding\n",
    "text_embedding_model = VertexAIEmbeddings(\n",
    "    model_name=TEXT_EMBEDDING_MODEL_NAME,\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    max_retries=6\n",
    ")\n",
    "\n",
    "text_embedding_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbaf897-cdb3-4c6d-a43b-681c04bf597c",
   "metadata": {},
   "source": [
    "#### 3.2.3. Create a VectorSearch Streaming Index\n",
    "\n",
    "In this step, we'll create the Vector Search Index.\n",
    "\n",
    "For more details, refer to the Create and Manage Index: https://cloud.google.com/vertex-ai/docs/vector-search/create-manage-index documentation.\n",
    "Vector Store supports two index types:\n",
    "\n",
    "- **Batch Updates:** Ideal for less frequent modifications.\n",
    "- **Streaming Updates:** Allows near-real-time additions and queries (our choice for this example)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c370bc-2b4e-41ef-8813-c18b6502bb02",
   "metadata": {},
   "source": [
    "<br>*Let's create a dummy embeddings file required to initialize the Vector Search Index.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8b41b6a-7cf9-403f-85d2-8b46fe157df6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ gsutil cp ../temp/dummy_embedding.json gs://propane-crawler-363311-me-bucket/init_index/dummy_embedding.json\n",
      "Copying file://../temp/dummy_embedding.json [Content-Type=application/json]...\n",
      "/ [1 files][  3.8 KiB/  3.8 KiB]                                                \n",
      "Operation completed over 1 objects/3.8 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "# Create Temp folder\n",
    "! mkdir -p $HOME/temp\n",
    "\n",
    "# Create Dummy Embdeddng Data\n",
    "dummy_embedding = {\"id\": str(uuid.uuid4()), \"embedding\": list(np.zeros(ME_DIMENSIONS))}\n",
    "\n",
    "# Write Dummy Embedding Data to a JSON file\n",
    "with open('../temp/dummy_embedding.json', 'w') as f:\n",
    "    json.dump(dummy_embedding, f)\n",
    "    \n",
    "# Copy the dummy_embedding.json to Cloud Storage Bucket.\n",
    "! set -x && gsutil cp ../temp/dummy_embedding.json gs://{ME_EMBEDDING_GCS_DIR}/init_index/dummy_embedding.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d3c682-2d08-4cef-817b-f99eaaacd857",
   "metadata": {},
   "source": [
    "<br>*Let us now create a Streaming Index*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e706ff3-73e5-43a1-b4b1-dd0c6f50990b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Index fhir_me_index already exists with id projects/884766917846/locations/us-central1/indexes/7340819014102286336\n"
     ]
    }
   ],
   "source": [
    "me_utils = MatchingEngineUtils(PROJECT_ID, REGION, ME_INDEX_NAME)\n",
    "\n",
    "me_index = me_utils.create_index(\n",
    "    embedding_gcs_uri=f'gs://{ME_EMBEDDING_GCS_DIR}/init_index',\n",
    "    dimensions=ME_DIMENSIONS,\n",
    "    index_update_method='streaming',\n",
    "    index_algorithm='tree-ah',\n",
    "    shard_size= ME_SHARD_SIZE,\n",
    "    distance_measure_type=ME_DISTANCE_MEASURE_TYPE,\n",
    "    description=ME_DESCRIPTION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "39b02c8a-f2d7-47ce-aada-d05e84fe1f06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index Details:\n",
      "- Index Name = projects/884766917846/locations/us-central1/indexes/7340819014102286336\n",
      "- Update Method = STREAM_UPDATE\n",
      "- Dimensions = 768.0\n",
      "- Shard Size = SHARD_SIZE_SMALL\n",
      "- Distance Measure Type = DOT_PRODUCT_DISTANCE\n",
      "- Algorithm = treeAhConfig\n"
     ]
    }
   ],
   "source": [
    "# Get information about the Index\n",
    "if me_index:\n",
    "    index_metadata = MessageToDict(me_index._pb)\n",
    "    print('Index Details:')\n",
    "    print(f'- Index Name = {index_metadata[\"name\"]}')\n",
    "    print(f'- Update Method = {index_metadata[\"indexUpdateMethod\"]}')\n",
    "    print(f'- Dimensions = {index_metadata[\"metadata\"][\"config\"][\"dimensions\"]}')\n",
    "    print(f'- Shard Size = {index_metadata[\"metadata\"][\"config\"][\"shardSize\"]}')\n",
    "    print(f'- Distance Measure Type = {index_metadata[\"metadata\"][\"config\"][\"distanceMeasureType\"]}')\n",
    "    algorithm = list(index_metadata[\"metadata\"][\"config\"]['algorithmConfig'].keys())[0]\n",
    "    print(f'- Algorithm = {algorithm}')\n",
    "    # print(f'- Index Stats = {index_metadata[\"indexStats\"]}')\n",
    "\n",
    "# index_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3d7067-4af7-4bec-97d9-8a3ab8f1afc2",
   "metadata": {},
   "source": [
    "#### 3.2.4. Create VectorSearch IndexEndpoint & deploy Index\n",
    "\n",
    "In this step, we'll deploy the Index to a Vector Search Index Endpoint. This endpoint is essential for sending queries to your index. \n",
    "\n",
    "We'll use a [Public endpoint](https://cloud.google.com/vertex-ai/docs/vector-search/deploy-index-public) for this example. \n",
    "\n",
    "To set up a [Private Endpoint](https://cloud.google.com/vertex-ai/docs/vector-search/deploy-index-vpc), please refer to the documentation.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ Important: Deploying an Index to an Endpoint takes time, typically around 15-25 minutes or more. ⚠️</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9269c5c7-0d5a-4011-96ce-9cdd90c2e19e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Index endpoint fhir_me_index-endpoint already exists with resource name as projects/884766917846/locations/us-central1/indexEndpoints/2407125622317907968 and endpoint domain name as 115788813.us-central1-884766917846.vdb.vertexai.goog\n",
      "INFO:root:Skipping deploying Index. Index fhir_me_indexalready deployed with id projects/884766917846/locations/us-central1/indexes/7340819014102286336 to the index endpoint fhir_me_index-endpoint\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "name: \"projects/884766917846/locations/us-central1/indexEndpoints/2407125622317907968\"\n",
       "display_name: \"fhir_me_index-endpoint\"\n",
       "deployed_indexes {\n",
       "  id: \"fhir_me_index_20240429164231\"\n",
       "  index: \"projects/884766917846/locations/us-central1/indexes/7340819014102286336\"\n",
       "  display_name: \"fhir_me_index_20240429164231\"\n",
       "  create_time {\n",
       "    seconds: 1714408951\n",
       "    nanos: 798613000\n",
       "  }\n",
       "  index_sync_time {\n",
       "    seconds: 1714409921\n",
       "    nanos: 439565000\n",
       "  }\n",
       "  deployment_group: \"default\"\n",
       "  dedicated_resources {\n",
       "    machine_spec {\n",
       "      machine_type: \"e2-standard-2\"\n",
       "    }\n",
       "    min_replica_count: 2\n",
       "    max_replica_count: 10\n",
       "  }\n",
       "}\n",
       "etag: \"AMEw9yOdCMM98NCY6uFJmdgw9gRbaAZFD3fXhYNcsVEsFZKyfjtxeVjRDxSQbdFuVfw=\"\n",
       "create_time {\n",
       "  seconds: 1714408891\n",
       "  nanos: 533732000\n",
       "}\n",
       "update_time {\n",
       "  seconds: 1714408892\n",
       "  nanos: 155730000\n",
       "}\n",
       "public_endpoint_domain_name: \"115788813.us-central1-884766917846.vdb.vertexai.goog\"\n",
       "encryption_spec {\n",
       "}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "me_endpoint = me_utils.deploy_index(\n",
    "    machine_type=ME_ENDPOINT_MACHINE_TYPE,\n",
    "    min_replica_count=ME_ENDPOINT_MIN_REPLICA_COUNT,\n",
    "    max_replica_count=ME_ENDPOINT_MAX_REPLICA_COUNT,\n",
    "    public_endpoint_enabled=True\n",
    ")\n",
    "\n",
    "me_endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ac68444a-2d3d-427e-91df-3617c7085520",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Name: projects/884766917846/locations/us-central1/indexEndpoints/2407125622317907968\n",
      "Endpoint Public Domain Name: 115788813.us-central1-884766917846.vdb.vertexai.goog\n",
      "Deployed indexes on the endpoint:\n",
      "    - Deployed Indexe ID = fhir_me_index_20240429164231\n",
      "      Machine Type = e2-standard-2\n",
      "      Min Replica Count = 2\n",
      "      Max Replica Count = 10\n"
     ]
    }
   ],
   "source": [
    "if me_endpoint:\n",
    "    endpoint_metadata = MessageToDict(me_endpoint._pb)\n",
    "    print(f'Endpoint Name: {endpoint_metadata[\"name\"]}')\n",
    "    print(f'Endpoint Public Domain Name: {endpoint_metadata[\"publicEndpointDomainName\"]}')\n",
    "    print('Deployed indexes on the endpoint:')\n",
    "    \n",
    "    for d in me_endpoint.deployed_indexes:\n",
    "        print(f'    - Deployed Indexe ID = {d.id}')\n",
    "        print(f'      Machine Type = {d.dedicated_resources.machine_spec.machine_type}')\n",
    "        print(f'      Min Replica Count = {d.dedicated_resources.min_replica_count}')\n",
    "        print(f'      Max Replica Count = {d.dedicated_resources.max_replica_count}')\n",
    "        \n",
    "# endpoint_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce235f8d-787b-4a32-8ef3-d0d6696b4293",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- ME_INDEX_ID:projects/884766917846/locations/us-central1/indexes/7340819014102286336\n",
      "- ME_INDEX_ENDPOINT_ID:projects/884766917846/locations/us-central1/indexEndpoints/2407125622317907968\n"
     ]
    }
   ],
   "source": [
    "# Get Matching Engine Index id and Endpoint id\n",
    "me_utils = MatchingEngineUtils(PROJECT_ID, REGION, ME_INDEX_NAME)\n",
    "ME_INDEX_ID, ME_INDEX_ENDPOINT_ID = me_utils.get_index_and_endpoint()\n",
    "\n",
    "print(f'- ME_INDEX_ID:{ME_INDEX_ID}\\n- ME_INDEX_ENDPOINT_ID:{ME_INDEX_ENDPOINT_ID}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef47d7b-ea3f-49d9-b2a0-4c45ab5df130",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3.2.5 Create a LangChain VectorStore instance of VertexAI VectorSearch\n",
    "In this section we create `VectorSearchVectorStore` Object and connect it to the Index & Endpoint We just created.\n",
    "We will also write and retrieve test data to test the connection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cf3aac9-3059-4ccb-9705-d079f750d4f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_google_vertexai.vectorstores.vectorstores.VectorSearchVectorStore at 0x7f77b94dbb50>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store = VectorSearchVectorStore.from_components(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=REGION,\n",
    "    gcs_bucket_name=f\"gs://{ME_EMBEDDING_GCS_DIR}\".split(\"/\")[2],\n",
    "    index_id=ME_INDEX_ID,\n",
    "    endpoint_id=ME_INDEX_ENDPOINT_ID,\n",
    "    stream_update=True,\n",
    "    embedding=text_embedding_model\n",
    ")\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95bd600-4223-4e23-8cf7-342eec6b39ac",
   "metadata": {},
   "source": [
    "#### 3.2.6. Testing VertexAI VectorSearch\n",
    "\n",
    "We use VertexAI VectorSearch to retrieve relevant FHIR data based on the user's query. The search incorporates:\n",
    "- **Semantic Matching**: Finding Resources with similar text embeddings to the query.\n",
    "- **Metadata Filters**: Narrowing down results based on patient_id and resource_type to ensure relevance. (Refer to the Filter vector matches: https://cloud.google.com/vertex-ai/docs/vector-search/filtering documentation.)\n",
    "\n",
    "A RAG based LLM prompt consist of 3 sections:\n",
    "- **Instructions**: Guidance for the LLM on how to generate an accurate and relevant response based on the retrieved context and the user query.\n",
    "- **Context**: We also apply filters based on patient_id and resource_type to ensure the retrieved context is relevant to the specific patient and the type of information sought. This is a key step, as the accuracy of the LLM's response directly depends on the relevance of this retrieved context. \n",
    "- **User Question**: The original query posed by the user.\n",
    "\n",
    "For Demonstration purpose we will:\n",
    "- First query VectorSearch without Filter.\n",
    "- Next query VectorSearch with Filters. Refer to VectoSearch [Filter vector matches](https://cloud.google.com/vertex-ai/docs/vector-search/filtering) documentation for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276b55f8-255f-450c-b4ff-7ba5c4c6c098",
   "metadata": {},
   "source": [
    "<br>*First, let us add sample data to vectorSearch*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b37b9ea-a31a-415b-a7d4-3cfe3a12ea53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Sample Data to Ingest\n",
    "resource_text_metadatas = [\n",
    "    {\n",
    "    \"fhir_patient_id\":\"pid_111111111\",\n",
    "    \"fhir_resource_id\":\"rid_111111111\",\n",
    "    \"fhir_resource_type\":\"Test_Resource_type\",\n",
    "    \"neo4j_node_id\": \"nid_111111111\"\n",
    "    },\n",
    "    {\n",
    "    \"fhir_patient_id\":\"pid_222222222\",\n",
    "    \"fhir_resource_id\":\"rid_222222222\",\n",
    "    \"fhir_resource_type\":\"Test_Resource_type\",\n",
    "    \"neo4j_node_id\": \"nid_222222222\"\n",
    "    },\n",
    "    {\n",
    "    \"fhir_patient_id\":\"pid_333333333\",\n",
    "    \"fhir_resource_id\":\"rid_333333333\",\n",
    "    \"fhir_resource_type\":\"Test_Resource_type\",\n",
    "    \"neo4j_node_id\": \"nid_333333333\"\n",
    "    }  \n",
    "]\n",
    "\n",
    "resource_texts = ['This is a sample Resource Type', 'This is a sample Patient Type', 'This is a sample Car Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4c084756-0468-4e64-a552-2d9b49d10049",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserting datapoints MatchingEngineIndex index: projects/884766917846/locations/us-central1/indexes/7340819014102286336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.matching_engine.matching_engine_index:Upserting datapoints MatchingEngineIndex index: projects/884766917846/locations/us-central1/indexes/7340819014102286336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MatchingEngineIndex index Upserted datapoints. Resource name: projects/884766917846/locations/us-central1/indexes/7340819014102286336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.matching_engine.matching_engine_index:MatchingEngineIndex index Upserted datapoints. Resource name: projects/884766917846/locations/us-central1/indexes/7340819014102286336\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['18072f8d-bdfb-49b6-b5de-9f2856fd30f3',\n",
       " 'fc2e1c4b-d1e2-4301-bae1-85043a0355a2',\n",
       " '9a2790d3-130c-4182-b1e9-4f60694c4b68']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write Sample Data to VectorSearch\n",
    "ids = vector_store.add_texts(texts=resource_texts, \n",
    "                             metadatas=resource_text_metadatas, \n",
    "                             is_complete_overwrite=True)\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5beeff7a-d39b-46e9-a9a7-a4a6b21fe76a",
   "metadata": {},
   "source": [
    "<br>*Query VectorSearch without Filter*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7dee9c72-da94-43ec-b23f-7550c4781ee2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='This is a sample Resource Type', metadata={'fhir_patient_id': 'pid_111111111', 'fhir_resource_id': 'rid_111111111', 'fhir_resource_type': 'Test_Resource_type', 'neo4j_node_id': 'nid_111111111'}),\n",
       "  0.7039443254470825),\n",
       " (Document(page_content=\"The type of information in this entry is observation. The status for this observation is final. The category of this observation is Survey. The code for this observation is Protocol for Responding to and Assessing Patients' Assets, Risks, and Experiences [PRAPARE]. This observation was effective date time on 09/01/2015 at 05:25:14. This observation was issued on 09/01/2015 at 05:25:14. This observation contains 21 components. The 1st component's code for this observation is Within the last year, have you been afraid of your partner or ex-partner?. The 1st component's value codeable concept for this observation is No. The 2nd component's code for this observation is Do you feel physically and emotionally safe where you currently live?. The 2nd component's value codeable concept for this observation is No. The 3rd component's code for this observation is Are you a refugee?. The 3rd component's value codeable concept for this observation is No. The 4th component's code for this observation is In the past year, have you spent more than 2 nights in a row in a jail, prison, detention center, or juvenile correctional facility?. The 4th component's value codeable concept for this observation is Yes. The 5th component's code for this observation is Stress level. The 5th component's value codeable concept for this observation is A little bit. The 6th component's code for this observation is How often do you see or talk to people that you care about and feel close to (For example: talking to friends on the phone, visiting friends or family, going to church or club meetings)?. The 6th component's value codeable concept for this observation is 5 or more times a week. The 7th component's code for this observation is Has lack of transportation kept you from medical appointments, meetings, work, or from getting things needed for daily living. The 7th component's value codeable concept for this observation is No. The 8th component's code for this observation is In the past year, have you or any family members you live with been unable to get any of the following when it was really needed?. The 8th component's value codeable concept for this observation is I choose not to answer this question. The 9th component's code for this observation is What was your best estimate of the total income of all family members from all sources, before taxes, in last year?. The 9th component's value quantity for this observation is 82383 /a. The 10th component's code for this observation is Primary insurance. The 10th component's value codeable concept for this observation is Medicare. The 11th component's code for this observation is Employment status - current. The 11th component's value codeable concept for this observation is Full-time work. The 12th component's code for this observation is Highest level of education. The 12th component's value codeable concept for this observation is More than high school. The 13th component's code for this observation is Address. The 13th component's value string for this observation is 1041 Reichert Pathway Apt 55 The 14th component's code for this observation is Are you worried about losing your housing?. The 14th component's value codeable concept for this observation is No. The 15th component's code for this observation is Housing status. The 15th component's value codeable concept for this observation is I have housing. The 16th component's code for this observation is How many people are living or staying at this address?. The 16th component's value quantity for this observation is 3 {#}. The 17th component's code for this observation is Preferred language. The 17th component's value codeable concept for this observation is English. The 18th component's code for this observation is Have you been discharged from the armed forces of the United States?. The 18th component's value codeable concept for this observation is Yes. The 19th component's code for this observation is At any point in the past 2 years, has season or migrant farm work been your or your family's main source of income?. The 19th component's value codeable concept for this observation is No. The 20th component's code for this observation is Race. The 20th component's value codeable concept for this observation is Black/African American. The 21st component's code for this observation is Do you consider yourself Hispanic/Latino?. The 21st component's value codeable concept for this observation is No.\", metadata={'fhir_patient_id': ['8865b6f3-9074-2a99-ee6f-1595444dcb11'], 'fhir_resource_id': ['36dbea63-2037-8b2e-8197-486572e6fd44'], 'fhir_resource_type': ['Observation'], 'neo4j_node_id': ['4:19fee34a-16b7-4325-ac38-466e532042ee:30930']}),\n",
       "  0.6175133585929871),\n",
       " (Document(page_content=\"The type of information in this entry is observation. The status for this observation is final. The category of this observation is Survey. The code for this observation is Protocol for Responding to and Assessing Patients' Assets, Risks, and Experiences [PRAPARE]. This observation was effective date time on 05/14/2019 at 05:17:57. This observation was issued on 05/14/2019 at 05:17:57. This observation contains 21 components. The 1st component's code for this observation is Within the last year, have you been afraid of your partner or ex-partner?. The 1st component's value codeable concept for this observation is No. The 2nd component's code for this observation is Do you feel physically and emotionally safe where you currently live?. The 2nd component's value codeable concept for this observation is Yes. The 3rd component's code for this observation is Are you a refugee?. The 3rd component's value codeable concept for this observation is No. The 4th component's code for this observation is In the past year, have you spent more than 2 nights in a row in a jail, prison, detention center, or juvenile correctional facility?. The 4th component's value codeable concept for this observation is Yes. The 5th component's code for this observation is Stress level. The 5th component's value codeable concept for this observation is A little bit. The 6th component's code for this observation is How often do you see or talk to people that you care about and feel close to (For example: talking to friends on the phone, visiting friends or family, going to church or club meetings)?. The 6th component's value codeable concept for this observation is Less than once a week. The 7th component's code for this observation is Has lack of transportation kept you from medical appointments, meetings, work, or from getting things needed for daily living. The 7th component's value codeable concept for this observation is No. The 8th component's code for this observation is In the past year, have you or any family members you live with been unable to get any of the following when it was really needed?. The 8th component's value codeable concept for this observation is Utilities. The 9th component's code for this observation is What was your best estimate of the total income of all family members from all sources, before taxes, in last year?. The 9th component's value quantity for this observation is 82383 /a. The 10th component's code for this observation is Primary insurance. The 10th component's value codeable concept for this observation is Medicare. The 11th component's code for this observation is Employment status - current. The 11th component's value codeable concept for this observation is Full-time work. The 12th component's code for this observation is Highest level of education. The 12th component's value codeable concept for this observation is More than high school. The 13th component's code for this observation is Address. The 13th component's value string for this observation is 1041 Reichert Pathway Apt 55 The 14th component's code for this observation is Are you worried about losing your housing?. The 14th component's value codeable concept for this observation is No. The 15th component's code for this observation is Housing status. The 15th component's value codeable concept for this observation is I have housing. The 16th component's code for this observation is How many people are living or staying at this address?. The 16th component's value quantity for this observation is 3 {#}. The 17th component's code for this observation is Preferred language. The 17th component's value codeable concept for this observation is English. The 18th component's code for this observation is Have you been discharged from the armed forces of the United States?. The 18th component's value codeable concept for this observation is Yes. The 19th component's code for this observation is At any point in the past 2 years, has season or migrant farm work been your or your family's main source of income?. The 19th component's value codeable concept for this observation is No. The 20th component's code for this observation is Race. The 20th component's value codeable concept for this observation is Black/African American. The 21st component's code for this observation is Do you consider yourself Hispanic/Latino?. The 21st component's value codeable concept for this observation is No.\", metadata={'fhir_patient_id': ['8865b6f3-9074-2a99-ee6f-1595444dcb11'], 'fhir_resource_id': ['8ffa63dd-d5f2-89b9-7f20-e06fefa641c3'], 'fhir_resource_type': ['Observation'], 'neo4j_node_id': ['4:19fee34a-16b7-4325-ac38-466e532042ee:35403']}),\n",
       "  0.6174685955047607),\n",
       " (Document(page_content=\"The type of information in this entry is observation. The status for this observation is final. The category of this observation is Survey. The code for this observation is Protocol for Responding to and Assessing Patients' Assets, Risks, and Experiences [PRAPARE]. This observation was effective date time on 05/02/2017 at 05:11:20. This observation was issued on 05/02/2017 at 05:11:20. This observation contains 21 components. The 1st component's code for this observation is Within the last year, have you been afraid of your partner or ex-partner?. The 1st component's value codeable concept for this observation is No. The 2nd component's code for this observation is Do you feel physically and emotionally safe where you currently live?. The 2nd component's value codeable concept for this observation is Yes. The 3rd component's code for this observation is Are you a refugee?. The 3rd component's value codeable concept for this observation is No. The 4th component's code for this observation is In the past year, have you spent more than 2 nights in a row in a jail, prison, detention center, or juvenile correctional facility?. The 4th component's value codeable concept for this observation is Yes. The 5th component's code for this observation is Stress level. The 5th component's value codeable concept for this observation is A little bit. The 6th component's code for this observation is How often do you see or talk to people that you care about and feel close to (For example: talking to friends on the phone, visiting friends or family, going to church or club meetings)?. The 6th component's value codeable concept for this observation is I choose not to answer this question. The 7th component's code for this observation is Has lack of transportation kept you from medical appointments, meetings, work, or from getting things needed for daily living. The 7th component's value codeable concept for this observation is No. The 8th component's code for this observation is In the past year, have you or any family members you live with been unable to get any of the following when it was really needed?. The 8th component's value codeable concept for this observation is I choose not to answer this question. The 9th component's code for this observation is What was your best estimate of the total income of all family members from all sources, before taxes, in last year?. The 9th component's value quantity for this observation is 82383 /a. The 10th component's code for this observation is Primary insurance. The 10th component's value codeable concept for this observation is Medicare. The 11th component's code for this observation is Employment status - current. The 11th component's value codeable concept for this observation is Full-time work. The 12th component's code for this observation is Highest level of education. The 12th component's value codeable concept for this observation is More than high school. The 13th component's code for this observation is Address. The 13th component's value string for this observation is 1041 Reichert Pathway Apt 55 The 14th component's code for this observation is Are you worried about losing your housing?. The 14th component's value codeable concept for this observation is No. The 15th component's code for this observation is Housing status. The 15th component's value codeable concept for this observation is I have housing. The 16th component's code for this observation is How many people are living or staying at this address?. The 16th component's value quantity for this observation is 3 {#}. The 17th component's code for this observation is Preferred language. The 17th component's value codeable concept for this observation is English. The 18th component's code for this observation is Have you been discharged from the armed forces of the United States?. The 18th component's value codeable concept for this observation is Yes. The 19th component's code for this observation is At any point in the past 2 years, has season or migrant farm work been your or your family's main source of income?. The 19th component's value codeable concept for this observation is No. The 20th component's code for this observation is Race. The 20th component's value codeable concept for this observation is Black/African American. The 21st component's code for this observation is Do you consider yourself Hispanic/Latino?. The 21st component's value codeable concept for this observation is No.\", metadata={'fhir_patient_id': ['8865b6f3-9074-2a99-ee6f-1595444dcb11'], 'fhir_resource_id': ['9b67e785-e10b-a1b6-f5c0-806f87ce4b97'], 'fhir_resource_type': ['Observation'], 'neo4j_node_id': ['4:19fee34a-16b7-4325-ac38-466e532042ee:34566']}),\n",
       "  0.6173504590988159),\n",
       " (Document(page_content=\"The type of information in this entry is observation. The status for this observation is final. The category of this observation is Survey. The code for this observation is Protocol for Responding to and Assessing Patients' Assets, Risks, and Experiences [PRAPARE]. This observation was effective date time on 04/15/2014 at 05:25:50. This observation was issued on 04/15/2014 at 05:25:50. This observation contains 21 components. The 1st component's code for this observation is Within the last year, have you been afraid of your partner or ex-partner?. The 1st component's value codeable concept for this observation is No. The 2nd component's code for this observation is Do you feel physically and emotionally safe where you currently live?. The 2nd component's value codeable concept for this observation is Yes. The 3rd component's code for this observation is Are you a refugee?. The 3rd component's value codeable concept for this observation is No. The 4th component's code for this observation is In the past year, have you spent more than 2 nights in a row in a jail, prison, detention center, or juvenile correctional facility?. The 4th component's value codeable concept for this observation is Yes. The 5th component's code for this observation is Stress level. The 5th component's value codeable concept for this observation is A little bit. The 6th component's code for this observation is How often do you see or talk to people that you care about and feel close to (For example: talking to friends on the phone, visiting friends or family, going to church or club meetings)?. The 6th component's value codeable concept for this observation is 3 to 5 times a week. The 7th component's code for this observation is Has lack of transportation kept you from medical appointments, meetings, work, or from getting things needed for daily living. The 7th component's value codeable concept for this observation is No. The 8th component's code for this observation is In the past year, have you or any family members you live with been unable to get any of the following when it was really needed?. The 8th component's value codeable concept for this observation is I choose not to answer this question. The 9th component's code for this observation is What was your best estimate of the total income of all family members from all sources, before taxes, in last year?. The 9th component's value quantity for this observation is 82383 /a. The 10th component's code for this observation is Primary insurance. The 10th component's value codeable concept for this observation is Medicare. The 11th component's code for this observation is Employment status - current. The 11th component's value codeable concept for this observation is Full-time work. The 12th component's code for this observation is Highest level of education. The 12th component's value codeable concept for this observation is More than high school. The 13th component's code for this observation is Address. The 13th component's value string for this observation is 1041 Reichert Pathway Apt 55 The 14th component's code for this observation is Are you worried about losing your housing?. The 14th component's value codeable concept for this observation is No. The 15th component's code for this observation is Housing status. The 15th component's value codeable concept for this observation is I have housing. The 16th component's code for this observation is How many people are living or staying at this address?. The 16th component's value quantity for this observation is 3 {#}. The 17th component's code for this observation is Preferred language. The 17th component's value codeable concept for this observation is English. The 18th component's code for this observation is Have you been discharged from the armed forces of the United States?. The 18th component's value codeable concept for this observation is Yes. The 19th component's code for this observation is At any point in the past 2 years, has season or migrant farm work been your or your family's main source of income?. The 19th component's value codeable concept for this observation is No. The 20th component's code for this observation is Race. The 20th component's value codeable concept for this observation is Black/African American. The 21st component's code for this observation is Do you consider yourself Hispanic/Latino?. The 21st component's value codeable concept for this observation is No.\", metadata={'fhir_patient_id': ['8865b6f3-9074-2a99-ee6f-1595444dcb11'], 'fhir_resource_id': ['206cc017-31bf-a777-a714-de4b8cbdcba8'], 'fhir_resource_type': ['Observation'], 'neo4j_node_id': ['4:19fee34a-16b7-4325-ac38-466e532042ee:27933']}),\n",
       "  0.6172276735305786),\n",
       " (Document(page_content=\"The type of information in this entry is observation. The status for this observation is final. The category of this observation is Survey. The code for this observation is Protocol for Responding to and Assessing Patients' Assets, Risks, and Experiences [PRAPARE]. This observation was effective date time on 08/01/2017 at 05:10:58. This observation was issued on 08/01/2017 at 05:10:58. This observation contains 21 components. The 1st component's code for this observation is Within the last year, have you been afraid of your partner or ex-partner?. The 1st component's value codeable concept for this observation is No. The 2nd component's code for this observation is Do you feel physically and emotionally safe where you currently live?. The 2nd component's value codeable concept for this observation is Yes. The 3rd component's code for this observation is Are you a refugee?. The 3rd component's value codeable concept for this observation is No. The 4th component's code for this observation is In the past year, have you spent more than 2 nights in a row in a jail, prison, detention center, or juvenile correctional facility?. The 4th component's value codeable concept for this observation is Yes. The 5th component's code for this observation is Stress level. The 5th component's value codeable concept for this observation is A little bit. The 6th component's code for this observation is How often do you see or talk to people that you care about and feel close to (For example: talking to friends on the phone, visiting friends or family, going to church or club meetings)?. The 6th component's value codeable concept for this observation is 5 or more times a week. The 7th component's code for this observation is Has lack of transportation kept you from medical appointments, meetings, work, or from getting things needed for daily living. The 7th component's value codeable concept for this observation is No. The 8th component's code for this observation is In the past year, have you or any family members you live with been unable to get any of the following when it was really needed?. The 8th component's value codeable concept for this observation is I choose not to answer this question. The 9th component's code for this observation is What was your best estimate of the total income of all family members from all sources, before taxes, in last year?. The 9th component's value quantity for this observation is 82383 /a. The 10th component's code for this observation is Primary insurance. The 10th component's value codeable concept for this observation is Medicare. The 11th component's code for this observation is Employment status - current. The 11th component's value codeable concept for this observation is Full-time work. The 12th component's code for this observation is Highest level of education. The 12th component's value codeable concept for this observation is More than high school. The 13th component's code for this observation is Address. The 13th component's value string for this observation is 1041 Reichert Pathway Apt 55 The 14th component's code for this observation is Are you worried about losing your housing?. The 14th component's value codeable concept for this observation is No. The 15th component's code for this observation is Housing status. The 15th component's value codeable concept for this observation is I have housing. The 16th component's code for this observation is How many people are living or staying at this address?. The 16th component's value quantity for this observation is 3 {#}. The 17th component's code for this observation is Preferred language. The 17th component's value codeable concept for this observation is English. The 18th component's code for this observation is Have you been discharged from the armed forces of the United States?. The 18th component's value codeable concept for this observation is Yes. The 19th component's code for this observation is At any point in the past 2 years, has season or migrant farm work been your or your family's main source of income?. The 19th component's value codeable concept for this observation is No. The 20th component's code for this observation is Race. The 20th component's value codeable concept for this observation is Black/African American. The 21st component's code for this observation is Do you consider yourself Hispanic/Latino?. The 21st component's value codeable concept for this observation is No.\", metadata={'fhir_patient_id': ['8865b6f3-9074-2a99-ee6f-1595444dcb11'], 'fhir_resource_id': ['dc9836f3-dd4c-8924-3843-de8e52c2c242'], 'fhir_resource_type': ['Observation'], 'neo4j_node_id': ['4:19fee34a-16b7-4325-ac38-466e532042ee:35167']}),\n",
       "  0.6171119809150696),\n",
       " (Document(page_content=\"The type of information in this entry is observation. The status for this observation is final. The category of this observation is Survey. The code for this observation is Protocol for Responding to and Assessing Patients' Assets, Risks, and Experiences [PRAPARE]. This observation was effective date time on 07/18/2017 at 05:06:47. This observation was issued on 07/18/2017 at 05:06:47. This observation contains 21 components. The 1st component's code for this observation is Within the last year, have you been afraid of your partner or ex-partner?. The 1st component's value codeable concept for this observation is No. The 2nd component's code for this observation is Do you feel physically and emotionally safe where you currently live?. The 2nd component's value codeable concept for this observation is Yes. The 3rd component's code for this observation is Are you a refugee?. The 3rd component's value codeable concept for this observation is No. The 4th component's code for this observation is In the past year, have you spent more than 2 nights in a row in a jail, prison, detention center, or juvenile correctional facility?. The 4th component's value codeable concept for this observation is Yes. The 5th component's code for this observation is Stress level. The 5th component's value codeable concept for this observation is Not at all. The 6th component's code for this observation is How often do you see or talk to people that you care about and feel close to (For example: talking to friends on the phone, visiting friends or family, going to church or club meetings)?. The 6th component's value codeable concept for this observation is 3 to 5 times a week. The 7th component's code for this observation is Has lack of transportation kept you from medical appointments, meetings, work, or from getting things needed for daily living. The 7th component's value codeable concept for this observation is No. The 8th component's code for this observation is In the past year, have you or any family members you live with been unable to get any of the following when it was really needed?. The 8th component's value codeable concept for this observation is Utilities. The 9th component's code for this observation is What was your best estimate of the total income of all family members from all sources, before taxes, in last year?. The 9th component's value quantity for this observation is 82383 /a. The 10th component's code for this observation is Primary insurance. The 10th component's value codeable concept for this observation is Medicare. The 11th component's code for this observation is Employment status - current. The 11th component's value codeable concept for this observation is Full-time work. The 12th component's code for this observation is Highest level of education. The 12th component's value codeable concept for this observation is More than high school. The 13th component's code for this observation is Address. The 13th component's value string for this observation is 1041 Reichert Pathway Apt 55 The 14th component's code for this observation is Are you worried about losing your housing?. The 14th component's value codeable concept for this observation is No. The 15th component's code for this observation is Housing status. The 15th component's value codeable concept for this observation is I have housing. The 16th component's code for this observation is How many people are living or staying at this address?. The 16th component's value quantity for this observation is 3 {#}. The 17th component's code for this observation is Preferred language. The 17th component's value codeable concept for this observation is English. The 18th component's code for this observation is Have you been discharged from the armed forces of the United States?. The 18th component's value codeable concept for this observation is Yes. The 19th component's code for this observation is At any point in the past 2 years, has season or migrant farm work been your or your family's main source of income?. The 19th component's value codeable concept for this observation is No. The 20th component's code for this observation is Race. The 20th component's value codeable concept for this observation is Black/African American. The 21st component's code for this observation is Do you consider yourself Hispanic/Latino?. The 21st component's value codeable concept for this observation is No.\", metadata={'fhir_patient_id': ['8865b6f3-9074-2a99-ee6f-1595444dcb11'], 'fhir_resource_id': ['6682c0a0-4f76-75ee-b53a-648d2f7de076'], 'fhir_resource_type': ['Observation'], 'neo4j_node_id': ['4:19fee34a-16b7-4325-ac38-466e532042ee:35100']}),\n",
       "  0.6170186400413513),\n",
       " (Document(page_content=\"The type of information in this entry is observation. The status for this observation is final. The category of this observation is Survey. The code for this observation is Protocol for Responding to and Assessing Patients' Assets, Risks, and Experiences [PRAPARE]. This observation was effective date time on 05/08/2018 at 05:02:37. This observation was issued on 05/08/2018 at 05:02:37. This observation contains 21 components. The 1st component's code for this observation is Within the last year, have you been afraid of your partner or ex-partner?. The 1st component's value codeable concept for this observation is No. The 2nd component's code for this observation is Do you feel physically and emotionally safe where you currently live?. The 2nd component's value codeable concept for this observation is No. The 3rd component's code for this observation is Are you a refugee?. The 3rd component's value codeable concept for this observation is No. The 4th component's code for this observation is In the past year, have you spent more than 2 nights in a row in a jail, prison, detention center, or juvenile correctional facility?. The 4th component's value codeable concept for this observation is Yes. The 5th component's code for this observation is Stress level. The 5th component's value codeable concept for this observation is Not at all. The 6th component's code for this observation is How often do you see or talk to people that you care about and feel close to (For example: talking to friends on the phone, visiting friends or family, going to church or club meetings)?. The 6th component's value codeable concept for this observation is 1 or 2 times a week. The 7th component's code for this observation is Has lack of transportation kept you from medical appointments, meetings, work, or from getting things needed for daily living. The 7th component's value codeable concept for this observation is No. The 8th component's code for this observation is In the past year, have you or any family members you live with been unable to get any of the following when it was really needed?. The 8th component's value codeable concept for this observation is Medicine or Any Health Care (Medical, Dental, Mental Health, Vision). The 9th component's code for this observation is What was your best estimate of the total income of all family members from all sources, before taxes, in last year?. The 9th component's value quantity for this observation is 82383 /a. The 10th component's code for this observation is Primary insurance. The 10th component's value codeable concept for this observation is Medicare. The 11th component's code for this observation is Employment status - current. The 11th component's value codeable concept for this observation is Part-time or temporary work. The 12th component's code for this observation is Highest level of education. The 12th component's value codeable concept for this observation is More than high school. The 13th component's code for this observation is Address. The 13th component's value string for this observation is 1041 Reichert Pathway Apt 55 The 14th component's code for this observation is Are you worried about losing your housing?. The 14th component's value codeable concept for this observation is No. The 15th component's code for this observation is Housing status. The 15th component's value codeable concept for this observation is I have housing. The 16th component's code for this observation is How many people are living or staying at this address?. The 16th component's value quantity for this observation is 3 {#}. The 17th component's code for this observation is Preferred language. The 17th component's value codeable concept for this observation is English. The 18th component's code for this observation is Have you been discharged from the armed forces of the United States?. The 18th component's value codeable concept for this observation is Yes. The 19th component's code for this observation is At any point in the past 2 years, has season or migrant farm work been your or your family's main source of income?. The 19th component's value codeable concept for this observation is No. The 20th component's code for this observation is Race. The 20th component's value codeable concept for this observation is Black/African American. The 21st component's code for this observation is Do you consider yourself Hispanic/Latino?. The 21st component's value codeable concept for this observation is No.\", metadata={'fhir_patient_id': ['8865b6f3-9074-2a99-ee6f-1595444dcb11'], 'fhir_resource_id': ['797b1a50-4ebe-3225-934f-9ddb331fb4a5'], 'fhir_resource_type': ['Observation'], 'neo4j_node_id': ['4:19fee34a-16b7-4325-ac38-466e532042ee:35231']}),\n",
       "  0.6167582273483276),\n",
       " (Document(page_content=\"The type of information in this entry is observation. The status for this observation is final. The category of this observation is Survey. The code for this observation is Protocol for Responding to and Assessing Patients' Assets, Risks, and Experiences [PRAPARE]. This observation was effective date time on 04/21/2015 at 05:03:38. This observation was issued on 04/21/2015 at 05:03:38. This observation contains 21 components. The 1st component's code for this observation is Within the last year, have you been afraid of your partner or ex-partner?. The 1st component's value codeable concept for this observation is No. The 2nd component's code for this observation is Do you feel physically and emotionally safe where you currently live?. The 2nd component's value codeable concept for this observation is No. The 3rd component's code for this observation is Are you a refugee?. The 3rd component's value codeable concept for this observation is No. The 4th component's code for this observation is In the past year, have you spent more than 2 nights in a row in a jail, prison, detention center, or juvenile correctional facility?. The 4th component's value codeable concept for this observation is Yes. The 5th component's code for this observation is Stress level. The 5th component's value codeable concept for this observation is Somewhat. The 6th component's code for this observation is How often do you see or talk to people that you care about and feel close to (For example: talking to friends on the phone, visiting friends or family, going to church or club meetings)?. The 6th component's value codeable concept for this observation is 5 or more times a week. The 7th component's code for this observation is Has lack of transportation kept you from medical appointments, meetings, work, or from getting things needed for daily living. The 7th component's value codeable concept for this observation is No. The 8th component's code for this observation is In the past year, have you or any family members you live with been unable to get any of the following when it was really needed?. The 8th component's value codeable concept for this observation is I choose not to answer this question. The 9th component's code for this observation is What was your best estimate of the total income of all family members from all sources, before taxes, in last year?. The 9th component's value quantity for this observation is 82383 /a. The 10th component's code for this observation is Primary insurance. The 10th component's value codeable concept for this observation is Medicare. The 11th component's code for this observation is Employment status - current. The 11th component's value codeable concept for this observation is Full-time work. The 12th component's code for this observation is Highest level of education. The 12th component's value codeable concept for this observation is More than high school. The 13th component's code for this observation is Address. The 13th component's value string for this observation is 1041 Reichert Pathway Apt 55 The 14th component's code for this observation is Are you worried about losing your housing?. The 14th component's value codeable concept for this observation is No. The 15th component's code for this observation is Housing status. The 15th component's value codeable concept for this observation is I have housing. The 16th component's code for this observation is How many people are living or staying at this address?. The 16th component's value quantity for this observation is 3 {#}. The 17th component's code for this observation is Preferred language. The 17th component's value codeable concept for this observation is English. The 18th component's code for this observation is Have you been discharged from the armed forces of the United States?. The 18th component's value codeable concept for this observation is Yes. The 19th component's code for this observation is At any point in the past 2 years, has season or migrant farm work been your or your family's main source of income?. The 19th component's value codeable concept for this observation is No. The 20th component's code for this observation is Race. The 20th component's value codeable concept for this observation is Black/African American. The 21st component's code for this observation is Do you consider yourself Hispanic/Latino?. The 21st component's value codeable concept for this observation is No.\", metadata={'fhir_patient_id': ['8865b6f3-9074-2a99-ee6f-1595444dcb11'], 'fhir_resource_id': ['cc319d51-c529-3c83-86d0-1eaeaefaf6c8'], 'fhir_resource_type': ['Observation'], 'neo4j_node_id': ['4:19fee34a-16b7-4325-ac38-466e532042ee:30151']}),\n",
       "  0.6166054010391235),\n",
       " (Document(page_content=\"The type of information in this entry is observation. The status for this observation is final. The category of this observation is Survey. The code for this observation is Protocol for Responding to and Assessing Patients' Assets, Risks, and Experiences [PRAPARE]. This observation was effective date time on 05/19/2020 at 05:17:57. This observation was issued on 05/19/2020 at 05:17:57. This observation contains 21 components. The 1st component's code for this observation is Within the last year, have you been afraid of your partner or ex-partner?. The 1st component's value codeable concept for this observation is No. The 2nd component's code for this observation is Do you feel physically and emotionally safe where you currently live?. The 2nd component's value codeable concept for this observation is Yes. The 3rd component's code for this observation is Are you a refugee?. The 3rd component's value codeable concept for this observation is No. The 4th component's code for this observation is In the past year, have you spent more than 2 nights in a row in a jail, prison, detention center, or juvenile correctional facility?. The 4th component's value codeable concept for this observation is Yes. The 5th component's code for this observation is Stress level. The 5th component's value codeable concept for this observation is Not at all. The 6th component's code for this observation is How often do you see or talk to people that you care about and feel close to (For example: talking to friends on the phone, visiting friends or family, going to church or club meetings)?. The 6th component's value codeable concept for this observation is Less than once a week. The 7th component's code for this observation is Has lack of transportation kept you from medical appointments, meetings, work, or from getting things needed for daily living. The 7th component's value codeable concept for this observation is No. The 8th component's code for this observation is In the past year, have you or any family members you live with been unable to get any of the following when it was really needed?. The 8th component's value codeable concept for this observation is I choose not to answer this question. The 9th component's code for this observation is What was your best estimate of the total income of all family members from all sources, before taxes, in last year?. The 9th component's value quantity for this observation is 82383 /a. The 10th component's code for this observation is Primary insurance. The 10th component's value codeable concept for this observation is Medicare. The 11th component's code for this observation is Employment status - current. The 11th component's value codeable concept for this observation is Part-time or temporary work. The 12th component's code for this observation is Highest level of education. The 12th component's value codeable concept for this observation is More than high school. The 13th component's code for this observation is Address. The 13th component's value string for this observation is 1041 Reichert Pathway Apt 55 The 14th component's code for this observation is Are you worried about losing your housing?. The 14th component's value codeable concept for this observation is No. The 15th component's code for this observation is Housing status. The 15th component's value codeable concept for this observation is I have housing. The 16th component's code for this observation is How many people are living or staying at this address?. The 16th component's value quantity for this observation is 3 {#}. The 17th component's code for this observation is Preferred language. The 17th component's value codeable concept for this observation is English. The 18th component's code for this observation is Have you been discharged from the armed forces of the United States?. The 18th component's value codeable concept for this observation is Yes. The 19th component's code for this observation is At any point in the past 2 years, has season or migrant farm work been your or your family's main source of income?. The 19th component's value codeable concept for this observation is No. The 20th component's code for this observation is Race. The 20th component's value codeable concept for this observation is Black/African American. The 21st component's code for this observation is Do you consider yourself Hispanic/Latino?. The 21st component's value codeable concept for this observation is No.\", metadata={'fhir_patient_id': ['8865b6f3-9074-2a99-ee6f-1595444dcb11'], 'fhir_resource_id': ['76e3f8fe-ff8e-5269-518b-96fb59bfa927'], 'fhir_resource_type': ['Observation'], 'neo4j_node_id': ['4:19fee34a-16b7-4325-ac38-466e532042ee:35459']}),\n",
       "  0.6165879964828491)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_text = 'sample Resource'\n",
    "response = vector_store.similarity_search_with_score(query=query_text, k=10)\n",
    "\n",
    "response \n",
    "\n",
    "# Note: The response will likely contain all 3 data points. \n",
    "# Although they have different lexical forms (Sample Resource, Sample Process), \n",
    "# they share the same semantic meaning related to \"sample,\" which is what our\n",
    "# semantic search is designed to capture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f87b44-0b16-4258-81c3-89f00918af6e",
   "metadata": {},
   "source": [
    "<br>*Query VectorSearch with Filters*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0f979bdf-37cb-40c6-a00a-a943f9795e80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='This is a sample Resource Type', metadata={'fhir_patient_id': 'pid_111111111', 'fhir_resource_id': 'rid_111111111', 'fhir_resource_type': 'Test_Resource_type', 'neo4j_node_id': 'nid_111111111'}),\n",
       "  0.7039443254470825)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieving Data from VectorSearch with Filter\n",
    "\n",
    "vs_filter = [Namespace(name=\"fhir_patient_id\", allow_tokens=['pid_111111111'])]\n",
    "response = vector_store.similarity_search_with_score(query=query_text, k=10, filter=vs_filter)\n",
    "response\n",
    "\n",
    "# Note: We now expect a single data point in the results, as we've applied a filter to retrieve\n",
    "# only the resource associated with the specific patient ID ('fhir_patient_id=pid_111111111'). \n",
    "# This filter ensures we're focusing on information relevant to this particular patient. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e03096b-bc95-4740-a28a-f7ea4efec943",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## 4. Fetch FHIR Synthetic Data\n",
    "\n",
    "Now that we established the prerequiste Infrastructure, let us fetch the sample data.\n",
    "\n",
    "We will be using sample FHIR data from [Synthea](https://synthea.mitre.org/) for the purpose of this demonstration. \n",
    "We will be using the pre-generated data available [here](https://synthetichealth.github.io/synthea-sample-data/downloads/latest/synthea_sample_data_fhir_latest.zip). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cb6154-8bf6-4b94-8960-7dcc55e6d54c",
   "metadata": {},
   "source": [
    "### 4.1. Download synthetic FHIR Data from Synthea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b652574-b870-4d1a-ac40-d31ca2738f2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! mkdir -p $HOME/working\n",
    "\n",
    "! curl  \\\n",
    "--url https://synthetichealth.github.io/synthea-sample-data/downloads/latest/synthea_sample_data_fhir_latest.zip \\\n",
    "--output $HOME/working/synthea_sample_data_fhir_latest.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d86884-d640-4c5b-a00a-f3aadf47afd6",
   "metadata": {},
   "source": [
    "<br>*Unzip the synthea_sample_data_fhir_latest.zip*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a02767-6f2e-4b3a-b583-77c3b654468b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! unzip $HOME/working/synthea_sample_data_fhir_latest.zip -d $HOME/working/bundles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc942558-b2af-4a4e-89b8-b5944f2eef66",
   "metadata": {},
   "source": [
    "### 4.2. Taking Inventory of FHIR Files\n",
    "\n",
    "<br>*Let us look at the attributes of the FHIR Data we are going to ingest.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a51c2e20-ebc2-413b-969e-dde035111f1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of FHIR Files that would be ingested: 20\n"
     ]
    }
   ],
   "source": [
    "# Feth all FHIR Files\n",
    "fhir_folder_path = '../working/bundles'\n",
    "fhir_files_list = glob.glob(f\"{fhir_folder_path}/*.json\")\n",
    "\n",
    "# Limiting files to ingest\n",
    "fhir_files_list.sort()\n",
    "fhir_files_list = fhir_files_list[:DEMO_FILES_INGEST_LIMIT]\n",
    "\n",
    "\n",
    "num_of_files = len(fhir_files_list)\n",
    "print(f'Number of FHIR Files that would be ingested: {num_of_files}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1a08d65a-8c89-449b-9033-ee5631595480",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resource Types Count:\n",
      "{'AllergyIntolerance': 10,\n",
      " 'CarePlan': 60,\n",
      " 'CareTeam': 60,\n",
      " 'Claim': 3486,\n",
      " 'Condition': 656,\n",
      " 'Device': 39,\n",
      " 'DiagnosticReport': 3100,\n",
      " 'DocumentReference': 1859,\n",
      " 'Encounter': 1859,\n",
      " 'ExplanationOfBenefit': 3486,\n",
      " 'ImagingStudy': 8,\n",
      " 'Immunization': 259,\n",
      " 'Medication': 957,\n",
      " 'MedicationAdministration': 957,\n",
      " 'MedicationRequest': 1627,\n",
      " 'Observation': 13501,\n",
      " 'Patient': 20,\n",
      " 'Procedure': 2966,\n",
      " 'Provenance': 20,\n",
      " 'SupplyDelivery': 239}\n",
      "\n",
      "File Resources Meta:\n",
      "[{'file_name': 'Akiko835_Jacelyn576_Larkin917_05c4608d-bd9a-5d04-41d7-a0293da7f5a5.json',\n",
      "  'resources_count': 1024},\n",
      " {'file_name': 'Anneliese170_Berge125_b2af96cc-588f-c029-3acb-37c9d59bb2b4.json',\n",
      "  'resources_count': 427},\n",
      " {'file_name': 'Annice210_Edris973_McClure239_e1a90b76-c8fb-724a-5053-7355db36519b.json',\n",
      "  'resources_count': 719},\n",
      " {'file_name': 'Anthony633_Renner328_6fabf2a0-6ce7-5cc4-cfd6-2858ef8fdd38.json',\n",
      "  'resources_count': 443},\n",
      " {'file_name': 'Arleen939_Kling921_076a24c5-c7f3-c743-e882-34b41a8af35b.json',\n",
      "  'resources_count': 12140},\n",
      " {'file_name': 'Ashlie138_Slyvia34_Williamson769_d46186a3-507c-0b22-1bfc-1f88bd7cbd7d.json',\n",
      "  'resources_count': 799},\n",
      " {'file_name': 'Avril120_Olson653_b573d409-cfaf-c585-a25b-e9d4cf510bd3.json',\n",
      "  'resources_count': 512},\n",
      " {'file_name': 'Basil991_Hilll811_d0a2790b-e10f-45d3-58e7-365d2ebb3d98.json',\n",
      "  'resources_count': 347},\n",
      " {'file_name': 'Benjamin360_Hintz995_ae9b0221-6ac3-e43a-c3ff-b96a5edc31f0.json',\n",
      "  'resources_count': 487},\n",
      " {'file_name': 'Brendan864_MacGyver246_ded32e6b-fed4-bee1-c697-2e79957caa7a.json',\n",
      "  'resources_count': 480},\n",
      " {'file_name': 'Bula88_Prohaska837_630750f7-4aef-7802-e2aa-0f40eb911ff0.json',\n",
      "  'resources_count': 487},\n",
      " {'file_name': 'Byron202_Rempel203_8b7ada46-ea6a-bc88-99e0-f2d5376828fc.json',\n",
      "  'resources_count': 274},\n",
      " {'file_name': 'Carey440_Stroman228_1d3e046a-bfde-008e-4546-9d9b8184c2bf.json',\n",
      "  'resources_count': 1094},\n",
      " {'file_name': 'Carin553_Vanetta255_Kassulke119_ce91b7b5-7064-8e8e-6327-3d836b2e180a.json',\n",
      "  'resources_count': 627},\n",
      " {'file_name': 'Carol737_Watsica258_c1d8ab2f-c0b3-ea11-9ac8-a39c8123564a.json',\n",
      "  'resources_count': 275},\n",
      " {'file_name': 'Casandra937_Alyce744_Grimes165_50164fde-695f-07dc-9e8b-6c7107fe40c8.json',\n",
      "  'resources_count': 4994},\n",
      " {'file_name': 'Cecille691_Mraz590_8af93b11-1e84-fc86-a5b9-2b33754c01ea.json',\n",
      "  'resources_count': 462},\n",
      " {'file_name': 'Cedrick207_King743_8865b6f3-9074-2a99-ee6f-1595444dcb11.json',\n",
      "  'resources_count': 7714},\n",
      " {'file_name': 'Chet188_Bruen238_29257306-9929-b1f9-49f8-ac43800341fe.json',\n",
      "  'resources_count': 460},\n",
      " {'file_name': 'Christeen33_Carlotta746_Schmitt836_8bf8d9a1-af2a-1dfa-c39a-78f14c3dc387.json',\n",
      "  'resources_count': 1404}]\n",
      "\n",
      "### Summary ###\n",
      "- Number of FHIR Files to process = 20\n",
      "- Total Resources:35169\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_counter = 1\n",
    "total_resources = 0\n",
    "file_resources_meta_list = []\n",
    "resource_types_count = {}\n",
    "\n",
    "for fhir_file in fhir_files_list:\n",
    "    fhir_file_name = os.path.basename(fhir_file)\n",
    "    # print(f'File {file_counter} of {num_of_files}: {fhir_file_name}')\n",
    "    file_counter += 1\n",
    "    \n",
    "    with open(fhir_file) as raw:\n",
    "        bundle = json.load(raw)\n",
    "        resources_entry_list = bundle['entry']\n",
    "            \n",
    "        # Resources Counter\n",
    "        num_of_resources = len(resources_entry_list)\n",
    "        total_resources += num_of_resources\n",
    "        file_resources_meta = {}\n",
    "        file_resources_meta['file_name'] = fhir_file_name\n",
    "        file_resources_meta['resources_count'] = num_of_resources\n",
    "        \n",
    "        file_resources_meta_list.append(file_resources_meta)\n",
    "        \n",
    "        # Count Individual Resource Types\n",
    "        for entry in resources_entry_list:\n",
    "            resource = entry['resource']\n",
    "            resource_type = resource[\"resourceType\"]\n",
    "\n",
    "            if resource_type not in resource_types_count.keys():\n",
    "                # print(f'Creating Dict entry for Resource: {resource_type}')\n",
    "                resource_types_count[resource_type] = 0\n",
    "            \n",
    "            resource_types_count[resource_type] += 1\n",
    "\n",
    "\n",
    "print(f'Resource Types Count:')\n",
    "pprint(resource_types_count)\n",
    "\n",
    "print('\\nFile Resources Meta:')\n",
    "pprint(file_resources_meta_list)\n",
    "\n",
    "print('\\n### Summary ###')\n",
    "print(f'- Number of FHIR Files to process = {num_of_files}')            \n",
    "print(f'- Total Resources:{total_resources}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2eaae4-9ad0-44ee-97c9-a21c93b8814c",
   "metadata": {},
   "source": [
    "## 5. Neo4J Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ba3ea1-acf2-4716-834c-bb115a2707f2",
   "metadata": {},
   "source": [
    "### 5.1. Neo4J Data Loading Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc98a33e-87b7-486b-ac8f-03c0674a6824",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create FHIR Resource nodes in Neo4j\n",
    "def create_resource_node(resource:dict) -> str:\n",
    "    # print(resource_to_node(resource))\n",
    "    node_creation_cypher = resource_to_node(resource)\n",
    "    query_result, runtime = graph.query(node_creation_cypher)\n",
    "    \n",
    "    if len(query_result[0]) != 1:\n",
    "        print(query_result)\n",
    "        raise Exception(\"Resource Node creation query result does not meet defined format.\")\n",
    "       \n",
    "    node_id = query_result[0][0]\n",
    "    return node_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "896709c4-5a2d-4b0c-8a05-2cb90d1cb617",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Date Nodes and Edges between Resource Nodes and Date Nodes\n",
    "def create_date_nodes_edges(resource):\n",
    "    edges = []\n",
    "    dates = set() # set is used here to make sure dates are unique\n",
    "\n",
    "    # generated the cypher for creating the reference & date edges and capture dates\n",
    "    node_edges, node_dates = resource_to_edges(resource)\n",
    "    edges += node_edges\n",
    "    dates.update(node_dates)\n",
    "    \n",
    "    # Create Date Nodes - 'MERGE' Skip if Node exists\n",
    "    for date in dates:\n",
    "        cypher = 'MERGE (n:Date {name:\"' + date + '\", id: \"' + date + '\"})'\n",
    "        graph.query(cypher)\n",
    "    \n",
    "    # Connect Resource Nodes and Date Nodes via Edges \n",
    "    for edge in edges:\n",
    "        try:\n",
    "            graph.query(edge)\n",
    "        except:\n",
    "            print(f'Failed to create edge: {edge}')\n",
    "    \n",
    "    return len(dates), len(edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29f0949-0851-49bc-bbde-abf566020d37",
   "metadata": {},
   "source": [
    "### 5.2. FHIR to Neo4J Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f63d024-d76e-4ff3-be49-2604f55801c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_fhir_neo4j(fhir_files_list):\n",
    "    file_counter = 0\n",
    "    \n",
    "    for fhir_file in fhir_files_list:\n",
    "        \n",
    "        fhir_file_name = os.path.basename(fhir_file)\n",
    "        file_counter += 1\n",
    "        \n",
    "        if file_counter > 1:\n",
    "            print(\"\\n\")\n",
    "        \n",
    "        print(f'File {file_counter} of {num_of_files}: {fhir_file_name}')\n",
    "        \n",
    "        with open(fhir_file) as raw:\n",
    "            bundle = json.load(raw)\n",
    "            resources_entry_list = bundle['entry']\n",
    "            \n",
    "            num_of_resources = len(resources_entry_list)\n",
    "            print(f'    - Number of resources = {num_of_resources}')\n",
    "                \n",
    "            resource_counter = 0\n",
    "            for entry in resources_entry_list:\n",
    "                resource_counter += 1\n",
    "                \n",
    "                resource = entry['resource']\n",
    "                # print(resource)\n",
    "                resource_id = resource[\"id\"]\n",
    "                resource_type = resource[\"resourceType\"]\n",
    "                \n",
    "                # Skip Provenance Resource\n",
    "                if resource_type == 'Provenance':\n",
    "                    continue\n",
    "\n",
    "                    \n",
    "                print(f'    - Processing Resource {resource_counter} of {num_of_resources}: Resource-Type = {resource_type}, Resource-ID = {resource_id}', end=\"\\r\", flush=True)\n",
    "                \n",
    "                #### LOAD DATA INTO NEO4J ####\n",
    "                \n",
    "                # Create Resource Nodes\n",
    "                node_id = create_resource_node(resource)\n",
    "                # print (node_id)\n",
    "                \n",
    "                # Create Date Nodes and Edges to connect Resource Nodes to Date Nodes\n",
    "                date_nodes_count, edges_count = create_date_nodes_edges(resource)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9514fd83-8890-4c11-a3c1-4095691f5f25",
   "metadata": {},
   "source": [
    "### 5.3 Neo4J Data Retrieval Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0ef39f8-c280-4049-852f-d9b52b9664ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_nodes_in_batches(graph, batch_size=1000):\n",
    "    skip = 0\n",
    "    while True:\n",
    "        query_string = f'MATCH (r:resource) RETURN r SKIP {skip} LIMIT {batch_size}'        \n",
    "        # print(query_string)\n",
    "        results = graph.query(query_string)\n",
    "        nodes = results[0]\n",
    "        \n",
    "        if not nodes:\n",
    "            break  # No more nodes to fetch\n",
    "        \n",
    "        yield nodes  # Process this batch\n",
    "        skip += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6f0374c-b87e-4200-838b-f45984df9c89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AllergyIntolerance': 10,\n",
      " 'CarePlan': 60,\n",
      " 'CareTeam': 60,\n",
      " 'Claim': 3486,\n",
      " 'Condition': 656,\n",
      " 'Device': 39,\n",
      " 'DiagnosticReport': 3100,\n",
      " 'DocumentReference': 1859,\n",
      " 'Encounter': 1859,\n",
      " 'ExplanationOfBenefit': 3486,\n",
      " 'ImagingStudy': 8,\n",
      " 'Immunization': 259,\n",
      " 'Medication': 957,\n",
      " 'MedicationAdministration': 957,\n",
      " 'MedicationRequest': 1627,\n",
      " 'Observation': 13501,\n",
      " 'Patient': 20,\n",
      " 'Procedure': 2966,\n",
      " 'SupplyDelivery': 239}\n",
      "Total Resource Nodes = 35149\n"
     ]
    }
   ],
   "source": [
    "# print report on Resource nodes\n",
    "def get_neo4j_resourceNodes_stats():\n",
    "    batch_counter = 0\n",
    "    resource_counter = 0\n",
    "    resources_types_dict = {}\n",
    "\n",
    "    for batch in get_nodes_in_batches(graph):\n",
    "        batch_counter += 1\n",
    "        # print(f'Batch: {batch_counter}, Len of Batch = {len(batch)}')\n",
    "\n",
    "        for node in batch:\n",
    "            resource_counter += 1\n",
    "            resource_type = node[0]['resource_type']\n",
    "\n",
    "            if resource_type not in resources_types_dict.keys():\n",
    "                resources_types_dict[resource_type] = 0\n",
    "\n",
    "            resources_types_dict[resource_type] += 1\n",
    "            # print(node[0])\n",
    "\n",
    "    sorted_dict = dict(sorted(resources_types_dict.items()))\n",
    "    pprint(sorted_dict)\n",
    "    print(f'Total Resource Nodes = {resource_counter}')\n",
    "    \n",
    "get_neo4j_resourceNodes_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30fbc1a-e7f3-4d4b-90d3-e7446d4fc873",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42b7a0e7-2962-490e-86e3-0a780a910a09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set to True to Ingest Data. This is a safety swithc to prevent accidental triggering of Data Ingestion\n",
    "ingest_data = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0b48ce-0bc1-4781-8cf0-79d806cdc0ef",
   "metadata": {},
   "source": [
    "### 6.1. Trigger Neo4J Data Ingestion\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ Important: This process may take several hours to complete depending on the number of files and FHIR resources you are ingesting!⚠️</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c88f40e-7909-402c-b58d-a408021b6c91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set ingest_data to True to ingest data. Currently ingest_data=False\n"
     ]
    }
   ],
   "source": [
    "if ingest_data:\n",
    "    start_time = time.time()\n",
    "    load_fhir_neo4j(fhir_files_list)\n",
    "    end_time = time.time()\n",
    "\n",
    "    total_duration = str(datetime.timedelta(seconds = end_time - start_time))\n",
    "    print(f'\\nData Loading Completed in {total_duration}')\n",
    "else:\n",
    "    print(f'Set ingest_data to True to ingest data. Currently ingest_data={ingest_data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3ddfb62-e2c3-4f84-b9a6-7256416665fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AllergyIntolerance': 10,\n",
      " 'CarePlan': 60,\n",
      " 'CareTeam': 60,\n",
      " 'Claim': 3486,\n",
      " 'Condition': 656,\n",
      " 'Device': 39,\n",
      " 'DiagnosticReport': 3100,\n",
      " 'DocumentReference': 1859,\n",
      " 'Encounter': 1859,\n",
      " 'ExplanationOfBenefit': 3486,\n",
      " 'ImagingStudy': 8,\n",
      " 'Immunization': 259,\n",
      " 'Medication': 957,\n",
      " 'MedicationAdministration': 957,\n",
      " 'MedicationRequest': 1627,\n",
      " 'Observation': 13501,\n",
      " 'Patient': 20,\n",
      " 'Procedure': 2966,\n",
      " 'SupplyDelivery': 239}\n",
      "Total Resource Nodes = 35149\n"
     ]
    }
   ],
   "source": [
    "# Get Report on Ingested Data\n",
    "get_neo4j_resourceNodes_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbe2030-09cd-4165-b319-6404ce8f701b",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### 6.2. VectorSearch Data Ingestion Helper Scripts\n",
    "[Vertex AI Vector Search Docs](https://cloud.google.com/vertex-ai/docs/vector-search/overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f1edf048-9efe-4165-9ec5-324831352c28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### Load Resource Text to Vector Search ####\n",
    "\n",
    "def add_resource_text_embedding(resource_data):      \n",
    "    \n",
    "    resource_text_metadata = {\n",
    "        \"fhir_patient_id\": [resource_data['patient_id']],\n",
    "        \"fhir_resource_id\": [resource_data['resource_id']],\n",
    "        \"fhir_resource_type\": [resource_data['resource_type']],\n",
    "        \"neo4j_node_id\": [resource_data['neo4j_id']]\n",
    "    }\n",
    "\n",
    "    # Add Resource Text Embeddings to Vector Search\n",
    "    resource_text = resource_data['resource_text']\n",
    "    if resource_text is not None:\n",
    "        try:\n",
    "            # ids = me.add_texts(texts=[resource_text], metadatas=[resource_text_metadata])\n",
    "            ids = vector_store.add_texts(texts=[resource_text], metadatas=[resource_text_metadata], is_complete_overwrite=True)\n",
    "            return ids\n",
    "        except Exception as err:\n",
    "            print(f\"\\nERROR: Unexpected while adding embeddings {err=}, {type(err)=}\")\n",
    "            print(f'Resource_Text = {resource_text}')\n",
    "            print(f'Resource Metadata = {resource_text_metadata}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ddec2491-21ad-48ad-b77b-0cb3d4cc3033",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_related_resource_node_texts(neo4j_node_id: str) -> str:\n",
    "    contextualize_query = f\"\"\"\n",
    "    MATCH (node :resource)\n",
    "    WHERE elementId(node)='{neo4j_node_id}'\n",
    "    MATCH(node)<-[]->(sc:resource)\n",
    "    with node.text as self, reduce(s=\"\", item in collect(distinct sc.text) | s + \"\\n\\nSecondary Entry:\\n\" + item ) as ctxt limit 1\n",
    "    return \"Primary Entry:\\n\" + self + ctxt as text\"\"\"\n",
    "\n",
    "    resource_text = graph.query(contextualize_query)[0][0][0]\n",
    "    \n",
    "    return resource_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70108bf5-e875-46ff-b16b-66d1cf5efa06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add Text Embeddings of all Resource Text to Vector Search\n",
    "\n",
    "def create_embeddings_of_all_resource_text():\n",
    "    batch_counter = 0\n",
    "    resource_counter = 0\n",
    "    resources_types_dict = {}\n",
    "    \n",
    "    resource_counter = 0\n",
    "    vector_ids_csv_file = 'vector_ids.csv'\n",
    "    with open(vector_ids_csv_file, 'w', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(['S.NO','FHIR_RESOURCE_ID','VECTORSEARCH_ID', 'PATIENT_ID'])\n",
    "    \n",
    "    # Since we have large number of Resources, we iterate in batches\n",
    "    for batch in get_nodes_in_batches(graph):\n",
    "        batch_counter += 1\n",
    "        # print(f'Batch: {batch_counter}, Len of Batch = {len(batch)}')\n",
    "        \n",
    "        for node in batch:\n",
    "            resource_counter += 1\n",
    "            resource_data = {\n",
    "                'neo4j_id': node[0].element_id,\n",
    "                'resource_id': node[0]['id'],\n",
    "                'resource_type': node[0]['resource_type'],\n",
    "                # 'resource_text': node[0]['text']        \n",
    "            }\n",
    "            \n",
    "            # Get related Patient ID of the Resource\n",
    "            resource_data['patient_id'] = \"\"\n",
    "            if node[0]['resource_type'] == 'Patient':\n",
    "                # print (node[0])\n",
    "                resource_data['patient_id'] = node[0]['id']\n",
    "            elif node[0]['subject_reference']:\n",
    "                resource_data['patient_id'] = node[0]['subject_reference'].split(':')[-1]\n",
    "            elif node[0]['patient_reference']: \n",
    "                resource_data['patient_id'] = node[0]['patient_reference'].split(':')[-1] \n",
    "            else:\n",
    "                print(f\"no patient id for resource_id: {node[0]['resource_id']}, resource_type: {node[0]['resource_type']}\")\n",
    "                # break\n",
    "\n",
    "            resource_data['resource_text'] = node[0]['text']\n",
    "\n",
    "\n",
    "            print(f\"Processing patient_id: {resource_data['patient_id']} node_id: {resource_data['neo4j_id']}, resource_id={resource_data['resource_id']}, resource_type={resource_data['resource_type']}\", end='\\r', flush=True)\n",
    "\n",
    "            # Create Embedding & Add to VectorSearch\n",
    "            ids = add_resource_text_embedding(resource_data)\n",
    "            print(f'Vector Search_ID: {ids}', end='\\r', flush=True)\n",
    "            \n",
    "            with open(vector_ids_csv_file, 'a', newline='') as csv_file:\n",
    "                resource_counter += 1\n",
    "                writer = csv.writer(csv_file)\n",
    "                \n",
    "                if ids is not None:\n",
    "                    writer.writerow([resource_counter, resource_data[\"resource_id\"], ids[0], resource_data['patient_id']])\n",
    "                else:\n",
    "                    writer.writerow([resource_counter, resource_data[\"resource_id\"], '', resource_data['patient_id']])                \n",
    "                \n",
    "            # break\n",
    "            \n",
    "            resource_type = resource_data['resource_type']\n",
    "            if resource_type not in resources_types_dict.keys():\n",
    "                resources_types_dict[resource_type] = 0\n",
    "\n",
    "            resources_types_dict[resource_type] += 1\n",
    "\n",
    "        # break\n",
    "    print(f'\\nProcessed Resources: {resource_counter}')\n",
    "    print('Resources Types Processed:')\n",
    "    print(resources_types_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3ec42b-3a6c-4725-a52f-3c492991099c",
   "metadata": {},
   "source": [
    "### 6.3 Trigger VectorSearch Data Ingestion\n",
    "In the Cell below we are creating Embeddings of Resource Text and ingesting it into Vertex AI Vector Search.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ Important: This process may take several hours to complete depending on the number of files and FHIR resources you are ingesting! ⚠️</b>\n",
    "</div>\n",
    "\n",
    "To monitor ingest progress, open a new bash terminal and run the below command:\n",
    "```bash\n",
    "tail -f unlock-fhir-with-rag-on-vertexai/vector_ids.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "965e2cb2-ea5f-4c32-9fa9-c5a484f5b378",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set ingest_data to True to ingest data. Currently ingest_data=False\n"
     ]
    }
   ],
   "source": [
    "if ingest_data:\n",
    "    start_time = time.time()\n",
    "    create_embeddings_of_all_resource_text()\n",
    "    end_time = time.time()\n",
    "\n",
    "    total_duration = str(datetime.timedelta(seconds = end_time - start_time))\n",
    "    print(f'\\nData Loading Completed in {total_duration}')\n",
    "\n",
    "else:\n",
    "    print(f'Set ingest_data to True to ingest data. Currently ingest_data={ingest_data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7420776e-14e3-4caf-bfed-36e4d2e9aa3d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 6.4 Query VectorSearch to Test Data Ingestion\n",
    "In the step below we query the Vector Search DB to verify data load is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "88ac8c69-3a78-4eb0-993c-0f48ecd66a52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_text = \"Sample resource type\"\n",
    "vs_response = vector_store.similarity_search_with_score(query=query_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7fb83052-a59e-42a9-a4c1-71292d5ae7ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vs_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5404cb45-8d3b-4744-bb17-080b7fcdcab2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='This is a sample Resource Type', metadata={'fhir_patient_id': 'pid_111111111', 'fhir_resource_id': 'rid_111111111', 'fhir_resource_type': 'Test_Resource_type', 'neo4j_node_id': 'nid_111111111'}),\n",
       "  0.7533161640167236)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering by Semantic Search Score\n",
    "def filter_result_by_score(min_score: float, vs_respone: list) -> list:\n",
    "    filtered_results = []\n",
    "    \n",
    "    for doc, score in vs_response:\n",
    "        if score >= min_score:\n",
    "            filtered_results.append((doc, score))\n",
    "    \n",
    "    return filtered_results\n",
    "\n",
    "    \n",
    "min_score = 0.7        \n",
    "filtered_results = filter_result_by_score(min_score, vs_response)\n",
    "len(filtered_results) # We will now get results with score min_score (0.7) and above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1988b401-d708-4909-9c2d-fc5f268c26e5",
   "metadata": {},
   "source": [
    "---\n",
    "## Preparing LLM Input: Prompts, Questions, Model\n",
    "This section covers the design of LangChain Prompt Templates, User questions for LLM , and the choice of the LLM model for task execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31aada8-bd70-42bd-a7b6-863b0458ac30",
   "metadata": {},
   "source": [
    "### Prompt Design\n",
    "\n",
    "This cell defines the prompt template used to interact with the LLM. Try different prompts to see how they influence the LLM's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6916cc74-c3ff-4bc9-abd0-82d90256b978",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "default_prompt='''\n",
    "System: Use the following pieces of context to answer the user's question. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "----------------\n",
    "{context}\n",
    "Human: {question}\n",
    "'''\n",
    "\n",
    "my_prompt='''\n",
    "System: The following information contains entries about the patient. \n",
    "Use the primary entry and then the secondary entries to answer the user's question.\n",
    "Each entry is its own type of data and secondary entries are supporting data for the primary one. \n",
    "You should restrict your answer to using the information in the entries provided. \n",
    "\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "----------------\n",
    "{context}\n",
    "----------------\n",
    "User: {question}\n",
    "'''\n",
    "\n",
    "my_prompt_2='''\n",
    "System: The context below contains entries about the patient's healthcare. \n",
    "Please limit your answer to the information provided in the context. Do not make up facts.\n",
    "Please limit your answers only about the patient in the user question. If you do not find the patient name in the context.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "If you are asked about the patient's name and one the entries is of type patient, you should look for the first given name and family name and answer with: [given] [family]\n",
    "----------------\n",
    "{context}\n",
    "Human: {question}\n",
    "'''\n",
    "\n",
    "prompt = PromptTemplate.from_template(my_prompt_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed268fa-c1b7-4c7a-8669-5d53d40a0e67",
   "metadata": {},
   "source": [
    "### Define User Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b56a97a7-89ec-4b76-83c9-3eab9c2e51ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_list = [\n",
    "    \"What can you tell me about Alfonso's claim created on 03/06/1977?\",\n",
    "    \"What can you tell me about the medical claim created on 03/06/1977?\",\n",
    "    \"Based on this explanation of benefits, how much did it cost and what service was provided?\",\n",
    "    \"Based on this explanation of benefits created on July 15, 2016, how much did it cost and what service was provided?\",\n",
    "    \"Based on this explanation of benefits created on March 6, 1978, how much did it cost and what service was provided?\",\n",
    "    \"Based on this explanation of benefits created on January 11, 2009, how much did it cost and what service was provided?\",\n",
    "    \"What was the blood pressure on 2/9/2014?\",\n",
    "    \"What was the blood pressure?\",\n",
    "    \"Based on this explanation of benefits created on January 18, 2014, how much did it cost and what service was provided?\",\n",
    "    \"How much did the colon scan eighteen days after the first of the year 2019 cost?\",\n",
    "    \"How much did the medical reconciliation on Dec. 29, 2023 cost?\",\n",
    "    \"What can you tell me about Andrea7's claim created on 12/25/2003?\",\n",
    "    \"What can you tell me about claim created on 12/25/2003?\",\n",
    "    \"What allergies does Antone63 have?\"\n",
    "]\n",
    "\n",
    "len(question_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c6e2bbef-fdb4-42a3-ba7a-a816cac1594e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What allergies does Antone63 have?'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = question_list[13]\n",
    "question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d2b5a7-39a1-451d-a074-2a8bf5e99cf3",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Testing Without RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5610c055-15dd-4249-8be8-c48cf4b838d4",
   "metadata": {},
   "source": [
    "The LLM would not be able to answer since it does not have the context. \n",
    "<br>Context is the Private User/Organization Data. FHIR Data in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "620d639d-f09b-4e80-83e2-333c60c1dba5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What allergies does Antone63 have?\n",
      "LLM Answer: I do not have access to personal medical information, including allergies, and cannot provide this information about Antone63.\n"
     ]
    }
   ],
   "source": [
    "llm = VertexAI(model_name=VERTEX_AI_MODEL_NAME)\n",
    "\n",
    "# Ask LLM the question\n",
    "no_rag_answer = llm(question)\n",
    "print(f'Question: {question}')\n",
    "print(f'LLM Answer: {no_rag_answer}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868c84ce-f603-41d7-99c8-cfe199872542",
   "metadata": {},
   "source": [
    "## Testing with RAG - Ask the LLM with Context\n",
    "\n",
    "This cell will ask the LLM with the string representation of the resource node that is found by the vector index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ed042f-94ed-42ed-bead-23c3aa6dab64",
   "metadata": {},
   "source": [
    "### Testing VertexAI VectorSearch\n",
    "\n",
    "We use VertexAI VectorSearch to retrieve relevant FHIR data based on the user's query. The search incorporates:\n",
    "- **Semantic Matching**: Finding Resources with similar text embeddings to the query.\n",
    "- **Metadata Filters**: Narrowing down results based on patient_id and resource_type to ensure relevance. (Refer to the Filter vector matches: https://cloud.google.com/vertex-ai/docs/vector-search/filtering documentation.)\n",
    "\n",
    "A RAG based LLM prompt consist of 3 sections:\n",
    "- **Instructions**: Guidance for the LLM on how to generate an accurate and relevant response based on the retrieved context and the user query.\n",
    "- **Context**: We also apply filters based on patient_id and resource_type to ensure the retrieved context is relevant to the specific patient and the type of information sought. This is a key step, as the accuracy of the LLM's response directly depends on the relevance of this retrieved context. \n",
    "- **User Question**: The original query posed by the user.\n",
    "\n",
    "For Demonstration purpose we will:\n",
    "- First query VectorSearch without Filter.\n",
    "- Next query VectorSearch with Filters. Refer to VectoSearch [Filter vector matches](https://cloud.google.com/vertex-ai/docs/vector-search/filtering) documentation for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000cefbf-c8b4-44c1-8057-d98e22559608",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec793d38-8756-478a-9f96-7399e6211889",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create chain to answer questions\n",
    "NUMBER_OF_RESULTS = 20\n",
    "SEARCH_DISTANCE_THRESHOLD = 0.7\n",
    "\n",
    "# Expose index to the retriever\n",
    "retriever = me.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\n",
    "        \"k\": NUMBER_OF_RESULTS,\n",
    "        \"search_distance\": SEARCH_DISTANCE_THRESHOLD,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51449911-c294-47fe-a95f-13ff5647403a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vector_qa = RetrievalQA.from_chain_type(\n",
    "    llm=ChatVertexAI(model_name=VERTEX_AI_MODEL_NAME),\n",
    "    chain_type='stuff',\n",
    "    retriever=retriever,\n",
    "    # return_source_documents=True,\n",
    "    verbose=True,\n",
    "    chain_type_kwargs={\"verbose\": False, \"prompt\": prompt}\n",
    ")\n",
    "\n",
    "pprint(vector_qa.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0d1392-eed3-4f28-aa1a-7a96e1583fd4",
   "metadata": {},
   "source": [
    "### Cons of this approach\n",
    "- The Vector Search similarity search fetches all matching Resource Types.\n",
    "- However, the context does not include the Patients name for the retrieved Resources.\n",
    "- Hence the LLM might summarize and extract the information from the context, but the Resources might not belong to a Patient in question.\n",
    "- Sometimes the LLM responds \"The context does not mention any patient\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8887ca35-5c02-4ade-826d-a22b7711e1e1",
   "metadata": {},
   "source": [
    "## Testing with RAG - Ask the LLM with Enhanced Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b0b64e-942b-4fa7-a212-f9c7284c2c9e",
   "metadata": {},
   "source": [
    "***Providing Context to the LLM***\n",
    "\n",
    "We enrich the LLM's understanding by fetching text from linked resource nodes.\n",
    "\n",
    "***Steps:***\n",
    "- *Similarity Search:* Identify a matching resource's Neo4J Node ID in the Vector Search database.\n",
    "- *Fetch Related Resources:* Query all nodes connected to the matching resource.\n",
    "- *Concatenate Text:* Combine the text from all retrieved nodes to provide comprehensive context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a844df6-379c-49f1-af86-a150ca3efe40",
   "metadata": {},
   "source": [
    "### Create a new Vector Search Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c1b0ad-cb0f-426e-b6c0-67683b509895",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create GCS Bucket for the new Enhanced Vector Search Index\n",
    "! set -x && gsutil mb -p $PROJECT_ID -l $REGION gs://$ME_ENHANCED_EMBEDDING_GCS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77e3938-9435-4ba7-b0a1-36f6c201b04d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a new Vector Search Index for Enhanced Context\n",
    "me_utils_enhanced = MatchingEngineUtils(PROJECT_ID, REGION, ME_ENHANCED_CONTEXT_INDEX_NAME)\n",
    "\n",
    "me_index_enhanced = me_utils_enhanced.create_index(\n",
    "    embedding_gcs_uri=f'gs://{ME_ENHANCED_EMBEDDING_GCS_DIR}/init_index',\n",
    "    dimensions=ME_DIMENSIONS,\n",
    "    index_update_method='streaming',\n",
    "    index_algorithm='tree-ah',\n",
    "    shard_size= ME_SHARD_SIZE,\n",
    "    distance_measure_type=ME_DISTANCE_MEASURE_TYPE,\n",
    "    description=ME_ENHANCED_DESCRIPTION\n",
    ")\n",
    "\n",
    "# me_index_enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d46345-01f6-45a3-b177-dc6a5438abbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get information about the Index\n",
    "if me_index_enhanced:\n",
    "    index_metadata = MessageToDict(me_index_enhanced._pb)\n",
    "    print('Index Details:')\n",
    "    print(f'- Index Name = {index_metadata[\"name\"]}')\n",
    "    print(f'- Update Method = {index_metadata[\"indexUpdateMethod\"]}')\n",
    "    print(f'- Dimensions = {index_metadata[\"metadata\"][\"config\"][\"dimensions\"]}')\n",
    "    print(f'- Shard Size = {index_metadata[\"metadata\"][\"config\"][\"shardSize\"]}')\n",
    "    print(f'- Distance Measure Type = {index_metadata[\"metadata\"][\"config\"][\"distanceMeasureType\"]}')\n",
    "    algorithm = list(index_metadata[\"metadata\"][\"config\"]['algorithmConfig'].keys())[0]\n",
    "    print(f'- Algorithm = {algorithm}')\n",
    "    # print(f'- Index Stats = {index_metadata[\"indexStats\"]}')\n",
    "\n",
    "# index_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f595b6cb-de6f-4ca6-8b59-f4ee65a90e15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "me_endpoint_enhanced = me_utils_enhanced.deploy_index(\n",
    "    machine_type=ME_ENDPOINT_MACHINE_TYPE,\n",
    "    min_replica_count=ME_ENDPOINT_MIN_REPLICA_COUNT,\n",
    "    max_replica_count=ME_ENDPOINT_MAX_REPLICA_COUNT,\n",
    "    public_endpoint_enabled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a4349a-6ff6-4227-b188-3c5b092e0469",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get Matching Engine Index id and Endpoint id\n",
    "me_utils_enhanced = MatchingEngineUtils(PROJECT_ID, REGION, ME_ENHANCED_CONTEXT_INDEX_NAME)\n",
    "ME_ENHANCED_INDEX_ID, ME_ENHANCED_INDEX_ENDPOINT_ID = me_utils_enhanced.get_index_and_endpoint()\n",
    "print(f\"ME_ENHANCED_INDEX_ID={ME_ENHANCED_INDEX_ID}\")\n",
    "print(f\"ME_ENHANCED_INDEX_ENDPOINT_ID={ME_ENHANCED_INDEX_ENDPOINT_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81b63b4-e5a8-4890-a592-211732a844c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize vector store\n",
    "me_enhanced = MatchingEngine.from_components(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=REGION,\n",
    "    gcs_bucket_name=f\"gs://{ME_ENHANCED_EMBEDDING_GCS_DIR}\".split(\"/\")[2],\n",
    "    embedding=embeddings,\n",
    "    index_id=ME_ENHANCED_INDEX_ID,\n",
    "    endpoint_id=ME_ENHANCED_INDEX_ENDPOINT_ID,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae8f0be-df3f-44ff-856d-815e773c3615",
   "metadata": {},
   "source": [
    "### Ingest Enhanced Context data to Vector Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4dd86-8140-4ae9-8c97-9cfb7b0783f8",
   "metadata": {},
   "source": [
    "In the Cell below we are creating Embeddings of Resource Text and ingesting it into Vertex AI Vector Search.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ Important: Below step will take few minutes to complete! ⚠️</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db35044a-efce-4ec2-910c-6247004da4ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Trigger Enhanced Context Data Ingestion to vector Search\n",
    "ingest_data = False\n",
    "\n",
    "if ingest_data:\n",
    "    create_embeddings_of_all_resource_text(me_enhanced, enhanced_context=True)\n",
    "else:\n",
    "    print(f'Set ingest_data to True to ingest data. Currently ingest_data={ingest_data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd850fcb-2609-4ecf-9e70-6fa63d3921da",
   "metadata": {},
   "source": [
    "### RAG with Enhanced Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9676b6-bd04-4839-b665-c4ffec235e05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# question = \"What procedure was performed on Antone63 on 2014-04-20?\"\n",
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10f4e11-c619-4dbf-840d-386e3ee5f360",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create chain to answer questions\n",
    "NUMBER_OF_RESULTS = 10\n",
    "SEARCH_DISTANCE_THRESHOLD = 0.6\n",
    "\n",
    "# Expose index to the retriever\n",
    "retriever_enhanced = me_enhanced.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\n",
    "        \"k\": NUMBER_OF_RESULTS,\n",
    "        \"search_distance\": SEARCH_DISTANCE_THRESHOLD,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8c79e2-4f44-4064-ae6b-960ca981de98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# question = \"Based on this explanation of benefits created on February 11, 1999, how much did it cost and what service was provided?\"\n",
    "vector_qa_enhanced = RetrievalQA.from_chain_type(\n",
    "    llm=ChatVertexAI(model_name=VERTEX_AI_MODEL_NAME),\n",
    "    chain_type='stuff',\n",
    "    retriever=retriever_enhanced,\n",
    "    # return_source_documents=True,\n",
    "    verbose=True,\n",
    "    chain_type_kwargs={\"verbose\": True, \"prompt\": prompt}\n",
    ")\n",
    "\n",
    "# question = 'Tell me about the latest medical reconciliation?'\n",
    "pprint(vector_qa_enhanced.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308f655a-3a15-45c7-b55a-b8783468e606",
   "metadata": {},
   "source": [
    "### Cons of this approach\n",
    "A problem with this approach is the data is repeated multiple times (e.g. Patient Information). This results in increased LLM Token consumption and costs.\n",
    "An alternate strategy would be:\n",
    "- First retrieve the Resource in Question from Vector Search\n",
    "- Use the Resource ID to query Neo4J for related Resources (Text field)\n",
    "- Remove Duplicate Resource Entries \n",
    "- Dynamically construct the Context and pass it to the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a879bf-4dd3-4dae-8b9f-272dda7774fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Testing with RAG - Custom Retreiver\n",
    "\n",
    "The Custom retriever addresses the above problem. It does so by:\n",
    "- Identifying the Patient Name from User Query. If Patient name not present prompt the User for the Patient Name\n",
    "- Idenitfy the FHIR Resource Type in Question\n",
    "- Perform a similarity search by narrowing down the results based on Patient ID and Resource Type\n",
    "\n",
    "Thus increasing the accuracy of the Context provided to the LLM for information extraction.\n",
    "\n",
    "**Enhancement** - This approach does not address the below use case of getting Related Resources for more context. This can be fixed by combining the Custom Retriever and fetching related Resource text from Neo4J to provide the necessary context to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21981d2b-a4b3-4c50-99e0-0cb9cb0127f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "me_utils = MatchingEngineUtils(PROJECT_ID, REGION, ME_INDEX_NAME)\n",
    "\n",
    "# Get Matching Engine Index id and Endpoint id\n",
    "ME_INDEX_ID, ME_INDEX_ENDPOINT_ID = me_utils.get_index_and_endpoint()\n",
    "\n",
    "# Create Text Embedding\n",
    "embeddings = VertexAIEmbeddings(\n",
    "    model_name=\"textembedding-gecko@003\",\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    max_retries=6\n",
    ")\n",
    "\n",
    "# Initialize vector store\n",
    "me = MatchingEngine.from_components(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=REGION,\n",
    "    gcs_bucket_name=f\"gs://{ME_EMBEDDING_GCS_DIR}\".split(\"/\")[2],\n",
    "    embedding=embeddings,\n",
    "    index_id=ME_INDEX_ID,\n",
    "    endpoint_id=ME_INDEX_ENDPOINT_ID,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f290337a-9dd3-4f40-917a-a4a847471552",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd0a873-774f-4a0e-8a9c-5ab32189d385",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "llm = VertexAI(model_name=VERTEX_AI_MODEL_NAME)\n",
    "\n",
    "graph = Graph(NEO4J_URL, NEO4J_USER, NEO4J_PASSWORD)\n",
    "retriever = FHIRResourcesRetriever(llm=llm, me=me, neo4j_graph=graph)\n",
    "\n",
    "vector_qa = RetrievalQA.from_chain_type(\n",
    "    llm=ChatVertexAI(model_name=VERTEX_AI_MODEL_NAME),\n",
    "    chain_type='stuff',\n",
    "    retriever=retriever,\n",
    "    # return_source_documents=True,\n",
    "    verbose=True,\n",
    "    chain_type_kwargs={\"verbose\": False, \"prompt\": prompt}\n",
    ")\n",
    "\n",
    "# print(vector_qa.__dir__())\n",
    "print(vector_qa.invoke(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff1d120-a2da-43dc-8b69-601d4e545c62",
   "metadata": {},
   "source": [
    "## RAG with Enhanced Context and Custom Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3d283e-c4ec-48b3-a965-87166054c6ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45673f3-9415-42fa-833b-2b9c94666bec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = VertexAI(model_name=VERTEX_AI_MODEL_NAME)\n",
    "\n",
    "graph = Graph(NEO4J_URL, NEO4J_USER, NEO4J_PASSWORD)\n",
    "retriever = FHIRResourcesRetriever(llm=llm, me=me_enhanced, neo4j_graph=graph)\n",
    "\n",
    "vector_qa = RetrievalQA.from_chain_type(\n",
    "    llm=ChatVertexAI(model_name=VERTEX_AI_MODEL_NAME),\n",
    "    chain_type='stuff',\n",
    "    retriever=retriever,\n",
    "    # return_source_documents=True,\n",
    "    verbose=True,\n",
    "    chain_type_kwargs={\"verbose\": True, \"prompt\": prompt}\n",
    ")\n",
    "\n",
    "print(vector_qa.invoke(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d1110f-4da5-428e-aa53-a7e6b977bd90",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64616611-3669-4f80-b561-1c706e540cd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def date_for_question(question_to_find_date):\n",
    "    _llm = text_bison_llm \n",
    "    _response = _llm(f'''\n",
    "    system:Given the following question from the user, extract the date the question is asking about.\n",
    "    Return the answer formatted as JSON only, as a single line.\n",
    "    Use the form:\n",
    "    \n",
    "    {{\"date\":\"[THE DATE IN THE QUESTION]\"}}\n",
    "    \n",
    "    Use the date format of month/day/year.\n",
    "    Use two digits for the month and day.\n",
    "    Use four digits for the year.\n",
    "    So 3/4/23 should be returned as {{\"date\":\"03/04/2023\"}}.\n",
    "    So 04/14/89 should be returned as {{\"date\":\"04/14/1989\"}}.\n",
    "    \n",
    "    Please do not include any special formatting characters, like new lines or \"\\\\n\".\n",
    "    Please do not include the word \"json\".\n",
    "    Please do not include triple quotes.\n",
    "    \n",
    "    If there is no date, do not make one up. \n",
    "    If there is no date return the word \"none\", like: {{\"date\":\"none\"}}\n",
    "    \n",
    "    user:{question_to_find_date}\n",
    "    ''')\n",
    "    date_json = json.loads(_response)\n",
    "    return date_json['date']\n",
    "\n",
    "date_str = date_for_question(question)\n",
    "print(date_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20483e2-8253-4d1f-9e3d-5fac6680a4a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_patient_name(question_to_find_names):\n",
    "    # _llm = text_bison_llm \n",
    "    \n",
    "    _response = llm(f'''\n",
    "    system:Given the following question from the user, identify all potential first and last names within this sentence.\n",
    "    \n",
    "    The name might also contain numbers e.g. Andrea7, Jenkins714, Chasity985, Pagac496\n",
    "    The name might also contain Apostrophe e.g. Andrea's, John's, Johns' James'\n",
    "    If the nameis in the format Smith, John then first-name = John, last-name = Smith\n",
    "    \n",
    "    Return the answer formatted as first-name last-name.\n",
    "    \n",
    "    Use the form:\n",
    "    first-name last name\n",
    "    \n",
    "    Please do not include any special formatting characters, like new lines or \"\\\\n\".\n",
    "    Please do not include triple quotes.\n",
    "    \n",
    "    If there are no names, do not make one up. \n",
    "    If there are no names return an empty string link \"\"\n",
    "    \n",
    "    user:{question_to_find_names}\n",
    "    ''')\n",
    "    names = _response\n",
    "    if names == \"\":\n",
    "        input(\"Please enter the Patient name:\")\n",
    "    return names\n",
    "\n",
    "# Jenkins714 Andrea7\n",
    "q = question\n",
    "patient_name = get_patient_name(q).strip()\n",
    "print(patient_name)\n",
    "\n",
    "\n",
    "# While Loading data into Neo4J we created a 'Text' Attribute to convert Resource JSON format to Text.\n",
    "# We will use the same sentence structure to get an accurate seearch result from Vector Store.\n",
    "patient_name_query=f\"The type of information in this entry is patient. The name use for this patient is official. The name family for this patient is {patient_name}. The name given 0 for this patient is {patient_name}.\"\n",
    "# print(patient_name_query)\n",
    "\n",
    "response = me.similarity_search(patient_name_query, k=2)\n",
    "for doc in response:\n",
    "    if doc.metadata['score'] > .85:\n",
    "        print(doc)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252a0185-3cd6-44ec-814d-2df02e162da3",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Cleaning Up\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ Important: To avoid incurring charges, please delete the Google Cloud resources used in this tutorial. ⚠️</b>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5bc8d1-c215-40f3-a504-14e248eff059",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CLEANUP_RESOURCES = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f903817-df2a-41e6-b640-23d60cff9663",
   "metadata": {},
   "source": [
    "### Delete Neo4J Docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56ed646-5dd0-427f-932a-41c12de2c877",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Wipe Neo4J Database\n",
    "graph = Graph(NEO4J_URL, NEO4J_USER, NEO4J_PASSWORD)\n",
    "if CLEANUP_RESOURCES:\n",
    "    graph.wipe_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1e88f7-aa31-469a-a9fa-3ff58c77a8cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DELETE NEO4J CONTAINER\n",
    "if CLEANUP_RESOURCES:\n",
    "    ! docker stop testneo4j\n",
    "    ! docker rm -fv testneo4j\n",
    "    ! sudo rm -rf $HOME/neo4j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8e87da-ab55-453d-b657-200759ceee42",
   "metadata": {},
   "source": [
    "### Delete Vector Search Indexes & Index-Endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1f6583-70f5-47a5-a481-df8a953bb4a1",
   "metadata": {},
   "source": [
    "- Delete ME Vector Search Index and Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6148d8bc-1d50-479c-b5ee-8092d727d0d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "me_utils = MatchingEngineUtils(PROJECT_ID, REGION, ME_INDEX_NAME)\n",
    "ME_INDEX_ID, ME_INDEX_ENDPOINT_ID = me_utils.get_index_and_endpoint()\n",
    "\n",
    "# Delete Endpoint\n",
    "if CLEANUP_RESOURCES and \"me_utils\" in globals():\n",
    "    print(\n",
    "        f\"Undeploying all deployed indexes and deleting the index endpoint {ME_INDEX_ENDPOINT_ID}\"\n",
    "    )\n",
    "    me_utils.delete_index_endpoint()\n",
    "\n",
    "# Delete Index     \n",
    "if CLEANUP_RESOURCES and \"me_utils\" in globals():\n",
    "    print(f\"Deleting the index {ME_INDEX_ID}\")\n",
    "    me_utils.delete_index()    \n",
    "\n",
    "# Delete Bucket    \n",
    "if CLEANUP_RESOURCES:\n",
    "    # Delete contents of the bucket \n",
    "    ! gsutil -m rm -r gs://{ME_EMBEDDING_GCS_DIR}\n",
    "    ! gsutil rb gs://{ME_EMBEDDING_GCS_DIR}\n",
    "\n",
    "print('Vector Search and GCS Bucket Cleaning complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61da45ba-8e02-4b35-bfad-69b7ff8c05e3",
   "metadata": {},
   "source": [
    "- Delete ME_ENHANCED Vector Search Index and Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6722282-b519-49e5-b617-9b9a2aa033ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "me_utils_enhanced = MatchingEngineUtils(PROJECT_ID, REGION, ME_ENHANCED_CONTEXT_INDEX_NAME)\n",
    "ME_ENHANCED_INDEX_ID, ME_ENHANCED_INDEX_ENDPOINT_ID = me_utils_enhanced.get_index_and_endpoint()\n",
    "\n",
    "# Delete Endpoint\n",
    "if CLEANUP_RESOURCES and \"me_utils_enhanced\" in globals():\n",
    "    print(\n",
    "        f\"Undeploying all deployed indexes and deleting the index endpoint {ME_ENHANCED_INDEX_ENDPOINT_ID}\"\n",
    "    )\n",
    "    me_utils_enhanced.delete_index_endpoint()\n",
    "\n",
    "# Delete Index    \n",
    "if CLEANUP_RESOURCES and \"me_utils_enhanced\" in globals():\n",
    "    print(f\"Deleting the index {ME_ENHANCED_INDEX_ID}\")\n",
    "    me_utils_enhanced.delete_index()\n",
    "\n",
    "# Delete Bucket\n",
    "if CLEANUP_RESOURCES:\n",
    "    ! gsutil -m rm -r gs://{ME_ENHANCED_EMBEDDING_GCS_DIR}\n",
    "    ! gsutil rb gs://{ME_ENHANCED_EMBEDDING_GCS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa673cfe-cda3-4312-ae82-8b300299b1e8",
   "metadata": {},
   "source": [
    "# To-Do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18457317-ea09-4a17-bb8e-ffcd8c4d277a",
   "metadata": {},
   "source": [
    "- Example MedLM Integration - Query all Patients with high risk of some disease (Stroke)\n",
    "- Query all Vacation candiates and based on time send Notification to them. E.g. Child vacccination message to Parent - Query Neo4J Date Nodes range"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
