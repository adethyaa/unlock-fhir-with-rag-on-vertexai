{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61b157ea-95f2-47f7-8e73-478d7ed6c4f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45016037-5afb-4d19-b22d-6c5fef54529d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Unlock FHIR with RAG on Vertex AI - Part-02 (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32b2b83-1923-4e92-ae86-1a57cdc8f415",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run the Notebook\n",
    "\n",
    "**_NOTE_**: This notebook has been tested in the following environment:\n",
    "\n",
    "* Python version = 3.10.13\n",
    "\n",
    "<table align=\"center\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/adethyaa/unlock-fhir-with-rag-on-vertexai/blob/main/02_FHIR_RAG.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/adethyaa/unlock-fhir-with-rag-on-vertexai/blob/main/02_FHIR_RAG.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/adethyaa/unlock-fhir-with-rag-on-vertexai/blob/main/02_FHIR_RAG.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb26615d-6727-452e-a865-87e0664238d6",
   "metadata": {},
   "source": [
    "## 1.Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e95ee05-8241-4bcf-88de-dff23dff503f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.1. Authenticate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee1aca4f-86dc-4332-a7df-595dfee9b5aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Authenticate Notebook\n",
    "\n",
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7d6abe-74e8-4868-ad98-96948d56de8e",
   "metadata": {},
   "source": [
    "### 1.2. Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9724e722-e716-4046-abba-3a83d443ede5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GCP Parameters\n",
    "PROJECT_ID = \"propane-crawler-363311\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"  # @param {type: \"string\"}\n",
    "\n",
    "# Neo4J Connection Parameters\n",
    "NEO4J_URL=\"bolt://localhost:7687\" # @param {type:\"string\"}\n",
    "NEO4J_USER=\"neo4j\" # @param {type:\"string\"}\n",
    "NEO4J_PASSWORD=\"password\" # @param {type:\"string\"}\n",
    "\n",
    "# Dimension Vertex PaLM Text Embedding\n",
    "ME_DIMENSIONS = 768 # @param {type:\"integer\"} \n",
    "ME_DISTANCE_MEASURE_TYPE = \"DOT_PRODUCT_DISTANCE\" # @param {type:\"string\"} \n",
    "\n",
    "# Update to bigger SHARDS for larger data volumes & performance\n",
    "# Doc - https://cloud.google.com/vertex-ai/docs/vector-search/create-manage-index\n",
    "ME_SHARD_SIZE = \"SHARD_SIZE_SMALL\" # @param [\"SHARD_SIZE_SMALL\", \"SHARD_SIZE_MEDIUM\", \"SHARD_SIZE_LARGE\"] \n",
    "\n",
    "# Vertex AI Vector Search (MatchingEngine) Endpoint Parameters\n",
    "# Doc - https://cloud.google.com/vertex-ai/docs/vector-search/create-manage-index\n",
    "\n",
    "# The machine types that you can use to deploy your index\n",
    "ME_ENDPOINT_MACHINE_TYPE = \"e2-standard-2\" # @param [\"n1-standard-16\", \"n1-standard-32\", \"e2-standard-2\", \"e2-standard-16\", \"e2-highmem-16\", \"n2d-standard-32\"] \n",
    "\n",
    "ME_ENDPOINT_MIN_REPLICA_COUNT = 2 # @param {type:\"integer\"} \n",
    "ME_ENDPOINT_MAX_REPLICA_COUNT = 10 # @param {type:\"integer\"} \n",
    "\n",
    "# Vertex AI Vector Search (MatchingEngine) Index Parameters\n",
    "ME_INDEX_NAME = 'fhir_me_index'  # @param {type: \"string\"}\n",
    "ME_EMBEDDING_GCS_DIR = f'{PROJECT_ID}-me-bucket' # @param {type:\"string\"} \n",
    "ME_DESCRIPTION = \"Index for FHIR Resources\" # @param {type:\"string\"} \n",
    "\n",
    "# Set the LLM to use\n",
    "VERTEX_AI_MODEL_NAME = 'gemini-1.0-pro-001'\n",
    "# VERTEX_AI_MODEL_NAME = 'gemini-1.5-pro-preview-0409'\n",
    "TEXT_EMBEDDING_MODEL_NAME = \"textembedding-gecko@003\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85415490-1446-4983-8691-0fcd43eee3cb",
   "metadata": {},
   "source": [
    "### 1.3. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbf5bd72-a688-441e-9fa9-43f25b2b86f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain version: 0.1.16\n"
     ]
    }
   ],
   "source": [
    "# Utils\n",
    "from pprint import pprint\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "from rich.console import Console\n",
    "from rich.markdown import Markdown\n",
    "\n",
    "from typing import Dict, Optional, Any, List\n",
    "\n",
    "# Google Libs\n",
    "import vertexai\n",
    "vertexai.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "from google.cloud.aiplatform.matching_engine.matching_engine_index_endpoint import (\n",
    "    Namespace,\n",
    "    NumericNamespace,\n",
    ")\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Langchain\n",
    "import langchain\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.chains import RetrievalQA, LLMChain, SequentialChain, TransformChain\n",
    "from langchain.globals import set_debug, set_verbose\n",
    "print(f\"LangChain version: {langchain.__version__}\")\n",
    "\n",
    "# LangChain Google Libs\n",
    "from langchain_google_vertexai import VertexAI\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_google_vertexai import VectorSearchVectorStore\n",
    "\n",
    "# Custom Utils\n",
    "## Custom Matching Engine\n",
    "from utils.matching_engine import MatchingEngine\n",
    "from utils.matching_engine_utils import MatchingEngineUtils\n",
    "\n",
    "## Neo4J\n",
    "from utils.NEO4J_Graph import Graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e33c02-67b0-45ff-ae3e-292c916ace19",
   "metadata": {},
   "source": [
    "### 1.4. Neo4J Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed0ba81b-b946-415a-8ec9-5797c77e25a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: NEO4J_USER=neo4j\n",
      "env: NEO4J_PASSWORD=password\n"
     ]
    }
   ],
   "source": [
    "%env NEO4J_USER={NEO4J_USER}\n",
    "%env NEO4J_PASSWORD={NEO4J_PASSWORD}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36967464-b2e3-4592-a19d-d038395f1929",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE          COMMAND                  CREATED      STATUS       PORTS                                                                                            NAMES\n",
      "c11076036c54   neo4j:latest   \"tini -g -- /startupâ€¦\"   6 days ago   Up 3 hours   0.0.0.0:7474->7474/tcp, :::7474->7474/tcp, 7473/tcp, 0.0.0.0:7687->7687/tcp, :::7687->7687/tcp   testneo4j\n"
     ]
    }
   ],
   "source": [
    "# Check if Docker Container is running\n",
    "! docker ps -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fefbdc-305b-49f4-b6e8-8a4cff2cb3cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start the Container if it is not running\n",
    "! docker start testneo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27ec5ac4-7816-413b-85bf-efecf6a4c498",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate & Connect to Neo4J graph\n",
    "graph = Graph(NEO4J_URL, NEO4J_USER, NEO4J_PASSWORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22503c29-f8ed-43d7-8cef-4cdf620e5dfc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['AllergyIntolerance', 10],\n",
      " ['CarePlan', 60],\n",
      " ['CareTeam', 60],\n",
      " ['Claim', 3486],\n",
      " ['Condition', 656],\n",
      " ['Device', 39],\n",
      " ['DiagnosticReport', 3100],\n",
      " ['DocumentReference', 1859],\n",
      " ['Encounter', 1859],\n",
      " ['ExplanationOfBenefit', 3486],\n",
      " ['ImagingStudy', 8],\n",
      " ['Immunization', 259],\n",
      " ['Medication', 957],\n",
      " ['MedicationAdministration', 957],\n",
      " ['MedicationRequest', 1627],\n",
      " ['Observation', 13501],\n",
      " ['Patient', 20],\n",
      " ['Procedure', 2966],\n",
      " ['SupplyDelivery', 239]]\n"
     ]
    }
   ],
   "source": [
    "# Test Neo4J Connection\n",
    "# Get type and number of each FHIR resource in the database\n",
    "resource_metrics = graph.resource_metrics()\n",
    "resource_metrics.sort()\n",
    "pprint(resource_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8df95e7b-c42e-4894-9ab3-70d7613f7fc4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database Metrics:\n",
      "    - Node Count = 37884\n",
      "    - Relationship Count = 190926\n"
     ]
    }
   ],
   "source": [
    "node_count, relationship_count = graph.database_metrics()\n",
    "print('Database Metrics:')\n",
    "print(f'    - Node Count = {node_count}')\n",
    "print(f'    - Relationship Count = {relationship_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60782d8-da1f-4493-a7db-35c81f4c0cd9",
   "metadata": {},
   "source": [
    "### 1.5. VertexAI VectorSearch Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04ca1452-bbac-440c-b059-31f378534808",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VertexAIEmbeddings(client=<vertexai.language_models.TextEmbeddingModel object at 0x7f1b4f8bf2b0>, project='propane-crawler-363311', location='us-central1', request_parallelism=5, max_retries=6, stop=None, model_name='textembedding-gecko@003', client_preview=None, temperature=None, max_output_tokens=None, top_p=None, top_k=None, credentials=None, n=1, streaming=False, safety_settings=None, api_transport=None, api_endpoint=None, instance={'max_batch_size': 250, 'batch_size': 250, 'min_batch_size': 5, 'min_good_batch_size': 5, 'lock': <unlocked _thread.lock object at 0x7f1b4fc307c0>, 'batch_size_validated': False, 'task_executor': <concurrent.futures.thread.ThreadPoolExecutor object at 0x7f1b5fe448b0>, 'embeddings_task_type_supported': True, 'get_embeddings_with_retry': <function TextEmbeddingModel.get_embeddings at 0x7f1b45337eb0>})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Text Embedding\n",
    "text_embedding_model = VertexAIEmbeddings(\n",
    "    model_name=TEXT_EMBEDDING_MODEL_NAME,\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    max_retries=6\n",
    ")\n",
    "\n",
    "text_embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "895a73bf-d1c5-4a28-ab1f-00cc06a6e4d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- ME_INDEX_ID:projects/884766917846/locations/us-central1/indexes/7340819014102286336\n",
      "- ME_INDEX_ENDPOINT_ID:projects/884766917846/locations/us-central1/indexEndpoints/2407125622317907968\n"
     ]
    }
   ],
   "source": [
    "# Get Matching Engine Index id and Endpoint id\n",
    "me_utils = MatchingEngineUtils(PROJECT_ID, REGION, ME_INDEX_NAME)\n",
    "ME_INDEX_ID, ME_INDEX_ENDPOINT_ID = me_utils.get_index_and_endpoint()\n",
    "\n",
    "print(f'- ME_INDEX_ID:{ME_INDEX_ID}\\n- ME_INDEX_ENDPOINT_ID:{ME_INDEX_ENDPOINT_ID}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b97460f8-06ae-4291-a930-e6105bbd813b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_google_vertexai.vectorstores.vectorstores.VectorSearchVectorStore at 0x7f1b4539ab30>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store = VectorSearchVectorStore.from_components(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=REGION,\n",
    "    gcs_bucket_name=f\"gs://{ME_EMBEDDING_GCS_DIR}\".split(\"/\")[2],\n",
    "    index_id=ME_INDEX_ID,\n",
    "    endpoint_id=ME_INDEX_ENDPOINT_ID,\n",
    "    stream_update=True,\n",
    "    embedding=text_embedding_model\n",
    ")\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ff24356-8257-4148-be70-39a854d13ba1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='This is a sample Resource Type', metadata={'fhir_patient_id': 'pid_111111111', 'fhir_resource_id': 'rid_111111111', 'fhir_resource_type': 'Test_Resource_type', 'neo4j_node_id': 'nid_111111111'}),\n",
       "  0.7039443254470825)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test VectorSearch is Connected\n",
    "query_text = 'sample Resource'\n",
    "response = vector_store.similarity_search_with_score(query=query_text, k=1)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8edb733-19b6-4c03-9e50-7af83b4e16dc",
   "metadata": {},
   "source": [
    "### 1.6. Google Vertex AI LLM Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed3d0d38-32c8-4f5a-86ce-44ec524ff618",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gemini-1.0-pro-001'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = VertexAI(model_name=VERTEX_AI_MODEL_NAME)\n",
    "llm.model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0666d80-ce8d-4b51-aaae-690d70201fbb",
   "metadata": {},
   "source": [
    "### 1.7. QA Without RAG\n",
    "\n",
    "Asking LLM a question without context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b73da16b-c9a8-44c8-9360-b67d23fbe3b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the Body Height of Benjamin360 Hintz995 and when was it measured?\n",
      "LLM Answer: The provided context does not mention anything about the body height of Benjamin360 Hintz995 or when it was measured, so I cannot answer this question from the provided context.\n"
     ]
    }
   ],
   "source": [
    "# Ask LLM a question\n",
    "question = \"What is the Body Height of Benjamin360 Hintz995 and when was it measured?\"\n",
    "\n",
    "no_rag_response = llm.invoke(question)\n",
    "\n",
    "print(f'Question: {question}')\n",
    "print(f'LLM Answer: {no_rag_response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d85e83-c32f-4fe5-a4ed-b6cc05679de1",
   "metadata": {},
   "source": [
    "## 2. Retrieval with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5a787f5-12e8-4633-a9f6-9b46716f99f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What is Benjamin360 Hintz995's latest Body Height and when was it measured?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ae2f905-57d4-4f84-aafc-58d247b1ce08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting Langchain Global Variables\n",
    "from langchain.globals import set_verbose, set_debug\n",
    "\n",
    "# Change to False if you do not want debug and execution information\n",
    "langchain_debug = True\n",
    "set_debug(langchain_debug)\n",
    "set_verbose(langchain_debug)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cd30d4-c484-4c16-a93b-d27c97951e6c",
   "metadata": {},
   "source": [
    "### 2.1. Step-01: Get Patient Name\n",
    "***Tip:*** \n",
    "- Minimize Cost & Latency - by first trying to extract patient name locally.\n",
    "- If regex does not help, then use LLM.\n",
    "- Fallback - Prompt user for Input\n",
    "- You can use less powerful LLMs for this to save cost. E.g. Gemma(offline) or Smaller LLMs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c010b54-9b78-4978-8276-8b6fa7087d71",
   "metadata": {},
   "source": [
    "<br>***Using Regex to extract patient name***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ab08fcf-cf91-45fd-83b7-8f1c7145b262",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get Patient Name Local Function using Python regex\n",
    "def extract_patient_name_with_custom_function(query: str) -> Optional[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Extracts patient's first and last name from the query using a regular expression.\n",
    "\n",
    "    Args:\n",
    "        query: The user's question or statement.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the extracted first and last names, or None if not found.\n",
    "    \"\"\"\n",
    "    # name_pattern = re.compile(r\"(?:Dr\\.|Mr\\.|Ms\\.|Mrs\\.)?\\s*([A-Z][a-zA-Z0-9']+)\\s+([A-Z][a-zA-Z0-9']*)\")  \n",
    "    name_pattern = re.compile(r\"(?:Dr\\.|Mr\\.|Ms\\.|Mrs\\.)?\\s*(\\b(?!What\\b)[A-Z][a-zA-Z0-9']*\\b)(?:\\s+([A-Z][a-zA-Z0-9']*)\\b)?\")\n",
    "    match = name_pattern.search(query)\n",
    "    if match:\n",
    "        first_name = match.group(1)\n",
    "        last_name = match.group(2) if match.group(2) else None\n",
    "        patient_name = {\"first_name\": first_name, \"last_name\": last_name}\n",
    "        return patient_name\n",
    "        # return None\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a81b8346-9f43-497e-9211-9380cc8b3a6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query: What is Benjamin360 Hintz995's latest Body Height and when was it measured?\n",
      "Patient Name = {'first_name': 'Benjamin360', 'last_name': \"Hintz995's\"}\n"
     ]
    }
   ],
   "source": [
    "print(f'User Query: {query}')\n",
    "name = extract_patient_name_with_custom_function(query)\n",
    "print(f'Patient Name = {name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf3fd48-721c-4efc-9026-28bef30e1c74",
   "metadata": {},
   "source": [
    "<br>***Manulaly get Patient name from user using Input Prompt***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "399fa6ca-afa8-4275-a27a-d3fc290ffdf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_patient_name_from_user():\n",
    "    while True:\n",
    "        user_input_name = input(\"Please enter the patient's full name: \")\n",
    "        confirmed = input(f\"Is '{user_input_name}' correct? (yes/no): \").lower()\n",
    "        if confirmed == 'yes':\n",
    "            name_parts = user_input_name.split()\n",
    "            patient_name = {\"first_name\": name_parts[0], \"last_name\": name_parts[-1] if len(name_parts) > 1 else None}\n",
    "            # print(type(patient_name))\n",
    "            return patient_name\n",
    "        elif confirmed == 'no':\n",
    "            continue\n",
    "        else:\n",
    "            print(\"Invalid input. Please enter 'yes' or 'no'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1fbdbd3-6369-4664-bae3-5951d01e2e1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query: What is Benjamin360 Hintz995's latest Body Height and when was it measured?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter the patient's full name:  Benjamin360 Hintz995\n",
      "Is 'Benjamin360 Hintz995' correct? (yes/no):  yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient Name = {'first_name': 'Benjamin360', 'last_name': 'Hintz995'}\n"
     ]
    }
   ],
   "source": [
    "print(f'User Query: {query}')\n",
    "name = get_patient_name_from_user()\n",
    "print(f'Patient Name = {name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf9abc9-26a7-4132-ae57-08d32b7c4387",
   "metadata": {},
   "source": [
    "<br> ***Use LLM to extract Patient Name***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26bcdf80-7826-4a83-a757-dc59736a9f5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JsonOutputParser(pydantic_object=<class '__main__.PatientName'>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Patient name Output Parser\n",
    "\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# Define your desired data structure.\n",
    "class PatientName(BaseModel):\n",
    "    first_name: str = Field(description=\"extracted first name or partial name of the patient\")\n",
    "    last_name: str = Field(description=\"extracted patient's last name of the patient or null\")\n",
    "    \n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "patient_name_parser = JsonOutputParser(pydantic_object=PatientName)\n",
    "patient_name_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a4b3f38-43a7-4349-98e0-c63fe8745d21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"first_name\": {\"title\": \"First Name\", \"description\": \"extracted first name or partial name of the patient\", \"type\": \"string\"}, \"last_name\": {\"title\": \"Last Name\", \"description\": \"extracted patient's last name of the patient or null\", \"type\": \"string\"}}, \"required\": [\"first_name\", \"last_name\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(patient_name_parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc695ca8-ab0c-4614-9251-a380ca8b344e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Formatted Prompt:\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "You are a medical assistant tasked with extracting patient names from text.\n",
      "The text may contain:\n",
      "1. The patient's full name (first and last)\n",
      "2. Only the patient's first name\n",
      "3. A partial name (e.g., a nickname, a last name with a prefix)\n",
      "4. Some Names will contain numbers and they are part of the name\n",
      "5. Names will contain special characters (e.g., apostrophes, hyphens)\n",
      "5. Names from diverse cultures and regions\n",
      "6. If you detect middle names, combine them into the last_name: last_name = '{All identified middle names} {last name}' (with a space between middle and last names)\n",
      "\n",
      "Identify and extract the patient's name information from the text. If you can identify both the first and last name, provide them. If you can only identify the first name or a partial name, provide that information and leave the missing part blank.\n",
      "\n",
      "Always provide the output in the following JSON format:\n",
      "{\"first_name\": \"[extracted first name or partial name]\", \"last_name\": \"[extracted last name or null]\"}\n",
      "\n",
      "Here's the text to analyze:\n",
      "\u001b[33;1m\u001b[1;3m{query}\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "patient_name_prompt_template = \"\"\"You are a medical assistant tasked with extracting patient names from text.\n",
    "The text may contain:\n",
    "1. The patient's full name (first and last)\n",
    "2. Only the patient's first name\n",
    "3. A partial name (e.g., a nickname, a last name with a prefix)\n",
    "4. Some Names will contain numbers and they are part of the name\n",
    "5. Names will contain special characters (e.g., apostrophes, hyphens)\n",
    "5. Names from diverse cultures and regions\n",
    "6. If you detect middle names, combine them into the last_name: last_name = '{{All identified middle names}} {{last name}}' (with a space between middle and last names)\n",
    "\n",
    "Identify and extract the patient's name information from the text. If you can identify both the first and last name, provide them. If you can only identify the first name or a partial name, provide that information and leave the missing part blank.\n",
    "\n",
    "Always provide the output in the following JSON format:\n",
    "{{\"first_name\": \"[extracted first name or partial name]\", \"last_name\": \"[extracted last name or null]\"}}\n",
    "\n",
    "Here's the text to analyze:\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "patient_name_prompt = ChatPromptTemplate.from_template(patient_name_prompt_template)\n",
    "patient_name_prompt.partial_variables = {\"format_instructions\": patient_name_parser.get_format_instructions()}\n",
    "\n",
    "# print('Output Parser Format Instructions:')\n",
    "# pprint(patient_name_prompt.partial_variables)\n",
    "\n",
    "print('\\nFormatted Prompt:')\n",
    "patient_name_prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cab351a6-345f-426f-a1d7-98166a0c076b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.exceptions import OutputParserException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3cc762c9-3e67-416c-ae5d-890fa9a836db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is Benjamin360 Hintz995's latest Body Height and when was it measured?\n",
      "patient_name_response: {'first_name': 'Benjamin360', 'last_name': 'Hintz995'}\n"
     ]
    }
   ],
   "source": [
    "# LangChain Debug\n",
    "langchain_debug = False\n",
    "set_debug(langchain_debug)\n",
    "set_verbose(langchain_debug)\n",
    "\n",
    "# Query\n",
    "print(f'Query: {query}')\n",
    "query_dict={'query': query}\n",
    "\n",
    "# Chain\n",
    "# fhir_chain = patient_name_prompt | llm | patient_name_parser\n",
    "fhir_chain = patient_name_prompt | llm | patient_name_parser\n",
    "\n",
    "try:\n",
    "    patient_name_response = fhir_chain.invoke(query_dict)\n",
    "except OutputParserException as e:\n",
    "    print(e)\n",
    "\n",
    "print(f'patient_name_response: {patient_name_response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa16b96-dc12-48b4-8837-f3d01fc94662",
   "metadata": {},
   "source": [
    "### 2.2. Step-02: Get Patient ID\n",
    "\n",
    "To find the relevant FHIR Patient Resource, even when dealing with potentially incomplete patient names from the user's query or LLM response, we:\n",
    "\n",
    "- **Construct the Query:** We strategically build our query using the same template as our pre-processed resource text representation. This ensures higher accuracy even with partial names.\n",
    "- **Perform Similarity Search:** This carefully crafted query is then used to search our VectorSearch Index, with the expectation that the top result is the matching FHIR Patient Resource.\n",
    "- **Extract ID:** Finally, we retrieve the fhir_patient_id directly from the metadata of the identified document.\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "978b7bce-677a-40e1-888d-1b1ad4ec8e12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_patient_id(patient_name :dict) -> str:\n",
    "    patient_vs_query_text = f\"\"\"The type of information in this entry is patient. The name use for this patient is official. The name family for this patient is {patient_name[\"last_name\"]}. The name given 0 for this patient is {patient_name[\"first_name\"]}\"\"\"\n",
    "    # Create Retriever\n",
    "    vs_retirever = vector_store.as_retriever(search_type=\"similarity\")\n",
    "\n",
    "    # Filter by resource_type = Patient\n",
    "    vs_filter = [Namespace(name=\"fhir_resource_type\", allow_tokens=[\"Patient\"])]\n",
    "\n",
    "    # k = 1 - We only want the top 1 result\n",
    "    vs_retirever.search_kwargs = {\"filter\": vs_filter, \"k\":1}\n",
    "    docs = vs_retirever.invoke(patient_vs_query_text)\n",
    "    \n",
    "    # print(f'Vector Search Results:\\n{docs}\\n')\n",
    "    \n",
    "    # Get patient id from Document Metadata\n",
    "    patient_id = docs[0].metadata['fhir_patient_id'][0]\n",
    "    return patient_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39e97553-dbfc-4e44-a4a7-804fffff9386",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is Benjamin360 Hintz995's latest Body Height and when was it measured?\n",
      "Patient ID: ae9b0221-6ac3-e43a-c3ff-b96a5edc31f0\n"
     ]
    }
   ],
   "source": [
    "print(f'Query: {query}')\n",
    "\n",
    "patient_id_response = get_patient_id(patient_name_response)\n",
    "\n",
    "print(f'Patient ID: {patient_id_response}')\n",
    "# print(f'{type(response)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205e3aff-db33-4b71-8e75-bfeaada72a69",
   "metadata": {},
   "source": [
    "### 2.3. Step-03: Identify FHIR Resource Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b339215-6cbd-4c30-ab54-07b1e32721c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JsonOutputParser(pydantic_object=<class '__main__.ResourceType'>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Resource Type Output Parser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# Define your desired data structure.\n",
    "class ResourceType(BaseModel):\n",
    "    resource_type: str = Field(description=\"extracted resource type name\")\n",
    "    \n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "resource_type_parser = JsonOutputParser(pydantic_object=ResourceType)\n",
    "resource_type_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "64a46c8f-80e4-46ee-9eb2-0042b1f1e0c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Formatted Prompt:\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "You are a healthcare specialist with deep knowledge of the FHIR standard.\n",
      "Your task is to identify the most appropriate FHIR resource type for the given query.\n",
      "Refer to the official FHIR resource guide at https://build.fhir.org/resourceguide.html. \n",
      "If needed, consult the detailed documentation linked from that guide.\n",
      "\n",
      "Return ONLY the resource type name if there is a clear match. If unsure, return \"Unknown\".\n",
      "\n",
      "Always provide the output in the following JSON format:\n",
      "{\"resource_type\": \"[extracted resource type name]\"}\n",
      "\n",
      "Here's the text to analyze:\n",
      "\u001b[33;1m\u001b[1;3m{query}\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FHIR_RESOURCE_LIST_URL = \"https://build.fhir.org/resourceguide.html\"\n",
    "\n",
    "resource_type_prompt_template = f\"\"\"You are a healthcare specialist with deep knowledge of the FHIR standard.\n",
    "Your task is to identify the most appropriate FHIR resource type for the given query.\n",
    "Refer to the official FHIR resource guide at {FHIR_RESOURCE_LIST_URL}. \n",
    "If needed, consult the detailed documentation linked from that guide.\n",
    "\n",
    "Return ONLY the resource type name if there is a clear match. If unsure, return \"Unknown\".\n",
    "\n",
    "Always provide the output in the following JSON format:\n",
    "{{{{\"resource_type\": \"[extracted resource type name]\"}}}}\n",
    "\n",
    "Here's the text to analyze:\n",
    "{{query}}\n",
    "\"\"\"\n",
    "resource_type_prompt = ChatPromptTemplate.from_template(resource_type_prompt_template)\n",
    "resource_type_prompt.partial_variables = {\"format_instructions\": resource_type_parser.get_format_instructions()}\n",
    "\n",
    "# print('Output Parser Format Instructions:')\n",
    "# pprint(resource_type_prompt.partial_variables)\n",
    "\n",
    "print('\\nFormatted Prompt:')\n",
    "resource_type_prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "439cc0d2-4684-44a9-9359-3e167129b90c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is Benjamin360 Hintz995's latest Body Height and when was it measured?\n",
      "Resource Type: Observation\n"
     ]
    }
   ],
   "source": [
    "# LangChain Debug\n",
    "debug_on = False\n",
    "set_debug(debug_on)\n",
    "set_verbose(debug_on)\n",
    "\n",
    "# Chain\n",
    "resource_type_chain = resource_type_prompt | llm | resource_type_parser\n",
    "resource_type_response = resource_type_chain.invoke({'query': query})\n",
    "\n",
    "print(f'Query: {query}')\n",
    "print(f'Resource Type: {resource_type_response[\"resource_type\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c570933-8446-4000-aee0-663858502631",
   "metadata": {},
   "source": [
    "### 2.4. Step-04: Vector Search\n",
    "\n",
    "In this step we perform a Similarity search on VertexAI VectoreSearch Index to retrieve FHIR Reources that match the user query.\n",
    "\n",
    "**Steps:**\n",
    "- Perform a Vector Search with Filters based on the retrieved patient_id and resource_type\n",
    "- Since FHIR Resources reference other resources, we also need to provide the referenced Resources to provide the full context to the LLM to imporve the accuracy of the respone. We do this by querying the Neo4J database to get immediate Neigbour resources for each resource returned by the \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "adc4e3cc-60f0-4330-b7f6-53e5befdd0d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def retrieve_relevant_resources(query: str, k: int, \n",
    "                                resource_type_text: str,\n",
    "                               patient_id: str) -> list[str]:\n",
    "    \n",
    "    # Create Retriever\n",
    "    vs_retirever = vector_store.as_retriever(search_type=\"similarity\")\n",
    "    \n",
    "    # Filter by fhir_resource_type and fhir_patient_id to retrieve only relevant FHIR Resources\n",
    "    vs_filter = [\n",
    "        Namespace(name=\"fhir_resource_type\", allow_tokens=[resource_type_text]),\n",
    "        Namespace(name=\"fhir_patient_id\", allow_tokens=[patient_id])\n",
    "    ]\n",
    "    \n",
    "    # print(f'retrieve_relevant_resources resource_type: {resource_type_text}')\n",
    "    if debug_on:\n",
    "        print(f'vs_filter:')\n",
    "        pprint(vs_filter)\n",
    "        print('\\n')\n",
    "        \n",
    "    # Retrieve all Resources based on above fitler\n",
    "    vs_retirever.search_kwargs = {\"filter\": vs_filter, 'k':k}\n",
    "    docs = vs_retirever.invoke(query)\n",
    "    \n",
    "    \n",
    "    # print(f'Retrieved Resource Documents:')\n",
    "    # pprint(docs)\n",
    "\n",
    "    # retrieved_resource_ids = [doc.metadata[\"fhir_resource_id\"][0] for doc in docs]\n",
    "    # return retrieved_resource_ids\n",
    "    return(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6f48f634-a667-45cb-9da4-8cef772e36d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is Benjamin360 Hintz995's latest Body Height and when was it measured?\n",
      "Total Resources Retrieved: 25\n"
     ]
    }
   ],
   "source": [
    "debug_on = False\n",
    "print(f'Query: {query}')\n",
    "docs = retrieve_relevant_resources(query,\n",
    "                                   k=25,\n",
    "                                  patient_id=patient_id_response,\n",
    "                                  resource_type_text=resource_type_response['resource_type'])\n",
    "\n",
    "retrieved_resource_ids = [doc.metadata['fhir_resource_id'][0] for doc in docs]\n",
    "\n",
    "# for doc in docs:\n",
    "#     resource_metadata = doc.metadata\n",
    "#     patient_id = resource_metadata['fhir_patient_id']\n",
    "#     resource_id = resource_metadata['fhir_resource_id']\n",
    "#     resource_type = resource_metadata['fhir_resource_type']    \n",
    "#     print(f'patient_id: {patient_id}\\t resource_id:{resource_id}\\t resource_type:{resource_type}')\n",
    "#     # print(doc.page_content)\n",
    "\n",
    "print(f'Total Resources Retrieved: {len(docs)}')\n",
    "# print(f'ResourcesIds List: {retrieved_resource_ids}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace65cdb-63b3-4b99-b070-87382616cc30",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.5. RAG with normal context\n",
    "\n",
    "*With the above retrieved resources as llm context, let us try to ask the LLM user query and check its response.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6194f180-6268-4ae7-8c53-a7c1c90008cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "\n",
      "System: The context below contains entries about the patient's healthcare. \n",
      "Please limit your answer to the information provided in the context. Do not make up facts.\n",
      "Please limit your answers only about the patient in the user question. If you do not find the patient name in the context.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "If you are asked about the patient's name and one the entries is of type patient, you should look for the first given name and family name and answer with: [given] [family]\n",
      "----------------\n",
      "\u001b[33;1m\u001b[1;3m{context}\u001b[0m\n",
      "\n",
      "Here's the text to analyze:\n",
      "\u001b[33;1m\u001b[1;3m{query}\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_query_prompt='''\n",
    "System: The context below contains entries about the patient's healthcare. \n",
    "Please limit your answer to the information provided in the context. Do not make up facts.\n",
    "Please limit your answers only about the patient in the user question. If you do not find the patient name in the context.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "If you are asked about the patient's name and one the entries is of type patient, you should look for the first given name and family name and answer with: [given] [family]\n",
    "----------------\n",
    "{context}\n",
    "\n",
    "Here's the text to analyze:\n",
    "{query}\n",
    "'''\n",
    "\n",
    "user_query_prompt = ChatPromptTemplate.from_template(user_query_prompt)\n",
    "user_query_prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1487a1e2-3ad9-4b20-ad52-d418cdac2c04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query: What is Benjamin360 Hintz995's latest Body Height and when was it measured?\n",
      "Response: I cannot find the patient name Benjamin360 Hintz995 in the context, so I cannot answer this question.\n"
     ]
    }
   ],
   "source": [
    "# LangChain Debug\n",
    "debug_on = False\n",
    "set_debug(debug_on)\n",
    "set_verbose(debug_on)\n",
    "\n",
    "# get all page_content of docs\n",
    "docs_page_contents_list = [doc.page_content for doc in docs]\n",
    "docs_page_contents = '\\n\\n'.join(docs_page_contents_list)\n",
    "# print(docs_page_contents)\n",
    "\n",
    "prompt_inputs = {'query': query, 'context': docs_page_contents}\n",
    "\n",
    "# Chain\n",
    "# print(f'Prompt Inputs:')\n",
    "# pprint(prompt_inputs)\n",
    "\n",
    "print(f'User Query: {query}')\n",
    "user_query_chain = user_query_prompt | llm\n",
    "user_query_response = user_query_chain.invoke(prompt_inputs)\n",
    "print(f'Response: {user_query_response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c663264-15aa-4e56-b966-b99e290911aa",
   "metadata": {},
   "source": [
    "<br>***The LLM couldn't answer the query because the necessary Patient FHIR Resource wasn't retrieved, leaving it without the required context.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216798cc-39ed-45ea-959b-fe265dcc6efb",
   "metadata": {},
   "source": [
    "### 2.6. Step-05: Enhanced Context \n",
    "\n",
    "<br>***Fetch referenced resources for additional context for LLM***\n",
    "\n",
    "To enhance the LLM's accuracy in answering user questions, it's crucial to fetch the text representation of all referenced FHIR resources. For instance, an Observation resource might reference Specimen, Device, Procedure, etc. This provides complete context to the LLM, enabling it to accurately answer queries involving these linked resources.\n",
    "\n",
    "Additionally, this ensures the inclusion of key information like patient names from the referenced Patient resource, preventing incorrect responses stating that the context lacks information about the patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a3c23989-44ac-4680-ba5b-81fcee79c58b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fetch_enhanced_context(in_resource_ids: list) -> str:\n",
    "    \n",
    "    # Fetch relevant text from the graph database\n",
    "    cipher = f\"\"\"\n",
    "    MATCH (node: resource)\n",
    "    WHERE node.id IN {in_resource_ids}\n",
    "\n",
    "    OPTIONAL MATCH (node)-[r]-(neighbor :resource)\n",
    "    WITH COLLECT(DISTINCT node) + COLLECT(DISTINCT neighbor) AS allNodes\n",
    "    UNWIND allNodes as uniqueNode\n",
    "    RETURN uniqueNode.text\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = graph.query(cipher)[0]\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error in Graph Query: {e}\")\n",
    "\n",
    "    relevant_resource_text_list = [resource_id[0] for resource_id in response]\n",
    "    \n",
    "    # print(f'Number of resources matching query: {len(relevant_resource_text_list)}')\n",
    "    # print(f'Enhanced Context Text:')\n",
    "    # pprint(relevant_resource_text_list)\n",
    "    \n",
    "    return relevant_resource_text_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0dafbba2-6909-4791-a90c-d59ac5b36889",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "enchanced_context_resource_text len: 36\n"
     ]
    }
   ],
   "source": [
    "enhanced_context_resources = fetch_enhanced_context(retrieved_resource_ids)\n",
    "print(type(enhanced_context_resources))\n",
    "print(f'enchanced_context_resource_text len: {len(enhanced_context_resources)}')\n",
    "# print(f'enhanced_context_resources: {enhanced_context_resources}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba998d9-2533-4531-96d3-044c8eb61bba",
   "metadata": {},
   "source": [
    "### 2.7. RAG with Enhanced Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "53749f61-b9a0-4301-ba9c-f7f39d993325",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query: What is Benjamin360 Hintz995's latest Body Height and when was it measured?\n",
      "Response: Benjamin360 Hintz995's latest Body Height measurement is 173.1 cm and it was measured on 05/19/2022.\n"
     ]
    }
   ],
   "source": [
    "# LangChain Debug\n",
    "debug_on = False\n",
    "set_debug(debug_on)\n",
    "set_verbose(debug_on)\n",
    "\n",
    "enhanced_context_resources = [res_text for res_text in enhanced_context_resources if res_text is not None]\n",
    "enchanced_context_resources_text = '\\n\\n'.join(enhanced_context_resources)\n",
    "# print(enhanced_context_resources)\n",
    "\n",
    "current_datetime = datetime.now(timezone.utc).astimezone(timezone(offset=timedelta(hours=5, minutes=30)))\n",
    "current_datetime_str = current_datetime.strftime(\"%m/%d/%Y\")\n",
    "    \n",
    "prompt_inputs = {'query': query, 'CurrentDateTime': current_datetime_str,'context': enchanced_context_resources_text}\n",
    "\n",
    "# Chain\n",
    "# print(f'Prompt Inputs:')\n",
    "# pprint(prompt_inputs)\n",
    "\n",
    "print(f'User Query: {query}')\n",
    "user_query_chain = user_query_prompt | llm\n",
    "user_query_response = user_query_chain.invoke(prompt_inputs)\n",
    "print(f'Response: {user_query_response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2139b11e-25d5-458e-96bf-da7921ae1cc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Prompt Engineering\n",
    "\n",
    "Prompt Engineering is the most critical component of designing LLM powered applications. The below prompt shows an example of all the components that needs to be considered while designing your prompts. Few Components are:\n",
    "\n",
    "***Prompt Engineering Best Practices for Maximizing LLM Performance:***\n",
    "- **Be Clear and Specific:** Use unambiguous language and explicit instructions.\n",
    "- **Provide Context:** Give the LLM relevant background information for accurate responses.\n",
    "- **Iterate and Refine:** Experiment with different prompts and improve them over time.\n",
    "- **Handle Dates/Time:** Include current time, guide relative time calculation, and consider time zones.\n",
    "- **Specify Output Format:** Clearly define the desired output format and provide examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bca6bf00-8569-4fb5-9d1f-c2f39d5d91ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "\n",
      "System:Â \n",
      "You are a Healthcare AI assistant. Your task is to respond to Doctors queries based on patient information from FHIR (Fast Healthcare Interoperability Resources) data.\n",
      "\n",
      "**Context Handling:**\n",
      "\n",
      "1. The context below contains entries about the patient's healthcare in FHIR format.\n",
      "2. Identify and parse relevant FHIR resources within the context (e.g., Patient, Observation, Encounter).\n",
      "3. Utilize the standard FHIR terminology and codes (e.g., LOINC for observations) to extract specific information.\n",
      "4. Limit your answer to the information provided in the context. Do not make up facts.\n",
      "5. Focus your answers on the patient specified in the user question. \n",
      "6. If you don't know the answer, simply state that you don't have enough information.\n",
      "7. Ensure to look for the correct FHIR resource type in context to answer the query. For Example, to answer questions about claims history, search for FHIR resource of type 'Claim'\n",
      "8. Utilize the 'CurrentDateTime' value in the context to calculate relative time periods (e.g., \"last week\") for queries referencing them.\n",
      "\n",
      "**Date Handling:**\n",
      "\n",
      "1. Pay very close attention to the dates in the context and user query.\n",
      "2. Prioritize information from the most recent dates when responding to queries without a specified date.\n",
      "3. Compare dates in the context and user query to determine the temporal relationship between events.\n",
      "4. Use the 'CurrentDateTime' value, which is in MM/DD/YYYY format, to calculate relative time periods and filter relevant FHIR resources based on those periods.\n",
      "5. Note that dates in the context are formatted as MM/DD/YYYY (e.g., 10/22/2015)\n",
      "\n",
      "**Output Formatting:**\n",
      "\n",
      "Before printing verify that you have considered the date and time in your response meets date time criteria in the user query (if mentioned).  \n",
      "\n",
      "1. Respond to the user question with the above in mind.\n",
      "2. Include the patient's name (given name and family name) in your response.\n",
      "3. Format your output in markdown for clarity.\n",
      "4. Make the patients name Bold.\n",
      "5. Format the data into Markdown table with clear headers for information you think can be better represented in a table.\n",
      "\n",
      "**Example Output:**\n",
      "   - For vital signs: \"[Patient Name]'s [Observation Name] was [Value] [Unit] on [Date].\"\n",
      "   - For encounters: \"[Patient Name] had a [Encounter Type] on [Date] (reason: [Reason if available]).\"\n",
      "\n",
      "----------------\n",
      "Today's date - CurrentDateTime = \u001b[33;1m\u001b[1;3m{CurrentDateTime}\u001b[0m\n",
      "\n",
      "Context about the Patient\n",
      "\u001b[33;1m\u001b[1;3m{context}\u001b[0m\n",
      "\n",
      "User Question:\n",
      "\u001b[33;1m\u001b[1;3m{query}\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_query_prompt='''\n",
    "System:Â \n",
    "You are a Healthcare AI assistant. Your task is to respond to Doctors queries based on patient information from FHIR (Fast Healthcare Interoperability Resources) data.\n",
    "\n",
    "**Context Handling:**\n",
    "\n",
    "1. The context below contains entries about the patient's healthcare in FHIR format.\n",
    "2. Identify and parse relevant FHIR resources within the context (e.g., Patient, Observation, Encounter).\n",
    "3. Utilize the standard FHIR terminology and codes (e.g., LOINC for observations) to extract specific information.\n",
    "4. Limit your answer to the information provided in the context. Do not make up facts.\n",
    "5. Focus your answers on the patient specified in the user question. \n",
    "6. If you don't know the answer, simply state that you don't have enough information.\n",
    "7. Ensure to look for the correct FHIR resource type in context to answer the query. For Example, to answer questions about claims history, search for FHIR resource of type 'Claim'\n",
    "8. Utilize the 'CurrentDateTime' value in the context to calculate relative time periods (e.g., \"last week\") for queries referencing them.\n",
    "\n",
    "**Date Handling:**\n",
    "\n",
    "1. Pay very close attention to the dates in the context and user query.\n",
    "2. Prioritize information from the most recent dates when responding to queries without a specified date.\n",
    "3. Compare dates in the context and user query to determine the temporal relationship between events.\n",
    "4. Use the 'CurrentDateTime' value, which is in MM/DD/YYYY format, to calculate relative time periods and filter relevant FHIR resources based on those periods.\n",
    "5. Note that dates in the context are formatted as MM/DD/YYYY (e.g., 10/22/2015)\n",
    "\n",
    "**Output Formatting:**\n",
    "\n",
    "Before printing verify that you have considered the date and time in your response meets date time criteria in the user query (if mentioned).  \n",
    "\n",
    "1. Respond to the user question with the above in mind.\n",
    "2. Include the patient's name (given name and family name) in your response.\n",
    "3. Format your output in markdown for clarity.\n",
    "4. Make the patients name Bold.\n",
    "5. Format the data into Markdown table with clear headers for information you think can be better represented in a table.\n",
    "\n",
    "**Example Output:**\n",
    "   - For vital signs: \"[Patient Name]'s [Observation Name] was [Value] [Unit] on [Date].\"\n",
    "   - For encounters: \"[Patient Name] had a [Encounter Type] on [Date] (reason: [Reason if available]).\"\n",
    "\n",
    "----------------\n",
    "Today's date - CurrentDateTime = {CurrentDateTime}\n",
    "\n",
    "Context about the Patient\n",
    "{context}\n",
    "\n",
    "User Question:\n",
    "{query}\n",
    "'''\n",
    "\n",
    "user_query_prompt = ChatPromptTemplate.from_template(user_query_prompt)\n",
    "user_query_prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c9e32285-6462-4424-896d-efce43d166d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query: What is Benjamin360 Hintz995's latest Body Height and when was it measured?\n",
      "Response: **Benjamin360 Hintz995**'s latest Body Height is 173.1 cm. It was measured on 05/19/2022.\n"
     ]
    }
   ],
   "source": [
    "# LangChain Debug\n",
    "debug_on = False\n",
    "set_debug(debug_on)\n",
    "set_verbose(debug_on)\n",
    "\n",
    "enhanced_context_resources = [res_text for res_text in enhanced_context_resources if res_text is not None]\n",
    "enchanced_context_resources_text = '\\n\\n'.join(enhanced_context_resources)\n",
    "# print(enhanced_context_resources)\n",
    "\n",
    "current_datetime = datetime.now(timezone.utc).astimezone(timezone(offset=timedelta(hours=5, minutes=30)))\n",
    "current_datetime_str = current_datetime.strftime(\"%m/%d/%Y\")\n",
    "    \n",
    "prompt_inputs = {'query': query, 'CurrentDateTime': current_datetime_str,'context': enchanced_context_resources_text}\n",
    "\n",
    "# Chain\n",
    "# print(f'Prompt Inputs:')\n",
    "# pprint(prompt_inputs)\n",
    "\n",
    "print(f'User Query: {query}')\n",
    "user_query_chain = user_query_prompt | llm\n",
    "user_query_response = user_query_chain.invoke(prompt_inputs)\n",
    "print(f'Response: {user_query_response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff17fa50-11c8-46b3-8f70-e1fc5ecd7cc6",
   "metadata": {},
   "source": [
    "## 4. Bringing it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b69ac212-f0c9-4641-8d24-edfc1d42e1ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "debug_on = False\n",
    "def answer_fhir_query(user_query: str):\n",
    "    \n",
    "    # Get Patient Name Chain\n",
    "    \n",
    "    # print(f'patient_name_response: {patient_name_response}')\n",
    "    \n",
    "    ## First try LLM to extracht name else fallback to user prompt\n",
    "    try: \n",
    "        patient_name_chain = patient_name_prompt | llm | patient_name_parser\n",
    "        patient_name_response = patient_name_chain.invoke({'query': user_query})\n",
    "        patient_name = patient_name_response\n",
    "        \n",
    "    except OutputParserException as e:\n",
    "        print(\"I couldn't find the Patient name.\")\n",
    "        patient_name = get_patient_name_from_user()      \n",
    "    \n",
    "    if debug_on:\n",
    "        print('Patient Name Info:')\n",
    "        print(f'patient_name type: {type(patient_name)}')\n",
    "        print(f'patient_name: {patient_name}')\n",
    "        print('\\n')\n",
    "        \n",
    "    # Get Patient_Id\n",
    "    patient_id = get_patient_id(patient_name)\n",
    "    if debug_on:\n",
    "        print('Patient Id Info:')\n",
    "        print(f'patient_id: {patient_id}')\n",
    "        print('\\n')\n",
    "        \n",
    "    # Identify Resource Type\n",
    "    resource_type_chain = resource_type_prompt | llm | resource_type_parser\n",
    "    resource_type_response = resource_type_chain.invoke(user_query)\n",
    "    resource_type_text=resource_type_response['resource_type']\n",
    "\n",
    "    \n",
    "    if debug_on:\n",
    "        print('Resource Type Info:')\n",
    "        print(f'resource_type: {resource_type_response[\"resource_type\"]}')\n",
    "        print('\\n')\n",
    "        \n",
    "    # Vector Search: Get Relevant Resources based on user query\n",
    "    k=25\n",
    "    vs_search_resource_docs = retrieve_relevant_resources(user_query, \n",
    "                                                          k,\n",
    "                                                          patient_id=patient_id,\n",
    "                                                          resource_type_text=resource_type_text)\n",
    "    \n",
    "    # for doc in vs_search_resource_docs:\n",
    "    #     print(doc.metadata['fhir_resource_id'], \"-\", doc.metadata['fhir_resource_type'])\n",
    "    \n",
    "    vs_search_resource_ids = [doc.metadata['fhir_resource_id'][0] for doc in vs_search_resource_docs]\n",
    "    \n",
    "    if debug_on:\n",
    "        print('\\n')\n",
    "        print('Resource Ids retrieved from Vector Search:')\n",
    "        print(f'vs_search_resource_ids len: {len(vs_search_resource_ids)}')\n",
    "        # print(f'vs_search_resource_ids: {vs_search_resource_ids}')\n",
    "        print('\\n')\n",
    "    \n",
    "    # Get Current Date and Time in format MM/DD/YYYY\n",
    "    current_datetime = datetime.now(timezone.utc).astimezone(timezone(offset=timedelta(hours=5, minutes=30)))\n",
    "    current_datetime_str = current_datetime.strftime(\"%m/%d/%Y\")\n",
    "    \n",
    "    if resource_type_text == 'Patient':\n",
    "        context_text = '\\n\\n'.join([doc.page_content for doc in vs_search_resource_docs])\n",
    "        prompt_inputs = {'query': user_query, 'CurrentDateTime': current_datetime_str,'context': context_text}\n",
    "        \n",
    "    else:\n",
    "        # Neo4J query - for getting enhanced context\n",
    "        enhanced_context = fetch_enhanced_context(in_resource_ids=vs_search_resource_ids)\n",
    "        enhanced_context_text = '\\n\\n'.join([res_text for res_text in enhanced_context if res_text is not None])\n",
    "        prompt_inputs = {'query': user_query, 'CurrentDateTime': current_datetime_str,'context': enhanced_context_text}\n",
    "\n",
    "        if debug_on:\n",
    "            print('\\n')\n",
    "            print(f'# of Enhanced Context Resources: {len(enhanced_context)}')\n",
    "            print(f'Enhanced Context Text:')\n",
    "            pprint(enhanced_context_text)\n",
    "            print('\\n')\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    # Finally calling the LLM to answer user query\n",
    "   \n",
    "    user_query_chain = user_query_prompt | llm\n",
    "    user_query_response = user_query_chain.invoke(prompt_inputs)\n",
    "    \n",
    "    return user_query_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8e75a5db-74c3-41ff-991c-5411b03ee0f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ                                                   <span style=\"font-weight: bold\">User Query</span>                                                    â”ƒ\n",
       "â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ                                                   \u001b[1mUser Query\u001b[0m                                                    â”ƒ\n",
       "â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is Benjamin360 Hintz995's latest Body Height and when was it measured?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ                                                  <span style=\"font-weight: bold\">LLM Response</span>                                                   â”ƒ\n",
       "â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ                                                  \u001b[1mLLM Response\u001b[0m                                                   â”ƒ\n",
       "â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Benjamin360 Hintz995</span>'s latest Body Height measurement is 173.1 cm. It was measured on 05/19/2022.                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mBenjamin360 Hintz995\u001b[0m's latest Body Height measurement is 173.1 cm. It was measured on 05/19/2022.                  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich.console import Console\n",
    "from rich.markdown import Markdown\n",
    "\n",
    "console = Console()\n",
    "\n",
    "langchain_debug = False\n",
    "set_debug(langchain_debug)\n",
    "set_verbose(langchain_debug)\n",
    "\n",
    "# query = \"What is the Body Height of Benjamin360 Hintz995 and when was it measured?\"\n",
    "# query = \"What is the Body Weight of Benjamin360 Hintz995 and when was it measured?\"\n",
    "# query = \"Tell me about the last 5 Benjamin360's Procedures?\"\n",
    "# query = \"Tell me about observations performed by Benjamin360 in the last 2 years?\"\n",
    "# query = \"What allergies does Benjamin360 have?\"\n",
    "\n",
    "console.print(Markdown('# User Query'))\n",
    "print(query)\n",
    "\n",
    "console.print(Markdown('# LLM Response'))\n",
    "llm_user_query_response = (answer_fhir_query(query))\n",
    "console.print(Markdown(llm_user_query_response))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dce5d55-6c56-4c92-81bd-8fc18f80a9fa",
   "metadata": {},
   "source": [
    "## 5. Generate a Patient Summary\n",
    "\n",
    "Using our RAG application, we'll create a Patient Summary with the following sections:\n",
    "- Demographics\n",
    "- Medical History\n",
    "- Medications\n",
    "- Allergies\n",
    "- Immunizations\n",
    "- Vital Signs\n",
    "\n",
    "Each section will be populated using ***carefully designed queries and well designed prompt***. This approach not only ***improves response accuracy but also minimizes LLM calls, reducing overall costs***.\n",
    "\n",
    "The results will be dynamically combined into a Markdown report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "81913be9-6c00-4491-8cb3-e7282512d6fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_formatting_instructions = \"List the ouput in a Markdown table format.\"\n",
    "\n",
    "vital_signs_prompt = \"\"\"please provide a summary of latest vital signs. Include the latest value, unit of measurement, and date taken (MM/DD/YYYY) for each of the following vital signs:\n",
    "\n",
    "*   Body Height (LOINC code 8302-2)\n",
    "*   Body Weight (LOINC code 29463-7)\n",
    "*   Body Mass Index (BMI) [Ratio]\n",
    "*   Body temperature (LOINC code 8310-5)\n",
    "*   Systolic Blood Pressure (LOINC code 8480-6)\n",
    "*   Diastolic Blood Pressure (LOINC code 8462-4)\n",
    "*   Heart rate (LOINC code 8867-4)\n",
    "*   Respiratory rate (LOINC code 9279-1)\n",
    "*   Oxygen saturation in Arterial blood by Pulse oximetry (LOINC code 59408-5)\n",
    "\n",
    "If a particular vital sign is not found in the records, please indicate so with 'N/A'. Present the information in a markdown table with the columns 'Vital Sign', 'Value', 'Unit', and 'Date Taken'.\"\"\"\n",
    "\n",
    "patient_summary_query = [\n",
    "    {'Demographics': \"please provide the following demographic information, if available in the context: full name, date of birth, gender, primary phone number, and home address.\"},\n",
    "    {'Medical History': \"Please provide a summary of all active Conditions. List the onset date, status and verification. List the ouput in a Markdown table format. If no active conditions are found, please state 'No active medical conditions found'.\"},\n",
    "    {'Medications': \"please list all current medications, including the medication name, dosage instructions, and status (e.g., active, completed). Present this informationin a markdown table with the columns 'Medication Name', 'Dosage', and 'Status'. If no current medications are found, please state 'No current medications found'.\"},\n",
    "    {'Allergies':\"please list all known allergies, including the allergen name and severity. Present this information in a markdown table with the columns 'Allergen' and 'Severity'. If no allergies are found, please state 'No known allergies found'.\"},\n",
    "    {'Immunizations':\"please list all immunizations for, including the vaccine code, date administered, and status. Present this information in a markdown table with the columns 'Vaccine Code', 'Date Administered', and 'Status'. If no immunizations are found, please state 'No immunizations found'.\"},\n",
    "    {'Vital Signs': vital_signs_prompt},\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "40f95f5d-eba3-4c81-b8c6-acba48e3f382",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "langchain_debug = False\n",
    "set_debug(langchain_debug)\n",
    "set_verbose(langchain_debug)\n",
    "def generate_patient_summary(patient_name):\n",
    "    \n",
    "    report_text = '# Patient Summary\\n'\n",
    "    \n",
    "    # console.print(Markdown('# Patient Summary'))\n",
    "    \n",
    "    # patient_context = f'For the given patient: {patient_name}, '\n",
    "    patient_context = f\"\\nThe patient name is {patient_summary_name}.\"\n",
    "    \n",
    "    for section in patient_summary_query:\n",
    "        section_title = list(section.keys())[0]\n",
    "        \n",
    "        print(f'Processing Section: {section_title}')\n",
    "\n",
    "        # report_text = report_text + '___\\n'\n",
    "        section_header = f'___\\n## {section_title}\\n___\\n\\n'\n",
    "\n",
    "        report_text = report_text + section_header\n",
    "        # report_text = report_text + '___\\n'\n",
    "        # console.print(Markdown(section_header))\n",
    "\n",
    "        section_question = list(section.values())[0]\n",
    "        # llm_user_query_response = patient_context + section_question + output_formatting_instructions\n",
    "        # llm_user_query_response = answer_fhir_query(patient_context + section_question + output_formatting_instructions)\n",
    "        summary_user_query = section_question + output_formatting_instructions + patient_context\n",
    "        llm_user_query_response = answer_fhir_query(summary_user_query)\n",
    "        report_text = report_text + llm_user_query_response + '\\n'\n",
    "\n",
    "        # console.print(Markdown(llm_user_query_response))\n",
    "    \n",
    "    # console.print(Markdown(report_text))\n",
    "    report_end_text = '___\\n***<p style=\"text-align: center;\">END OF REPORT</p>***\\n___\\n'\n",
    "    report_text = report_text + report_end_text\n",
    "    \n",
    "    \n",
    "    \n",
    "    return report_text\n",
    "        \n",
    "#generate_patient_summary('Akiko835 Larkin917')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "01b5fbfc-c438-41e9-85ee-fd240e780820",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Section: Demographics\n",
      "Processing Section: Medical History\n",
      "Processing Section: Medications\n",
      "I couldn't find the Patient name.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter the patient's full name:  Carey440 Stroman228\n",
      "Is 'Carey440 Stroman228' correct? (yes/no):  yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Section: Allergies\n",
      "Processing Section: Immunizations\n",
      "Processing Section: Vital Signs\n",
      "The patient summary file \"Carey440 Stroman228.md\" has been generated\n"
     ]
    }
   ],
   "source": [
    "# patient_summary_name = 'Akiko835 Larkin917'\n",
    "patient_summary_name = 'Carey440 Stroman228' # has allergies\n",
    "\n",
    "\n",
    "patient_summary_response = generate_patient_summary(patient_summary_name) \n",
    "\n",
    "patient_summary_md_file = f'{patient_summary_name}.md'\n",
    "with open (patient_summary_md_file, 'w') as f:\n",
    "    f.write(patient_summary_response)\n",
    "    \n",
    "print(f'The patient summary file \"{patient_summary_name}.md\" has been generated')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f1c361-8eaf-4e11-82d0-e31f69bee7ba",
   "metadata": {},
   "source": [
    "<br> **Note**: \n",
    "\n",
    "Few Patient Summary sections might not get popolated e.g. Vital Signs - Systolic blood pressure.\n",
    "For these scenarios you can:\n",
    "- Prompt the LLM for each field of the section.\n",
    "- Create dedicated LLMChains that can populate each section,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc078cc-7909-419e-8eaf-5c3480bd6674",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## 6. Cleaning Up\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>âš ï¸ Important: To avoid incurring charges, please delete the Google Cloud resources used in this tutorial. âš ï¸</b>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc24654-0a0d-451f-bddf-60902564ee99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CLEANUP_RESOURCES = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68767fc-5733-4ed8-837a-e78a1988a604",
   "metadata": {},
   "source": [
    "### Delete Neo4J Docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3603ef-7c35-4f9a-9372-d8479b21fee1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Wipe Neo4J Database\n",
    "graph = Graph(NEO4J_URL, NEO4J_USER, NEO4J_PASSWORD)\n",
    "if CLEANUP_RESOURCES:\n",
    "    graph.wipe_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1aab300-2964-4597-adec-0c905b71d919",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DELETE NEO4J CONTAINER\n",
    "if CLEANUP_RESOURCES:\n",
    "    ! docker stop testneo4j\n",
    "    ! docker rm -fv testneo4j\n",
    "    ! sudo rm -rf $HOME/neo4j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd5ebea-fb76-45f3-a07f-36d788d942a8",
   "metadata": {},
   "source": [
    "### Delete Vector Search Indexes & Index-Endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b30c5a3-63e7-403e-873f-f8332b10bf66",
   "metadata": {},
   "source": [
    "- Delete ME Vector Search Index and Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511aab38-d457-47ca-9000-1dd66d97dfb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "me_utils = MatchingEngineUtils(PROJECT_ID, REGION, ME_INDEX_NAME)\n",
    "ME_INDEX_ID, ME_INDEX_ENDPOINT_ID = me_utils.get_index_and_endpoint()\n",
    "\n",
    "# Delete Endpoint\n",
    "if CLEANUP_RESOURCES and \"me_utils\" in globals():\n",
    "    print(\n",
    "        f\"Undeploying all deployed indexes and deleting the index endpoint {ME_INDEX_ENDPOINT_ID}\"\n",
    "    )\n",
    "    me_utils.delete_index_endpoint()\n",
    "\n",
    "# Delete Index     \n",
    "if CLEANUP_RESOURCES and \"me_utils\" in globals():\n",
    "    print(f\"Deleting the index {ME_INDEX_ID}\")\n",
    "    me_utils.delete_index()    \n",
    "\n",
    "# Delete Bucket    \n",
    "if CLEANUP_RESOURCES:\n",
    "    # Delete contents of the bucket \n",
    "    ! gsutil -m rm -r gs://{ME_EMBEDDING_GCS_DIR}\n",
    "    ! gsutil rb gs://{ME_EMBEDDING_GCS_DIR}\n",
    "\n",
    "print('Vector Search and GCS Bucket Cleaning complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b60e88-c2fb-41d2-b200-77e66897c188",
   "metadata": {},
   "source": [
    "- Delete ME_ENHANCED Vector Search Index and Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8378ec-f187-435a-90d5-107ec091d723",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "me_utils_enhanced = MatchingEngineUtils(PROJECT_ID, REGION, ME_ENHANCED_CONTEXT_INDEX_NAME)\n",
    "ME_ENHANCED_INDEX_ID, ME_ENHANCED_INDEX_ENDPOINT_ID = me_utils_enhanced.get_index_and_endpoint()\n",
    "\n",
    "# Delete Endpoint\n",
    "if CLEANUP_RESOURCES and \"me_utils_enhanced\" in globals():\n",
    "    print(\n",
    "        f\"Undeploying all deployed indexes and deleting the index endpoint {ME_ENHANCED_INDEX_ENDPOINT_ID}\"\n",
    "    )\n",
    "    me_utils_enhanced.delete_index_endpoint()\n",
    "\n",
    "# Delete Index    \n",
    "if CLEANUP_RESOURCES and \"me_utils_enhanced\" in globals():\n",
    "    print(f\"Deleting the index {ME_ENHANCED_INDEX_ID}\")\n",
    "    me_utils_enhanced.delete_index()\n",
    "\n",
    "# Delete Bucket\n",
    "if CLEANUP_RESOURCES:\n",
    "    ! gsutil -m rm -r gs://{ME_ENHANCED_EMBEDDING_GCS_DIR}\n",
    "    ! gsutil rb gs://{ME_ENHANCED_EMBEDDING_GCS_DIR}"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
