{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a181094-0c00-4366-9012-3d51de9f14cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# FHIR RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb26615d-6727-452e-a865-87e0664238d6",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e95ee05-8241-4bcf-88de-dff23dff503f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Authenticate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee1aca4f-86dc-4332-a7df-595dfee9b5aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Authenticate Notebook\n",
    "\n",
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7d6abe-74e8-4868-ad98-96948d56de8e",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9724e722-e716-4046-abba-3a83d443ede5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GCP Parameters\n",
    "PROJECT_ID = \"propane-crawler-363311\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"  # @param {type: \"string\"}\n",
    "\n",
    "# Neo4J Connection Parameters\n",
    "NEO4J_URL=\"bolt://localhost:7687\" # @param {type:\"string\"}\n",
    "NEO4J_USER=\"neo4j\" # @param {type:\"string\"}\n",
    "NEO4J_PASSWORD=\"password\" # @param {type:\"string\"}\n",
    "\n",
    "# Dimension Vertex PaLM Text Embedding\n",
    "ME_DIMENSIONS = 768 # @param {type:\"integer\"} \n",
    "ME_DISTANCE_MEASURE_TYPE = \"DOT_PRODUCT_DISTANCE\" # @param {type:\"string\"} \n",
    "\n",
    "# Update to bigger SHARDS for larger data volumes & performance\n",
    "# Doc - https://cloud.google.com/vertex-ai/docs/vector-search/create-manage-index\n",
    "ME_SHARD_SIZE = \"SHARD_SIZE_SMALL\" # @param [\"SHARD_SIZE_SMALL\", \"SHARD_SIZE_MEDIUM\", \"SHARD_SIZE_LARGE\"] \n",
    "\n",
    "# Vertex AI Vector Search (MatchingEngine) Endpoint Parameters\n",
    "# Doc - https://cloud.google.com/vertex-ai/docs/vector-search/create-manage-index\n",
    "\n",
    "# The machine types that you can use to deploy your index\n",
    "ME_ENDPOINT_MACHINE_TYPE = \"e2-standard-2\" # @param [\"n1-standard-16\", \"n1-standard-32\", \"e2-standard-2\", \"e2-standard-16\", \"e2-highmem-16\", \"n2d-standard-32\"] \n",
    "\n",
    "ME_ENDPOINT_MIN_REPLICA_COUNT = 2 # @param {type:\"integer\"} \n",
    "ME_ENDPOINT_MAX_REPLICA_COUNT = 10 # @param {type:\"integer\"} \n",
    "\n",
    "# Vertex AI Vector Search (MatchingEngine) Index Parameters\n",
    "ME_INDEX_NAME = 'fhir_me_index'  # @param {type: \"string\"}\n",
    "ME_EMBEDDING_GCS_DIR = f'{PROJECT_ID}-me-bucket' # @param {type:\"string\"} \n",
    "ME_DESCRIPTION = \"Index for FHIR Resources\" # @param {type:\"string\"} \n",
    "\n",
    "# Set the LLM to use\n",
    "VERTEX_AI_MODEL_NAME = 'gemini-1.0-pro-001'\n",
    "TEXT_EMBEDDING_MODEL_NAME = \"textembedding-gecko@003\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85415490-1446-4983-8691-0fcd43eee3cb",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbf5bd72-a688-441e-9fa9-43f25b2b86f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain version: 0.1.16\n"
     ]
    }
   ],
   "source": [
    "# Utils\n",
    "from pprint import pprint\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "from rich.console import Console\n",
    "from rich.markdown import Markdown\n",
    "\n",
    "from typing import Dict, Optional, Any, List\n",
    "\n",
    "# Google Libs\n",
    "import vertexai\n",
    "vertexai.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "from google.cloud.aiplatform.matching_engine.matching_engine_index_endpoint import (\n",
    "    Namespace,\n",
    "    NumericNamespace,\n",
    ")\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Langchain\n",
    "import langchain\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.chains import RetrievalQA, LLMChain, SequentialChain, TransformChain\n",
    "from langchain.globals import set_debug, set_verbose\n",
    "print(f\"LangChain version: {langchain.__version__}\")\n",
    "\n",
    "# LangChain Google Libs\n",
    "from langchain_google_vertexai import VertexAI\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_google_vertexai import VectorSearchVectorStore\n",
    "\n",
    "# Custom Utils\n",
    "## Custom Matching Engine\n",
    "from utils.matching_engine import MatchingEngine\n",
    "from utils.matching_engine_utils import MatchingEngineUtils\n",
    "\n",
    "## Neo4J\n",
    "from utils.NEO4J_Graph import Graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e33c02-67b0-45ff-ae3e-292c916ace19",
   "metadata": {},
   "source": [
    "### Establish Neo4J Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed0ba81b-b946-415a-8ec9-5797c77e25a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: NEO4J_USER=neo4j\n",
      "env: NEO4J_PASSWORD=password\n"
     ]
    }
   ],
   "source": [
    "%env NEO4J_USER={NEO4J_USER}\n",
    "%env NEO4J_PASSWORD={NEO4J_PASSWORD}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36967464-b2e3-4592-a19d-d038395f1929",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE          COMMAND                  CREATED      STATUS       PORTS                                                                                            NAMES\n",
      "c11076036c54   neo4j:latest   \"tini -g -- /startup…\"   6 days ago   Up 4 hours   0.0.0.0:7474->7474/tcp, :::7474->7474/tcp, 7473/tcp, 0.0.0.0:7687->7687/tcp, :::7687->7687/tcp   testneo4j\n"
     ]
    }
   ],
   "source": [
    "# Check if Docker Container is running\n",
    "! docker ps -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8fefbdc-305b-49f4-b6e8-8a4cff2cb3cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testneo4j\n"
     ]
    }
   ],
   "source": [
    "# Start the Container if it is not running\n",
    "! docker start testneo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27ec5ac4-7816-413b-85bf-efecf6a4c498",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate & Connect to Neo4J graph\n",
    "graph = Graph(NEO4J_URL, NEO4J_USER, NEO4J_PASSWORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22503c29-f8ed-43d7-8cef-4cdf620e5dfc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['AllergyIntolerance', 10],\n",
      " ['CarePlan', 60],\n",
      " ['CareTeam', 60],\n",
      " ['Claim', 3486],\n",
      " ['Condition', 656],\n",
      " ['Device', 39],\n",
      " ['DiagnosticReport', 3100],\n",
      " ['DocumentReference', 1859],\n",
      " ['Encounter', 1859],\n",
      " ['ExplanationOfBenefit', 3486],\n",
      " ['ImagingStudy', 8],\n",
      " ['Immunization', 259],\n",
      " ['Medication', 957],\n",
      " ['MedicationAdministration', 957],\n",
      " ['MedicationRequest', 1627],\n",
      " ['Observation', 13501],\n",
      " ['Patient', 20],\n",
      " ['Procedure', 2966],\n",
      " ['SupplyDelivery', 239]]\n"
     ]
    }
   ],
   "source": [
    "# Test Neo4J Connection\n",
    "# Get type and number of each FHIR resource in the database\n",
    "resource_metrics = graph.resource_metrics()\n",
    "resource_metrics.sort()\n",
    "pprint(resource_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8df95e7b-c42e-4894-9ab3-70d7613f7fc4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database Metrics:\n",
      "    - Node Count = 37884\n",
      "    - Relationship Count = 190926\n"
     ]
    }
   ],
   "source": [
    "node_count, relationship_count = graph.database_metrics()\n",
    "print('Database Metrics:')\n",
    "print(f'    - Node Count = {node_count}')\n",
    "print(f'    - Relationship Count = {relationship_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60782d8-da1f-4493-a7db-35c81f4c0cd9",
   "metadata": {},
   "source": [
    "### Establish VectorSearch Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04ca1452-bbac-440c-b059-31f378534808",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VertexAIEmbeddings(client=<vertexai.language_models.TextEmbeddingModel object at 0x7f82ee9964d0>, project='propane-crawler-363311', location='us-central1', request_parallelism=5, max_retries=6, stop=None, model_name='textembedding-gecko@003', client_preview=None, temperature=None, max_output_tokens=None, top_p=None, top_k=None, credentials=None, n=1, streaming=False, safety_settings=None, api_transport=None, api_endpoint=None, instance={'max_batch_size': 250, 'batch_size': 250, 'min_batch_size': 5, 'min_good_batch_size': 5, 'lock': <unlocked _thread.lock object at 0x7f82fcb8d640>, 'batch_size_validated': False, 'task_executor': <concurrent.futures.thread.ThreadPoolExecutor object at 0x7f82fc1e0340>, 'embeddings_task_type_supported': True, 'get_embeddings_with_retry': <function TextEmbeddingModel.get_embeddings at 0x7f82ee831a20>})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Text Embedding\n",
    "text_embedding_model = VertexAIEmbeddings(\n",
    "    model_name=TEXT_EMBEDDING_MODEL_NAME,\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    max_retries=6\n",
    ")\n",
    "\n",
    "text_embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "895a73bf-d1c5-4a28-ab1f-00cc06a6e4d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- ME_INDEX_ID:projects/884766917846/locations/us-central1/indexes/7340819014102286336\n",
      "- ME_INDEX_ENDPOINT_ID:projects/884766917846/locations/us-central1/indexEndpoints/2407125622317907968\n"
     ]
    }
   ],
   "source": [
    "# Get Matching Engine Index id and Endpoint id\n",
    "me_utils = MatchingEngineUtils(PROJECT_ID, REGION, ME_INDEX_NAME)\n",
    "ME_INDEX_ID, ME_INDEX_ENDPOINT_ID = me_utils.get_index_and_endpoint()\n",
    "\n",
    "print(f'- ME_INDEX_ID:{ME_INDEX_ID}\\n- ME_INDEX_ENDPOINT_ID:{ME_INDEX_ENDPOINT_ID}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b97460f8-06ae-4291-a930-e6105bbd813b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_google_vertexai.vectorstores.vectorstores.VectorSearchVectorStore at 0x7f82ee881c60>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store = VectorSearchVectorStore.from_components(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=REGION,\n",
    "    gcs_bucket_name=f\"gs://{ME_EMBEDDING_GCS_DIR}\".split(\"/\")[2],\n",
    "    index_id=ME_INDEX_ID,\n",
    "    endpoint_id=ME_INDEX_ENDPOINT_ID,\n",
    "    stream_update=True,\n",
    "    embedding=text_embedding_model\n",
    ")\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ff24356-8257-4148-be70-39a854d13ba1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='This is a sample Resource Type', metadata={'fhir_patient_id': 'pid_111111111', 'fhir_resource_id': 'rid_111111111', 'fhir_resource_type': 'Test_Resource_type', 'neo4j_node_id': 'nid_111111111'}),\n",
       "  0.7039443254470825)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test VectorSearch is Connected\n",
    "query_text = 'sample Resource'\n",
    "response = vector_store.similarity_search_with_score(query=query_text, k=1)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8edb733-19b6-4c03-9e50-7af83b4e16dc",
   "metadata": {},
   "source": [
    "### Google Vertex AI LLM Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed3d0d38-32c8-4f5a-86ce-44ec524ff618",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gemini-1.0-pro-001'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = VertexAI(model_name=VERTEX_AI_MODEL_NAME)\n",
    "llm.model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0666d80-ce8d-4b51-aaae-690d70201fbb",
   "metadata": {},
   "source": [
    "<br>***QA Without RAG***\n",
    "\n",
    "Asking LLM a question without context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b73da16b-c9a8-44c8-9360-b67d23fbe3b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What allergies does Antone63 have?\n",
      "LLM Answer: I do not have any information on any allergies that Antone63 may have.\n"
     ]
    }
   ],
   "source": [
    "# Ask LLM a question\n",
    "question = \"What allergies does Antone63 have?\"\n",
    "\n",
    "no_rag_response = llm.invoke(question)\n",
    "\n",
    "print(f'Question: {question}')\n",
    "print(f'LLM Answer: {no_rag_response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d85e83-c32f-4fe5-a4ed-b6cc05679de1",
   "metadata": {},
   "source": [
    "## Retrieval with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19990da0-d0eb-4ce4-a89e-969e286795fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"Please provide a summary of Akiko835 Larkin917's Conditions. If there are multiple active conditions, list the most recent one. In the case of multiple conditions with the same most recent date, list all of them. If no active conditions are found, please state 'No active medical conditions found'.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae2f905-57d4-4f84-aafc-58d247b1ce08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting Langchain Global Variables\n",
    "from langchain.globals import set_verbose, set_debug\n",
    "\n",
    "# Change to False if you do not want debug and execution information\n",
    "langchain_debug = True\n",
    "set_debug(langchain_debug)\n",
    "set_verbose(langchain_debug)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cd30d4-c484-4c16-a93b-d27c97951e6c",
   "metadata": {},
   "source": [
    "### Step-01: Get Patient Name\n",
    "***Tip:*** \n",
    "- Minimize Cost & Latency - by first trying to extract patient name locally.\n",
    "- If regex does not help, then use LLM.\n",
    "- Fallback - Prompt user for Input\n",
    "- You can use less powerful LLMs for this to save cost. E.g. Gemma(offline) or Smaller LLMs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c010b54-9b78-4978-8276-8b6fa7087d71",
   "metadata": {},
   "source": [
    "<br>***Using Regex to extract patient name***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ab08fcf-cf91-45fd-83b7-8f1c7145b262",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get Patient Name Local Function using Python regex\n",
    "def extract_patient_name_with_custom_function(query: str) -> Optional[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Extracts patient's first and last name from the query using a regular expression.\n",
    "\n",
    "    Args:\n",
    "        query: The user's question or statement.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the extracted first and last names, or None if not found.\n",
    "    \"\"\"\n",
    "    # name_pattern = re.compile(r\"(?:Dr\\.|Mr\\.|Ms\\.|Mrs\\.)?\\s*([A-Z][a-zA-Z0-9']+)\\s+([A-Z][a-zA-Z0-9']*)\")  \n",
    "    name_pattern = re.compile(r\"(?:Dr\\.|Mr\\.|Ms\\.|Mrs\\.)?\\s*(\\b(?!What\\b)[A-Z][a-zA-Z0-9']*\\b)(?:\\s+([A-Z][a-zA-Z0-9']*)\\b)?\")\n",
    "    match = name_pattern.search(query)\n",
    "    if match:\n",
    "        first_name = match.group(1)\n",
    "        last_name = match.group(2) if match.group(2) else None\n",
    "        patient_name = {\"first_name\": first_name, \"last_name\": last_name}\n",
    "        return patient_name\n",
    "        # return None\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81b8346-9f43-497e-9211-9380cc8b3a6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'User Query: {query}')\n",
    "name = extract_patient_name_with_custom_function(query)\n",
    "print(f'Patient Name = {name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf3fd48-721c-4efc-9026-28bef30e1c74",
   "metadata": {},
   "source": [
    "<br>***Manulaly get Patient name from user using Input Prompt***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "399fa6ca-afa8-4275-a27a-d3fc290ffdf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_patient_name_from_user():\n",
    "    while True:\n",
    "        user_input_name = input(\"Please enter the patient's full name: \")\n",
    "        confirmed = input(f\"Is '{user_input_name}' correct? (yes/no): \").lower()\n",
    "        if confirmed == 'yes':\n",
    "            name_parts = user_input_name.split()\n",
    "            patient_name = {\"first_name\": name_parts[0], \"last_name\": name_parts[-1] if len(name_parts) > 1 else None}\n",
    "            # print(type(patient_name))\n",
    "            return patient_name\n",
    "        elif confirmed == 'no':\n",
    "            continue\n",
    "        else:\n",
    "            print(\"Invalid input. Please enter 'yes' or 'no'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fbdbd3-6369-4664-bae3-5951d01e2e1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "name = get_patient_name_from_user()\n",
    "print(f'Patient Name = {name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf9abc9-26a7-4132-ae57-08d32b7c4387",
   "metadata": {},
   "source": [
    "<br> ***Use LLM to extract Patient Name***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26bcdf80-7826-4a83-a757-dc59736a9f5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JsonOutputParser(pydantic_object=<class '__main__.PatientName'>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Patient name Output Parser\n",
    "\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# Define your desired data structure.\n",
    "class PatientName(BaseModel):\n",
    "    first_name: str = Field(description=\"extracted first name or partial name\")\n",
    "    last_name: str = Field(description=\"extracted last name or null\")\n",
    "    \n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "patient_name_parser = JsonOutputParser(pydantic_object=PatientName)\n",
    "patient_name_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc695ca8-ab0c-4614-9251-a380ca8b344e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Formatted Prompt:\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "You are a medical assistant tasked with extracting patient names from text.\n",
      "The text may contain:\n",
      "1. The patient's full name (first and last)\n",
      "2. Only the patient's first name\n",
      "3. A partial name (e.g., a nickname, a last name with a prefix)\n",
      "4. Some Names will contain numbers and they are part of the name\n",
      "5. Names will contain special characters (e.g., apostrophes, hyphens)\n",
      "5. Names from diverse cultures and regions\n",
      "6. If you detect middle names, combine them into the last_name: last_name = '{All identified middle names} {last name}' (with a space between middle and last names)\n",
      "\n",
      "Identify and extract the patient's name information from the text. If you can identify both the first and last name, provide them. If you can only identify the first name or a partial name, provide that information and leave the missing part blank.\n",
      "\n",
      "Always provide the output in the following JSON format:\n",
      "{\"first_name\": \"[extracted first name or partial name]\", \"last_name\": \"[extracted last name or null]\"}\n",
      "\n",
      "Here's the text to analyze:\n",
      "\u001b[33;1m\u001b[1;3m{query}\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "patient_name_prompt_template = \"\"\"You are a medical assistant tasked with extracting patient names from text.\n",
    "The text may contain:\n",
    "1. The patient's full name (first and last)\n",
    "2. Only the patient's first name\n",
    "3. A partial name (e.g., a nickname, a last name with a prefix)\n",
    "4. Some Names will contain numbers and they are part of the name\n",
    "5. Names will contain special characters (e.g., apostrophes, hyphens)\n",
    "5. Names from diverse cultures and regions\n",
    "6. If you detect middle names, combine them into the last_name: last_name = '{{All identified middle names}} {{last name}}' (with a space between middle and last names)\n",
    "\n",
    "Identify and extract the patient's name information from the text. If you can identify both the first and last name, provide them. If you can only identify the first name or a partial name, provide that information and leave the missing part blank.\n",
    "\n",
    "Always provide the output in the following JSON format:\n",
    "{{\"first_name\": \"[extracted first name or partial name]\", \"last_name\": \"[extracted last name or null]\"}}\n",
    "\n",
    "Here's the text to analyze:\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "patient_name_prompt = ChatPromptTemplate.from_template(patient_name_prompt_template)\n",
    "patient_name_prompt.partial_variables = {\"format_instructions\": patient_name_parser.get_format_instructions()}\n",
    "\n",
    "# print('Output Parser Format Instructions:')\n",
    "# pprint(patient_name_prompt.partial_variables)\n",
    "\n",
    "print('\\nFormatted Prompt:')\n",
    "patient_name_prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3cc762c9-3e67-416c-ae5d-890fa9a836db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please list all current medications, including the medication name, dosage instructions, and status (e.g., active, completed). Present this informationin a markdown table with the columns ''Medication Name'', ''Dosage'', and ''Status''. If no current medications are found, please state ''No current medications found''.\n",
      "{'query': 'please list all current medications, including the medication name, '\n",
      "          'dosage instructions, and status (e.g., active, completed). Present '\n",
      "          \"this informationin a markdown table with the columns ''Medication \"\n",
      "          \"Name'', ''Dosage'', and ''Status''. If no current medications are \"\n",
      "          \"found, please state ''No current medications found''.\"}\n",
      "{'first_name': None, 'last_name': None}\n"
     ]
    }
   ],
   "source": [
    "# LangChain Debug\n",
    "debug_on = False\n",
    "set_debug(debug_on)\n",
    "set_verbose(debug_on)\n",
    "\n",
    "patient_name = 'Akiko835 Larkin917'\n",
    "\n",
    "patient_context = f'For the given patient: {patient_name}, '\n",
    "query= f\"please list all current medications, including the medication name, dosage instructions, and status (e.g., active, completed). Present this informationin a markdown table with the columns ''Medication Name'', ''Dosage'', and ''Status''. If no current medications are found, please state ''No current medications found''.\"\n",
    "\n",
    "print(query)\n",
    "\n",
    "# Query\n",
    "query_dict={'query': query}\n",
    "pprint(query_dict)\n",
    "\n",
    "# Chain\n",
    "fhir_chain = patient_name_prompt | llm\n",
    "patient_name_response = fhir_chain.invoke(query_dict)\n",
    "\n",
    "\n",
    "patient_name_response = json.loads(patient_name_response)\n",
    "print(patient_name_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa16b96-dc12-48b4-8837-f3d01fc94662",
   "metadata": {},
   "source": [
    "### Step-02: Get Patient ID\n",
    "\n",
    "To find the relevant FHIR Patient Resource, even when dealing with potentially incomplete patient names from the user's query or LLM response, we:\n",
    "\n",
    "- **Construct the Query:** We strategically build our query using the same template as our pre-processed resource text representation. This ensures higher accuracy even with partial names.\n",
    "- **Perform Similarity Search:** This carefully crafted query is then used to search our VectorSearch Index, with the expectation that the top result is the matching FHIR Patient Resource.\n",
    "- **Extract ID:** Finally, we retrieve the fhir_patient_id directly from the metadata of the identified document.\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "978b7bce-677a-40e1-888d-1b1ad4ec8e12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_patient_id(patient_name :dict) -> str:\n",
    "    patient_vs_query_text = f\"\"\"The type of information in this entry is patient. The name use for this patient is official. The name family for this patient is {patient_name[\"last_name\"]}. The name given 0 for this patient is {patient_name[\"first_name\"]}\"\"\"\n",
    "    # Create Retriever\n",
    "    vs_retirever = vector_store.as_retriever(search_type=\"similarity\")\n",
    "\n",
    "    # Filter by resource_type = Patient\n",
    "    vs_filter = [Namespace(name=\"fhir_resource_type\", allow_tokens=[\"Patient\"])]\n",
    "\n",
    "    # k = 1 - We only want the top 1 result\n",
    "    vs_retirever.search_kwargs = {\"filter\": vs_filter, \"k\":1}\n",
    "    docs = vs_retirever.invoke(patient_vs_query_text)\n",
    "    \n",
    "    # print(f'Vector Search Results:\\n{docs}\\n')\n",
    "    \n",
    "    # Get patient id from Document Metadata\n",
    "    patient_id = docs[0].metadata['fhir_patient_id'][0]\n",
    "    return patient_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "39e97553-dbfc-4e44-a4a7-804fffff9386",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient ID: 05c4608d-bd9a-5d04-41d7-a0293da7f5a5\n"
     ]
    }
   ],
   "source": [
    "patient_id_response = get_patient_id(patient_name_response)\n",
    "\n",
    "print(f'Patient ID: {patient_id_response}')\n",
    "# print(f'{type(response)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205e3aff-db33-4b71-8e75-bfeaada72a69",
   "metadata": {},
   "source": [
    "### Step-03: Identify FHIR Resource Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b339215-6cbd-4c30-ab54-07b1e32721c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JsonOutputParser(pydantic_object=<class '__main__.ResourceType'>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Resource Type Output Parser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# Define your desired data structure.\n",
    "class ResourceType(BaseModel):\n",
    "    resource_type: str = Field(description=\"extracted resource type name\")\n",
    "    \n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "resource_type_parser = JsonOutputParser(pydantic_object=ResourceType)\n",
    "resource_type_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "64a46c8f-80e4-46ee-9eb2-0042b1f1e0c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Formatted Prompt:\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "You are a healthcare specialist with deep knowledge of the FHIR standard.\n",
      "Your task is to identify the most appropriate FHIR resource type for the given query.\n",
      "Refer to the official FHIR resource guide at https://build.fhir.org/resourceguide.html. \n",
      "If needed, consult the detailed documentation linked from that guide.\n",
      "\n",
      "Return ONLY the resource type name if there is a clear match. If unsure, return \"Unknown\".\n",
      "\n",
      "Always provide the output in the following JSON format:\n",
      "{\"resource_type\": \"[extracted resource type name]\"}\n",
      "\n",
      "Here's the text to analyze:\n",
      "\u001b[33;1m\u001b[1;3m{query}\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FHIR_RESOURCE_LIST_URL = \"https://build.fhir.org/resourceguide.html\"\n",
    "\n",
    "resource_type_prompt_template = f\"\"\"You are a healthcare specialist with deep knowledge of the FHIR standard.\n",
    "Your task is to identify the most appropriate FHIR resource type for the given query.\n",
    "Refer to the official FHIR resource guide at {FHIR_RESOURCE_LIST_URL}. \n",
    "If needed, consult the detailed documentation linked from that guide.\n",
    "\n",
    "Return ONLY the resource type name if there is a clear match. If unsure, return \"Unknown\".\n",
    "\n",
    "Always provide the output in the following JSON format:\n",
    "{{{{\"resource_type\": \"[extracted resource type name]\"}}}}\n",
    "\n",
    "Here's the text to analyze:\n",
    "{{query}}\n",
    "\"\"\"\n",
    "resource_type_prompt = ChatPromptTemplate.from_template(resource_type_prompt_template)\n",
    "resource_type_prompt.partial_variables = {\"format_instructions\": resource_type_parser.get_format_instructions()}\n",
    "\n",
    "# print('Output Parser Format Instructions:')\n",
    "# pprint(resource_type_prompt.partial_variables)\n",
    "\n",
    "print('\\nFormatted Prompt:')\n",
    "resource_type_prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "439cc0d2-4684-44a9-9359-3e167129b90c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Please provide a summary of Akiko835 Larkin917's Conditions. If there are multiple active conditions, list the most recent one. In the case of multiple conditions with the same most recent date, list all of them. If no active conditions are found, please state 'No active medical conditions found'.\n",
      "Resource Type: Condition\n"
     ]
    }
   ],
   "source": [
    "# LangChain Debug\n",
    "debug_on = False\n",
    "set_debug(debug_on)\n",
    "set_verbose(debug_on)\n",
    "\n",
    "# Chain\n",
    "resource_type_chain = resource_type_prompt | llm | resource_type_parser\n",
    "resource_type_response = resource_type_chain.invoke({'query': query})\n",
    "\n",
    "print(f'Query: {query}')\n",
    "print(f'Resource Type: {resource_type_response[\"resource_type\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c570933-8446-4000-aee0-663858502631",
   "metadata": {},
   "source": [
    "### Step-04: Vector Search\n",
    "\n",
    "In this step we perform a Similarity search on VertexAI VectoreSearch Index to retrieve FHIR Reources that match the user query.\n",
    "\n",
    "**Steps:**\n",
    "- Perform a Vector Search with Filters based on the retrieved patient_id and resource_type\n",
    "- Since FHIR Resources reference other resources, we also need to provide the referenced Resources to provide the full context to the LLM to imporve the accuracy of the respone. We do this by querying the Neo4J database to get immediate Neigbour resources for each resource returned by the \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "adc4e3cc-60f0-4330-b7f6-53e5befdd0d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def retrieve_relevant_resources(query: str, k: int, \n",
    "                                resource_type_text: str,\n",
    "                               patient_id: str) -> list[str]:\n",
    "    \n",
    "    # Create Retriever\n",
    "    vs_retirever = vector_store.as_retriever(search_type=\"similarity\")\n",
    "    \n",
    "    # Filter by fhir_resource_type and fhir_patient_id to retrieve only relevant FHIR Resources\n",
    "    vs_filter = [\n",
    "        Namespace(name=\"fhir_resource_type\", allow_tokens=[resource_type_text]),\n",
    "        Namespace(name=\"fhir_patient_id\", allow_tokens=[patient_id])\n",
    "    ]\n",
    "    \n",
    "    # print(f'retrieve_relevant_resources resource_type: {resource_type_text}')\n",
    "    if debug_on:\n",
    "        print(f'vs_filter:')\n",
    "        pprint(vs_filter)\n",
    "        print('\\n')\n",
    "        \n",
    "    # Retrieve all Resources based on above fitler\n",
    "    vs_retirever.search_kwargs = {\"filter\": vs_filter, 'k':k}\n",
    "    docs = vs_retirever.invoke(query)\n",
    "    \n",
    "    \n",
    "    # print(f'Retrieved Resource Documents:')\n",
    "    # pprint(docs)\n",
    "\n",
    "    # retrieved_resource_ids = [doc.metadata[\"fhir_resource_id\"][0] for doc in docs]\n",
    "    # return retrieved_resource_ids\n",
    "    return(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6f48f634-a667-45cb-9da4-8cef772e36d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide a summary of Akiko835 Larkin917's Conditions. If there are multiple active conditions, list the most recent one. In the case of multiple conditions with the same most recent date, list all of them. If no active conditions are found, please state 'No active medical conditions found'.\n",
      "retrieve_relevant_resources resource_type: Condition\n",
      "Total Resources: 25\n"
     ]
    }
   ],
   "source": [
    "debug_on = False\n",
    "print(query)\n",
    "docs = retrieve_relevant_resources(query,\n",
    "                                   k=25,\n",
    "                                  patient_id=patient_id_response,\n",
    "                                  resource_type_text=resource_type_response['resource_type'])\n",
    "\n",
    "retrieved_resource_ids = [doc.metadata['fhir_resource_id'][0] for doc in docs]\n",
    "\n",
    "# for doc in docs:\n",
    "#     resource_metadata = doc.metadata\n",
    "#     patient_id = resource_metadata['fhir_patient_id']\n",
    "#     resource_id = resource_metadata['fhir_resource_id']\n",
    "#     resource_type = resource_metadata['fhir_resource_type']    \n",
    "#     print(f'patient_id: {patient_id}\\t resource_id:{resource_id}\\t resource_type:{resource_type}')\n",
    "#     # print(doc.page_content)\n",
    "\n",
    "print(f'Total Resources: {len(docs)}')\n",
    "# print(f'ResourcesIds List: {retrieved_resource_ids}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace65cdb-63b3-4b99-b070-87382616cc30",
   "metadata": {},
   "source": [
    "<br> ***With the above retrieved resources as llm context, let us try to ask the LLM user query and check its response***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6194f180-6268-4ae7-8c53-a7c1c90008cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "\n",
      "System: The context below contains entries about the patient's healthcare. \n",
      "Please limit your answer to the information provided in the context. Do not make up facts.\n",
      "Please limit your answers only about the patient in the user question. If you do not find the patient name in the context.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "If you are asked about the patient's name and one the entries is of type patient, you should look for the first given name and family name and answer with: [given] [family]\n",
      "----------------\n",
      "\u001b[33;1m\u001b[1;3m{context}\u001b[0m\n",
      "\n",
      "Here's the text to analyze:\n",
      "\u001b[33;1m\u001b[1;3m{query}\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_query_prompt='''\n",
    "System: The context below contains entries about the patient's healthcare. \n",
    "Please limit your answer to the information provided in the context. Do not make up facts.\n",
    "Please limit your answers only about the patient in the user question. If you do not find the patient name in the context.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "If you are asked about the patient's name and one the entries is of type patient, you should look for the first given name and family name and answer with: [given] [family]\n",
    "----------------\n",
    "{context}\n",
    "\n",
    "Here's the text to analyze:\n",
    "{query}\n",
    "'''\n",
    "\n",
    "user_query_prompt = ChatPromptTemplate.from_template(user_query_prompt)\n",
    "user_query_prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1487a1e2-3ad9-4b20-ad52-d418cdac2c04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query: Please provide a summary of Akiko835 Larkin917's Conditions. If there are multiple active conditions, list the most recent one. In the case of multiple conditions with the same most recent date, list all of them. If no active conditions are found, please state 'No active medical conditions found'.\n",
      "Response: Akiko835 Larkin917 has the following active conditions:\n",
      "- Social isolation (finding) - onset date time: 05/24/2023 at 07:49:17\n",
      "- Abnormal findings diagnostic imaging heart+coronary circulat (finding) - onset date time: 11/14/1987 at 00:07:02\n",
      "- Hyperlipidemia - onset date time: 06/30/1965 at 06:56:35\n",
      "- Has a criminal record (finding) - onset date time: 07/15/1959 at 07:47:50\n",
      "- Ischemic heart disease (disorder) - onset date time: 11/04/1987 at 06:56:35\n",
      "- Prediabetes - onset date time: 09/08/1943 at 06:56:35\n",
      "- Anemia (disorder) - onset date time: 09/08/1943 at 06:56:35\n",
      "- Osteoporosis (disorder) - onset date time: 10/23/1985 at 06:56:35\n"
     ]
    }
   ],
   "source": [
    "# LangChain Debug\n",
    "debug_on = False\n",
    "set_debug(debug_on)\n",
    "set_verbose(debug_on)\n",
    "\n",
    "# get all page_content of docs\n",
    "docs_page_contents_list = [doc.page_content for doc in docs]\n",
    "docs_page_contents = '\\n\\n'.join(docs_page_contents_list)\n",
    "# print(docs_page_contents)\n",
    "\n",
    "prompt_inputs = {'query': query, 'context': docs_page_contents}\n",
    "\n",
    "# Chain\n",
    "# print(f'Prompt Inputs:')\n",
    "# pprint(prompt_inputs)\n",
    "\n",
    "print(f'User Query: {query}')\n",
    "user_query_chain = user_query_prompt | llm\n",
    "user_query_response = user_query_chain.invoke(prompt_inputs)\n",
    "print(f'Response: {user_query_response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216798cc-39ed-45ea-959b-fe265dcc6efb",
   "metadata": {},
   "source": [
    "### Step-05 Neo4J Query - Enhanced Context\n",
    "\n",
    "<br>***Fetch referenced resources for additional context for LLM***\n",
    "\n",
    "To enhance the LLM's accuracy in answering user questions, it's crucial to fetch the text representation of all referenced FHIR resources. For instance, an Observation resource might reference Specimen, Device, Procedure, etc. This provides complete context to the LLM, enabling it to accurately answer queries involving these linked resources.\n",
    "\n",
    "Additionally, this ensures the inclusion of key information like patient names from the referenced Patient resource, preventing incorrect responses stating that the context lacks information about the patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a3c23989-44ac-4680-ba5b-81fcee79c58b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fetch_enhanced_context(in_resource_ids: list) -> str:\n",
    "    \n",
    "    # Fetch relevant text from the graph database\n",
    "    cipher = f\"\"\"\n",
    "    MATCH (node: resource)\n",
    "    WHERE node.id IN {in_resource_ids}\n",
    "\n",
    "    OPTIONAL MATCH (node)-[r]-(neighbor :resource)\n",
    "    WITH COLLECT(DISTINCT node) + COLLECT(DISTINCT neighbor) AS allNodes\n",
    "    UNWIND allNodes as uniqueNode\n",
    "    RETURN uniqueNode.text\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = graph.query(cipher)[0]\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error in Graph Query: {e}\")\n",
    "\n",
    "    relevant_resource_text_list = [resource_id[0] for resource_id in response]\n",
    "    \n",
    "    # print(f'Number of resources matching query: {len(relevant_resource_text_list)}')\n",
    "    # print(f'Enhanced Context Text:')\n",
    "    # pprint(relevant_resource_text_list)\n",
    "    \n",
    "    return relevant_resource_text_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0dafbba2-6909-4791-a90c-d59ac5b36889",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "enchanced_context_resource_text len: 93\n"
     ]
    }
   ],
   "source": [
    "enhanced_context_resources = fetch_enhanced_context(retrieved_resource_ids)\n",
    "print(type(enhanced_context_resources))\n",
    "print(f'enchanced_context_resource_text len: {len(enhanced_context_resources)}')\n",
    "# print(f'enhanced_context_resources: {enhanced_context_resources}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bca6bf00-8569-4fb5-9d1f-c2f39d5d91ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "\n",
      "System: \n",
      "You are a Healthcare AI assistant. Your task is to respond to Doctors queries based on patient information from FHIR (Fast Healthcare Interoperability Resources) data.\n",
      "\n",
      "**Context Handling:**\n",
      "\n",
      "1. The context below contains entries about the patient's healthcare in FHIR format.\n",
      "2. Identify and parse relevant FHIR resources within the context (e.g., Patient, Observation, Encounter).\n",
      "3. Utilize the standard FHIR terminology and codes (e.g., LOINC for observations) to extract specific information.\n",
      "4. Limit your answer to the information provided in the context. Do not make up facts.\n",
      "5. Focus your answers on the patient specified in the user question. \n",
      "6. If you don't know the answer, simply state that you don't have enough information.\n",
      "7. Ensure to look for the correct FHIR resource type in context to answer the query. For Example, to answer questions about claims history, search for FHIR resource of type 'Claim'\n",
      "8. Utilize the 'CurrentDateTime' value in the context to calculate relative time periods (e.g., \"last week\") for queries referencing them.\n",
      "\n",
      "**Date Handling:**\n",
      "\n",
      "1. Pay very close attention to the dates in the context and user query.\n",
      "2. Prioritize information from the most recent dates when responding to queries without a specified date.\n",
      "3. Compare dates in the context and user query to determine the temporal relationship between events.\n",
      "4. Use the 'CurrentDateTime' value, which is in MM/DD/YYYY format, to calculate relative time periods and filter relevant FHIR resources based on those periods.\n",
      "5. Note that dates in the context are formatted as MM/DD/YYYY (e.g., 10/22/2015)\n",
      "\n",
      "**Output Formatting:**\n",
      "\n",
      "Before printing verify that you have considered the date and time in your response meets date time criteria in the user query (if mentioned).  \n",
      "\n",
      "1. Respond to the user question with the above in mind.\n",
      "2. Include the patient's name (given name and family name) in your response.\n",
      "3. Format your output in markdown for clarity.\n",
      "4. Make the patients name Bold.\n",
      "5. Format the data into Markdown table with clear headers for information you think can be better represented in a table.\n",
      "\n",
      "**Example Output:**\n",
      "   - For vital signs: \"[Patient Name]'s [Observation Name] was [Value] [Unit] on [Date].\"\n",
      "   - For encounters: \"[Patient Name] had a [Encounter Type] on [Date] (reason: [Reason if available]).\"\n",
      "\n",
      "----------------\n",
      "Today's date - CurrentDateTime = \u001b[33;1m\u001b[1;3m{CurrentDateTime}\u001b[0m\n",
      "\n",
      "Context about the Patient\n",
      "\u001b[33;1m\u001b[1;3m{context}\u001b[0m\n",
      "\n",
      "User Question:\n",
      "\u001b[33;1m\u001b[1;3m{query}\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_query_prompt='''\n",
    "System: \n",
    "You are a Healthcare AI assistant. Your task is to respond to Doctors queries based on patient information from FHIR (Fast Healthcare Interoperability Resources) data.\n",
    "\n",
    "**Context Handling:**\n",
    "\n",
    "1. The context below contains entries about the patient's healthcare in FHIR format.\n",
    "2. Identify and parse relevant FHIR resources within the context (e.g., Patient, Observation, Encounter).\n",
    "3. Utilize the standard FHIR terminology and codes (e.g., LOINC for observations) to extract specific information.\n",
    "4. Limit your answer to the information provided in the context. Do not make up facts.\n",
    "5. Focus your answers on the patient specified in the user question. \n",
    "6. If you don't know the answer, simply state that you don't have enough information.\n",
    "7. Ensure to look for the correct FHIR resource type in context to answer the query. For Example, to answer questions about claims history, search for FHIR resource of type 'Claim'\n",
    "8. Utilize the 'CurrentDateTime' value in the context to calculate relative time periods (e.g., \"last week\") for queries referencing them.\n",
    "\n",
    "**Date Handling:**\n",
    "\n",
    "1. Pay very close attention to the dates in the context and user query.\n",
    "2. Prioritize information from the most recent dates when responding to queries without a specified date.\n",
    "3. Compare dates in the context and user query to determine the temporal relationship between events.\n",
    "4. Use the 'CurrentDateTime' value, which is in MM/DD/YYYY format, to calculate relative time periods and filter relevant FHIR resources based on those periods.\n",
    "5. Note that dates in the context are formatted as MM/DD/YYYY (e.g., 10/22/2015)\n",
    "\n",
    "**Output Formatting:**\n",
    "\n",
    "Before printing verify that you have considered the date and time in your response meets date time criteria in the user query (if mentioned).  \n",
    "\n",
    "1. Respond to the user question with the above in mind.\n",
    "2. Include the patient's name (given name and family name) in your response.\n",
    "3. Format your output in markdown for clarity.\n",
    "4. Make the patients name Bold.\n",
    "5. Format the data into Markdown table with clear headers for information you think can be better represented in a table.\n",
    "\n",
    "**Example Output:**\n",
    "   - For vital signs: \"[Patient Name]'s [Observation Name] was [Value] [Unit] on [Date].\"\n",
    "   - For encounters: \"[Patient Name] had a [Encounter Type] on [Date] (reason: [Reason if available]).\"\n",
    "\n",
    "----------------\n",
    "Today's date - CurrentDateTime = {CurrentDateTime}\n",
    "\n",
    "Context about the Patient\n",
    "{context}\n",
    "\n",
    "User Question:\n",
    "{query}\n",
    "'''\n",
    "\n",
    "user_query_prompt = ChatPromptTemplate.from_template(user_query_prompt)\n",
    "user_query_prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "53749f61-b9a0-4301-ba9c-f7f39d993325",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query: Please provide a summary of Akiko835 Larkin917's Conditions. If there are multiple active conditions, list the most recent one. In the case of multiple conditions with the same most recent date, list all of them. If no active conditions are found, please state 'No active medical conditions found'.\n",
      "Response: The most recent active condition for Akiko835 Larkin917 is **Social isolation (finding)**, last recorded on 05/24/2023.\n"
     ]
    }
   ],
   "source": [
    "# LangChain Debug\n",
    "debug_on = False\n",
    "set_debug(debug_on)\n",
    "set_verbose(debug_on)\n",
    "\n",
    "enhanced_context_resources = [res_text for res_text in enhanced_context_resources if res_text is not None]\n",
    "enchanced_context_resources_text = '\\n\\n'.join(enhanced_context_resources)\n",
    "# print(enhanced_context_resources)\n",
    "\n",
    "current_datetime = datetime.now(timezone.utc).astimezone(timezone(offset=timedelta(hours=5, minutes=30)))\n",
    "current_datetime_str = current_datetime.strftime(\"%m/%d/%Y\")\n",
    "    \n",
    "prompt_inputs = {'query': query, 'CurrentDateTime': current_datetime_str,'context': enchanced_context_resources_text}\n",
    "\n",
    "# Chain\n",
    "# print(f'Prompt Inputs:')\n",
    "# pprint(prompt_inputs)\n",
    "\n",
    "print(f'User Query: {query}')\n",
    "user_query_chain = user_query_prompt | llm\n",
    "user_query_response = user_query_chain.invoke(prompt_inputs)\n",
    "print(f'Response: {user_query_response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff17fa50-11c8-46b3-8f70-e1fc5ecd7cc6",
   "metadata": {},
   "source": [
    "## Bringing it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b69ac212-f0c9-4641-8d24-edfc1d42e1ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "debug_on = False\n",
    "def answer_fhir_query(user_query: str):\n",
    "    \n",
    "    # Get Patient Name Chain\n",
    "    patient_name_chain = patient_name_prompt | llm\n",
    "    patient_name_response = patient_name_chain.invoke({'query': user_query})\n",
    "    \n",
    "    if patient_name_response:\n",
    "        patient_name = json.loads(patient_name_response)\n",
    "    else:\n",
    "        patient_name = get_patient_name_from_user()\n",
    "        \n",
    "    \n",
    "    if debug_on:\n",
    "        print('Patient Name Info:')\n",
    "        print(f'patient_name type: {type(patient_name)}')\n",
    "        print(f'patient_name: {patient_name}')\n",
    "        print('\\n')\n",
    "        \n",
    "    # Get Patient_Id\n",
    "    patient_id = get_patient_id(patient_name)\n",
    "    if debug_on:\n",
    "        print('Patient Id Info:')\n",
    "        print(f'patient_id: {patient_id}')\n",
    "        print('\\n')\n",
    "        \n",
    "    # Identify Resource Type\n",
    "    resource_type_chain = resource_type_prompt | llm | resource_type_parser\n",
    "    resource_type_response = resource_type_chain.invoke(user_query)\n",
    "    resource_type_text=resource_type_response['resource_type']\n",
    "\n",
    "    \n",
    "    if debug_on:\n",
    "        print('Resource Type Info:')\n",
    "        print(f'resource_type: {resource_type_response[\"resource_type\"]}')\n",
    "        print('\\n')\n",
    "        \n",
    "    # Vector Search: Get Relevant Resources based on user query\n",
    "    k=25\n",
    "    vs_search_resource_docs = retrieve_relevant_resources(user_query, \n",
    "                                                          k,\n",
    "                                                          patient_id=patient_id,\n",
    "                                                          resource_type_text=resource_type_text)\n",
    "    \n",
    "    # for doc in vs_search_resource_docs:\n",
    "    #     print(doc.metadata['fhir_resource_id'], \"-\", doc.metadata['fhir_resource_type'])\n",
    "    \n",
    "    vs_search_resource_ids = [doc.metadata['fhir_resource_id'][0] for doc in vs_search_resource_docs]\n",
    "    \n",
    "    if debug_on:\n",
    "        print('\\n')\n",
    "        print('Resource Ids retrieved from Vector Search:')\n",
    "        print(f'vs_search_resource_ids len: {len(vs_search_resource_ids)}')\n",
    "        # print(f'vs_search_resource_ids: {vs_search_resource_ids}')\n",
    "        print('\\n')\n",
    "    \n",
    "    # Get Current Date and Time in format MM/DD/YYYY\n",
    "    current_datetime = datetime.now(timezone.utc).astimezone(timezone(offset=timedelta(hours=5, minutes=30)))\n",
    "    current_datetime_str = current_datetime.strftime(\"%m/%d/%Y\")\n",
    "    \n",
    "    if resource_type_text == 'Patient':\n",
    "        context_text = '\\n\\n'.join([doc.page_content for doc in vs_search_resource_docs])\n",
    "        prompt_inputs = {'query': user_query, 'CurrentDateTime': current_datetime_str,'context': context_text}\n",
    "        \n",
    "    else:\n",
    "        # Neo4J query - for getting enhanced context\n",
    "        enhanced_context = fetch_enhanced_context(in_resource_ids=vs_search_resource_ids)\n",
    "        enhanced_context_text = '\\n\\n'.join([res_text for res_text in enhanced_context if res_text is not None])\n",
    "        prompt_inputs = {'query': user_query, 'CurrentDateTime': current_datetime_str,'context': enhanced_context_text}\n",
    "\n",
    "        if debug_on:\n",
    "            print('\\n')\n",
    "            print(f'# of Enhanced Context Resources: {len(enhanced_context)}')\n",
    "            print(f'Enhanced Context Text:')\n",
    "            pprint(enhanced_context_text)\n",
    "            print('\\n')\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    # Finally calling the LLM to answer user query\n",
    "   \n",
    "    user_query_chain = user_query_prompt | llm\n",
    "    user_query_response = user_query_chain.invoke(prompt_inputs)\n",
    "    \n",
    "    return user_query_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8e75a5db-74c3-41ff-991c-5411b03ee0f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃                                                   <span style=\"font-weight: bold\">User Query</span>                                                    ┃\n",
       "┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃                                                   \u001b[1mUser Query\u001b[0m                                                    ┃\n",
       "┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the Body Height of Benjamin360 Hintz995 and when was it measured?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃                                                  <span style=\"font-weight: bold\">LLM Response</span>                                                   ┃\n",
       "┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃                                                  \u001b[1mLLM Response\u001b[0m                                                   ┃\n",
       "┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Benjamin360 Hintz995's</span> Body Height was 173.1 cm on the following dates:                                            \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>10/22/2015                                                                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>12/22/2016                                                                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>10/25/2018                                                                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>01/21/2021                                                                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>02/11/2021                                                                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>10/28/2021                                                                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>05/19/2022                                                                                                      \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mBenjamin360 Hintz995's\u001b[0m Body Height was 173.1 cm on the following dates:                                            \n",
       "\n",
       "\u001b[1;33m • \u001b[0m10/22/2015                                                                                                      \n",
       "\u001b[1;33m • \u001b[0m12/22/2016                                                                                                      \n",
       "\u001b[1;33m • \u001b[0m10/25/2018                                                                                                      \n",
       "\u001b[1;33m • \u001b[0m01/21/2021                                                                                                      \n",
       "\u001b[1;33m • \u001b[0m02/11/2021                                                                                                      \n",
       "\u001b[1;33m • \u001b[0m10/28/2021                                                                                                      \n",
       "\u001b[1;33m • \u001b[0m05/19/2022                                                                                                      \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich.console import Console\n",
    "from rich.markdown import Markdown\n",
    "\n",
    "console = Console()\n",
    "\n",
    "langchain_debug = False\n",
    "set_debug(langchain_debug)\n",
    "set_verbose(langchain_debug)\n",
    "\n",
    "user_query = \"What is the Body Height of Benjamin360 Hintz995 and when was it measured?\"\n",
    "# user_query = \"What is the Body Weight of Benjamin360 Hintz995 and when was it measured?\"\n",
    "# user_query = \"Tell me about the last 5 Benjamin360's Procedures?\"\n",
    "# user_query = \"Tell me about observations performed by Benjamin360 in the last 2 years?\"\n",
    "# user_query = \"What allergies does Benjamin360 have?\"\n",
    "\n",
    "console.print(Markdown('# User Query'))\n",
    "print(user_query)\n",
    "\n",
    "console.print(Markdown('# LLM Response'))\n",
    "llm_user_query_response = (answer_fhir_query(user_query))\n",
    "console.print(Markdown(llm_user_query_response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "81913be9-6c00-4491-8cb3-e7282512d6fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_formatting_instructions = \"List the ouput in a Markdown table format.\"\n",
    "\n",
    "vital_signs_prompt = \"\"\"Please provide a summary of latest vital signs. Include the latest value, unit of measurement, and date taken (MM/DD/YYYY) for each of the following vital signs:\n",
    "\n",
    "*   Body Height (LOINC code 8302-2)\n",
    "*   Body Weight (LOINC code 29463-7)\n",
    "*   Body Mass Index (BMI) [Ratio] (LOINC code 39156-5)\n",
    "*   Body temperature (LOINC code 8310-5)\n",
    "*   Systolic blood pressure (LOINC code 8480-6)\n",
    "*   Diastolic blood pressure (LOINC code 8462-4)\n",
    "*   Heart rate (LOINC code 8867-4)\n",
    "*   Respiratory rate (LOINC code 9279-1)\n",
    "*   Oxygen saturation in Arterial blood by Pulse oximetry (LOINC code 59408-5)\n",
    "\n",
    "If a particular vital sign is not found in the records, please indicate so with 'N/A'. Present the information in a markdown table with the columns 'Vital Sign', 'Value', 'Unit', and 'Date Taken'.\"\"\"\n",
    "\n",
    "patient_summary_query = [\n",
    "    {'Demographics': \"please provide the following demographic information, if available in the context: full name, date of birth, gender, primary phone number, and home address.\"},\n",
    "    {'Medical History': \"Please provide a summary of all active Conditions. List the onset date, status and verification. List the ouput in a Markdown table format. If no active conditions are found, please state 'No active medical conditions found'.\"},\n",
    "    {'Medications': \"please list all current medications, including the medication name, dosage instructions, and status (e.g., active, completed). Present this informationin a markdown table with the columns 'Medication Name', 'Dosage', and 'Status'. If no current medications are found, please state 'No current medications found'.\"},\n",
    "    {'Allergies':\"please list all known allergies, including the allergen name and severity. Present this information in a markdown table with the columns 'Allergen' and 'Severity'. If no allergies are found, please state 'No known allergies found'.\"},\n",
    "    {'Immunizations':\"please list all immunizations for, including the vaccine code, date administered, and status. Present this information in a markdown table with the columns 'Vaccine Code', 'Date Administered', and 'Status'. If no immunizations are found, please state 'No immunizations found'.\"},\n",
    "    {'Vital Signs': vital_signs_prompt},\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "40f95f5d-eba3-4c81-b8c6-acba48e3f382",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "langchain_debug = False\n",
    "set_debug(langchain_debug)\n",
    "set_verbose(langchain_debug)\n",
    "def generate_patient_summary(patient_name):\n",
    "    \n",
    "    report_text = '# Patient Summary\\n'\n",
    "    \n",
    "    # console.print(Markdown('# Patient Summary'))\n",
    "    \n",
    "    patient_context = f'For the given patient: {patient_name}, '\n",
    "    \n",
    "    for section in patient_summary_query:\n",
    "        section_title = list(section.keys())[0]\n",
    "        print(f'Processing Section: {section_title}')\n",
    "        \n",
    "        report_text = report_text + '___\\n'\n",
    "        section_header = f'## {section_title}\\n'\n",
    "        report_text = report_text + section_header + '\\n'\n",
    "        report_text = report_text + '___\\n'\n",
    "        # console.print(Markdown(section_header))\n",
    "        \n",
    "        section_question = list(section.values())[0]\n",
    "        # llm_user_query_response = patient_context + section_question + output_formatting_instructions\n",
    "        llm_user_query_response = answer_fhir_query(patient_context + section_question + output_formatting_instructions)\n",
    "        report_text = report_text + llm_user_query_response + '\\n'\n",
    "        \n",
    "        # console.print(Markdown(llm_user_query_response))\n",
    "    \n",
    "    # console.print(Markdown(report_text))\n",
    "    report_text = report_text + '**END OF REPORT**'\n",
    "    return report_text\n",
    "        \n",
    "#generate_patient_summary('Akiko835 Larkin917')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "01b5fbfc-c438-41e9-85ee-fd240e780820",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Section: Demographics\n",
      "Processing Section: Medical History\n",
      "Processing Section: Medications\n",
      "Processing Section: Allergies\n",
      "Processing Section: Immunizations\n",
      "Processing Section: Vital Signs\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter the patient's full name:  Akiko835 Larkin917\n",
      "Is 'Akiko835 Larkin917' correct? (yes/no):  yes\n"
     ]
    }
   ],
   "source": [
    "patient_summary_name = 'Akiko835 Larkin917'\n",
    "patient_summary_response = generate_patient_summary(patient_name) \n",
    "\n",
    "patient_summary_md_file = f'{patient_summary_name}.md'\n",
    "with open (patient_summary_md_file, 'w') as f:\n",
    "    f.write(patient_summary_response)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c87d3bf-c11c-455c-a947-7806a006d3bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
